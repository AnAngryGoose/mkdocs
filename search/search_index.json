{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Homelab Docs <p>     Various guides/references for some self hosted projects.   </p> <p> Overview github </p>"},{"location":"infrastructure/docker/cheatsheet/","title":"Docker Cheat Sheet &amp; Workflows","text":"<p>tags:</p> <ul> <li>docker</li> <li>service</li> <li>cheat_sheet</li> <li>infrastructure</li> </ul>"},{"location":"infrastructure/docker/cheatsheet/#1-command-reference","title":"1. Command Reference","text":""},{"location":"infrastructure/docker/cheatsheet/#11-project-management-docker-compose","title":"1.1. Project Management (Docker Compose)","text":"<p>Primary method for managing stacks.</p> Command Description <code>docker compose up -d</code> Create/Start services in detached mode. <code>docker compose up -d --force-recreate</code> Force container recreation (useful if config changed but image didn't). <code>docker compose restart</code> Stop then restart containers. Does not rebuild/pull updated images. <code>docker compose down</code> Stop and remove containers/networks. <code>docker compose down -v</code> DANGER: Removes containers AND standard volumes. <code>docker compose down --volumes --rmi all --remove-orphans</code> DANGER: Removes all volumes, images built by container, and orphaned compose files. <code>docker compose stop</code> Stops services without removing containers. <code>docker compose pull</code> Downloads latest images defined in YAML. <code>docker compose logs -f</code> Stream logs for all services in the stack. <code>docker compose config</code> Validate YAML syntax and print the resolved configuration. <code>docker compose exec &lt;svc&gt; &lt;cmd&gt;</code> Run command inside service (e.g., <code>docker compose exec pihole bash</code>)."},{"location":"infrastructure/docker/cheatsheet/#12-container-resource-monitoring","title":"1.2. Container &amp; Resource Monitoring","text":"<p>Debugging individual containers or checking resource load.</p> Command Description <code>docker ps</code> List running containers. <code>docker ps -a</code> List all containers (including stopped/exited). <code>docker stats</code> Live stream of CPU, RAM, and Net I/O usage per container. <code>docker logs -f &lt;container&gt;</code> Follow log output for a specific container (e.g., <code>omada-controller</code>). <code>docker inspect &lt;container&gt;</code> JSON dump of container config (IP address, mounts, env vars). <code>docker top &lt;container&gt;</code> Display the running processes of a container."},{"location":"infrastructure/docker/cheatsheet/#13-storage-volumes","title":"1.3. Storage &amp; Volumes","text":"<p>Managing persistence. Critical for the \"External Volume\" workflow.</p> Command Description <code>docker volume ls</code> List all volumes. <code>docker volume create &lt;name&gt;</code> Manually create a volume (required for <code>external: true</code>). <code>docker volume inspect &lt;name&gt;</code> Show volume mount point (usually <code>/var/lib/docker/volumes/...</code>). <code>docker volume rm &lt;name&gt;</code> Delete a volume. Data loss permanent. <code>docker volume prune</code> Remove all unused local volumes. <code>docker run --rm -v &lt;vol&gt;:/data busybox ls /data</code> Quickly list files inside a volume without attaching to the main app."},{"location":"infrastructure/docker/cheatsheet/#14-networking","title":"1.4. Networking","text":"<p>Essential for debugging communication issues.</p> Command Description <code>docker network ls</code> List all created networks. <code>docker network inspect &lt;net_name&gt;</code> Show containers and IP addresses assigned to a specific network. <code>docker network create &lt;name&gt;</code> Create a manual bridge network. <code>docker network prune</code> Remove all unused networks."},{"location":"infrastructure/docker/cheatsheet/#15-system-maintenance-housekeeping","title":"1.5. System Maintenance (Housekeeping)","text":"<p>Periodic cleanup.</p> Command Description <code>docker image prune -a</code> Remove all unused images (frees disk space). <code>docker system df</code> Show docker disk usage summary. <code>docker system prune</code> Cleanup: Removes stopped containers, unused networks, and dangling images. <code>docker system prune -a --volumes</code> Nuclear: Wipes everything not currently running."},{"location":"infrastructure/docker/cheatsheet/#2-workflows","title":"2. Workflows","text":""},{"location":"infrastructure/docker/cheatsheet/#21-deploying-new-stack-external-volume-method","title":"2.1. Deploying New Stack (External Volume Method)","text":"<p>Standard procedure for stateful services (Omada, Databases).</p> <ol> <li> <p>Pre-create Volumes: <pre><code>docker volume create service_data\ndocker volume create service_config\n</code></pre></p> </li> <li> <p>Deploy: <pre><code># Navigate to project folder\ndocker compose up -d\n</code></pre></p> </li> <li> <p>Verify Persistence: <pre><code>docker volume inspect service_data\n</code></pre></p> </li> </ol>"},{"location":"infrastructure/docker/cheatsheet/#22-updating-a-stack-zero-downtime-attempt","title":"2.2. Updating a Stack (Zero-Downtime Attempt)","text":"<p>Routine update cycle.</p> <pre><code>cd /opt/docker/project_name\n\n# 1. Pull updates\ndocker compose pull\n\n# 2. Recreate only changed containers\ndocker compose up -d\n\n# 3. Clean up old images to save space\ndocker image prune -f\n</code></pre>"},{"location":"infrastructure/docker/cheatsheet/#23-hot-backup-of-a-volume","title":"2.3. \"Hot\" Backup of a Volume","text":"<p>Backup data without stopping the container (using <code>tar</code>).</p> <pre><code># Syntax: docker run --rm -v &lt;volume_name&gt;:/data -v &lt;host_backup_dir&gt;:/backup alpine tar czf /backup/&lt;filename&gt;.tar.gz -C /data .\n\ndocker run --rm \\\n  -v omada_data:/data \\\n  -v $(pwd):/backup \\\n  alpine tar czf /backup/omada_data_backup_$(date +%F).tar.gz -C /data .\n</code></pre>"},{"location":"infrastructure/docker/cheatsheet/#24-debugging-connectivity","title":"2.4. Debugging Connectivity","text":"<p>If services (e.g., Pi-hole, Unbound) are not responding.</p> <pre><code># 1. Check if container is healthy\ndocker ps \n\n# 2. Check internal IP assignment\ndocker network inspect bridge_network_name\n\n# 3. Test resolution from INSIDE the container\ndocker exec -it pihole nslookup google.com 127.0.0.1\n</code></pre>"},{"location":"infrastructure/docker/cheatsheet/#3-environment-variables-env","title":"3. Environment Variables (<code>.env</code>)","text":"<p>Separates configuration (passwords, versions, ports) from the code (<code>compose.yaml</code>).</p>"},{"location":"infrastructure/docker/cheatsheet/#31-directory-location","title":"3.1. Directory Location","text":"<p>The <code>.env</code> file must be located in the same directory as the <code>compose.yaml</code> file for Docker Compose to automatically read it.</p> <p>File Structure:</p> <pre><code>~/homelab/docker/\n\u251c\u2500\u2500 .gitignore             # Global git ignore\n\u251c\u2500\u2500 core/                  # Project Stack (e.g., Homepage, Glances)\n\u2502   \u251c\u2500\u2500 .env               # &lt;--- GOES HERE (Specific to Core)\n\u2502   \u2514\u2500\u2500 compose.yaml\n\u251c\u2500\u2500 omada/                 # Project Stack (Network)\n\u2502   \u251c\u2500\u2500 .env               # &lt;--- GOES HERE (Specific to Omada)\n\u2502   \u2514\u2500\u2500 compose.yaml\n\u2514\u2500\u2500 couchdb/               # Project Stack (Obsidian Sync)\n    \u251c\u2500\u2500 .env               # &lt;--- GOES HERE (Specific to CouchDB)\n    \u2514\u2500\u2500 compose.yaml\n</code></pre>"},{"location":"infrastructure/docker/cheatsheet/#32-syntax-rules","title":"3.2. Syntax Rules","text":"<ul> <li>Format: <code>KEY=VALUE</code></li> <li>No Spaces: <code>PASSWORD = 123</code> is invalid. Use <code>PASSWORD=123</code>.</li> <li>Comments: Use <code>#</code> for comments.</li> <li>Special Characters: If a password contains <code>#</code>, <code>$</code>, or spaces, wrap the value in quotes: <code>DB_PASS=\"Secure#Pass$123\"</code>.</li> </ul>"},{"location":"infrastructure/docker/cheatsheet/#33-workflow-example-couchdb","title":"3.3. Workflow Example (CouchDB)","text":"<ol> <li> <p>Create the file: <pre><code>nano ~/homelab/docker/couchdb/.env\n</code></pre></p> </li> <li> <p>Define Variables: <pre><code># User Configuration\nCOUCH_USER=admin\nCOUCH_PASS=MySecretPassword123!\n\n# Network Configuration\nHOST_PORT=5984\n</code></pre></p> </li> <li> <p>Implement in <code>compose.yaml</code>: Use the <code>${VARIABLE_NAME}</code> syntax. <pre><code>services:\n  couchdb:\n    image: couchdb:latest\n    environment:\n      - COUCHDB_USER=${COUCH_USER}      # Pulls 'admin'\n      - COUCHDB_PASSWORD=${COUCH_PASS}  # Pulls 'MySecretPassword123!'\n    ports:\n      - ${HOST_PORT}:5984               # Maps port 5984\n</code></pre></p> </li> </ol>"},{"location":"infrastructure/docker/cheatsheet/#34-verification","title":"3.4. Verification","text":"<p>To verify that Docker is reading the variables correctly without actually starting the container:</p> <pre><code>docker compose config\n</code></pre> <p>This prints the \"resolved\" YAML to the terminal, showing the actual values instead of the <code>${VAR}</code> placeholders.</p>"},{"location":"infrastructure/docker/cheatsheet/#35-security-git","title":"3.5. Security (Git)","text":"<p>NEVER commit <code>.env</code> files to GitHub. They contain secrets.</p> <p>Ensure the global <code>.gitignore</code> includes:</p> <pre><code>**/.env\n**/.env.*\n</code></pre>"},{"location":"infrastructure/docker/docker/","title":"Docker Management Guide: Homelab Reference","text":""},{"location":"infrastructure/docker/docker/#1-overview","title":"1. Overview","text":"<ul> <li>Docker: A platform that virtualizes the OS, running applications in isolated environments called \"containers.\"</li> <li>Docker Compose: A declarative tool for defining and managing multi-container applications. It uses a <code>docker-compose.yml</code> file to configure services, networks, and volumes for a \"project.\"</li> <li>Docker Volumes: The preferred mechanism for persisting data generated by and used by Docker containers. Volumes are managed by Docker (<code>/var/lib/docker/volumes/</code>) and are decoupled from the container's lifecycle.</li> </ul>"},{"location":"infrastructure/docker/docker/#2-core-strategies-storage-networking","title":"2. Core Strategies: Storage &amp; Networking","text":"<p>I use two distinct patterns for volume and network management.</p> <p>I'm putting volumes first because it was somewhat hard to understand at first and led to a few lost files.</p>"},{"location":"infrastructure/docker/docker/#21-volume-management-strategy","title":"2.1. Volume Management Strategy","text":"<p>Volumes are essential for data persistence. Containers are ephemeral; volumes are persistent. Two types of named volumes are utilized, along with specific use cases for bind mounts.</p>"},{"location":"infrastructure/docker/docker/#strategy-1-standard-project-scoped-named-volumes","title":"Strategy 1: Standard (Project-Scoped) Named Volumes","text":"<ul> <li>Definition: Volumes are defined in the <code>docker-compose.yml</code> file without the <code>external</code> flag.</li> <li>Lifecycle: The volume is created upon the first execution of <code>docker compose up</code>. The volume's name is prefixed with the project (directory) name (e.g., <code>core_portainer_data</code>).</li> <li>Risk: This volume is \"owned\" by the Compose project. Running <code>docker compose down -v</code> will permanently delete this volume and all its data.</li> <li>Use Case: Non-critical data, caches, or applications where data is easily reproducible (e.g., <code>core</code> stack).</li> </ul>"},{"location":"infrastructure/docker/docker/#strategy-2-external-named-volumes","title":"Strategy 2: External Named Volumes","text":"<ul> <li>Definition: Volumes are created manually before running Compose, using <code>docker volume create &lt;name&gt;</code>. The <code>docker-compose.yml</code> file then references them using <code>external: true</code>.</li> <li>Lifecycle: The volume's lifecycle is decoupled from the Compose project. Commands such as <code>docker compose down -v</code> can be executed, or the project recreated, without affecting the stored data.</li> <li>Risk: Low. Data is safe from accidental deletion via Compose commands.</li> <li>Use Case: Recommended for all critical data. (e.g., <code>omada</code> stack, <code>couchdb_data</code>, <code>portainer_data</code>).</li> </ul>"},{"location":"infrastructure/docker/docker/#strategy-3-bind-mounts","title":"Strategy 3: Bind Mounts","text":"<ul> <li>Definition: Maps a specific file or directory on the Host OS directly to a location inside the container (e.g., <code>- /mnt/storage:/media</code>).</li> <li>Use Case: Essential for accessing massive datasets managed by the host OS (like a MergerFS pool) or passing through specific hardware sockets (<code>/var/run/docker.sock</code>).</li> </ul>"},{"location":"infrastructure/docker/docker/#22-network-management-strategy","title":"2.2. Network Management Strategy","text":"<p>Containers within a project must be able to communicate.</p>"},{"location":"infrastructure/docker/docker/#strategy-1-default-bridge-network","title":"Strategy 1: Default Bridge Network","text":"<ul> <li>Definition: By default, Docker Compose creates a new \"bridge\" network for the project (e.g., <code>core_default</code>).</li> <li>Communication: All services in the <code>core</code> project can reach each other by their service name (e.g., <code>portainer</code> can ping <code>homepage</code>).</li> <li>Isolation: This network is isolated from the host and other Docker projects. Ports must be explicitly <code>published</code> (e.g., <code>\"3000:3000\"</code>) to be accessible from the outside.</li> </ul>"},{"location":"infrastructure/docker/docker/#strategy-2-host-network","title":"Strategy 2: Host Network","text":"<ul> <li>Definition: Used by <code>omada-controller</code> via <code>network_mode: host</code>.</li> <li>Communication: This mode bypasses Docker's networking isolation. The container shares the host's networking namespace.</li> <li>Behavior: The service binds directly to the host's IP address. There is no port mapping; if the service listens on port 8043, it is available on <code>HOST_IP:8043</code>.</li> <li>Use Case: Required by some applications (like Omada) for service discovery. Can offer a slight performance benefit but reduces security and can cause port conflicts on the host.</li> </ul>"},{"location":"infrastructure/docker/docker/#3-homelab-project-structure","title":"3. Homelab Project Structure","text":"<p>This diagram outlines the <code>docker-compose.yml</code> files, the services they manage, and the resources they use.</p> <pre><code>/homelab/docker/\n\u2502\n\u251c\u2500\u2500 core/\n\u2502   \u2514\u2500\u2500 docker-compose.yml\n\u2502       \u251c\u2500\u2500 services:\n\u2502       \u2502   \u251c\u2500\u2500 portainer\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 volume:  core_portainer_data (standard)\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500 volumes:\n\u2502           \u251c\u2500\u2500 (Defines standard volumes)\n\u2502\n\u251c\u2500\u2500 omada/\n\u2502   \u2514\u2500\u2500 docker-compose.yml\n\u2502       \u251c\u2500\u2500 services:\n\u2502       \u2502   \u2514\u2500\u2500 omada-controller\n\u2502       \u2502       \u251c\u2500\u2500 volume:  omada_data (external)\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500 volumes:\n\u2502           \u251c\u2500\u2500 (References external volumes)\n\u2502\n\u2514\u2500\u2500 media/\n    \u2514\u2500\u2500 docker-compose.yml\n        \u251c\u2500\u2500 services:\n        \u2502   \u2514\u2500\u2500 plex\n        \u2502       \u251c\u2500\u2500 volume: /mnt/storage/media (bind mount)\n        \u2502       \u251c\u2500\u2500 volume: plex_config (standard)\n</code></pre>"},{"location":"infrastructure/docker/docker/#4-annotated-compose-examples","title":"4. Annotated Compose Examples","text":"<p>These examples demonstrate the three primary storage strategies: Standard Volumes, External Volumes, and Hybrid Bind Mounts.</p>"},{"location":"infrastructure/docker/docker/#41-standard-volumes-coredocker-composeyml","title":"4.1. Standard Volumes (<code>core/docker-compose.yml</code>)","text":"<p>Best for self-contained apps where data is strictly internal to the container.</p> <pre><code>services:\n  portainer:\n    image: portainer/portainer-ce:2.20.2\n    container_name: portainer\n    restart: unless-stopped \n    volumes:\n      # --- Volume Definition (Standard) ---\n      # This creates a volume managed by Docker.\n      # Location: /var/lib/docker/volumes/core_portainer_data/_data\n      # WARNING: Deleted if `docker compose down -v` is run.\n      - portainer_data:/data\n\n      # --- Bind Mount (Socket) ---\n      # Special case: mapping a system file\n      - /var/run/docker.sock:/var/run/docker.sock\n    ports:\n      - \"9443:9443\"\n\n# Volume defined here lets Docker create it automatically\nvolumes:\n  portainer_data:\n</code></pre>"},{"location":"infrastructure/docker/docker/#42-external-volumes-omadadocker-composeyml","title":"4.2. External Volumes (<code>omada/docker-compose.yml</code>)","text":"<p>Best for critical databases where accidental deletion must be impossible.</p> <pre><code>services:\n  omada-controller:\n    image: mbentley/omada-controller:latest\n    container_name: omada-controller\n    restart: always\n    network_mode: host\n    volumes:\n      # --- Volume Definition (External) ---\n      # These volumes MUST be created manually first:\n      # `docker volume create omada_data`\n      - omada_data:/opt/tplink/EAPController/data\n      - omada_logs:/opt/tplink/EAPController/logs\n\n# 'external: true' tells Docker \"Don't create this, just use it.\"\n# Prevents `docker compose down -v` from deleting the data.\nvolumes:\n  omada_data:\n    external: true\n  omada_logs:\n    external: true\n</code></pre>"},{"location":"infrastructure/docker/docker/#43-hybrid-bind-mounts-named-volumes-mediadocker-composeyml","title":"4.3. Hybrid: Bind Mounts &amp; Named Volumes (<code>media/docker-compose.yml</code>)","text":"<p>Best for media servers (Plex, Jellyfin) where the \"content\" is huge and exists on the host filesystem, but the \"application state\" (metadata) is internal.</p> <pre><code>services:\n  plex:\n    image: lscr.io/linuxserver/plex:latest\n    container_name: plex\n    network_mode: host\n    environment:\n      - PUID=1000\n      - PGID=1000\n    volumes:\n      # --- Bind Mount (Content) ---\n      # Maps the host's large storage pool directly to the container.\n      # The data resides on the physical disks at /mnt/storage.\n      # Format: /host/path:/container/path\n      - /mnt/storage/media:/media\n\n      # --- Named Volume (Config) ---\n      # Keeps the Plex database (metadata, watch history) in a \n      # fast, Docker-managed volume (usually on the OS SSD).\n      - plex_config:/config\n    restart: unless-stopped\n\nvolumes:\n  plex_config:\n</code></pre>"},{"location":"infrastructure/docker/docker/#5-docker-cheat-sheet","title":"5. Docker Cheat Sheet","text":""},{"location":"infrastructure/docker/docker/#docker-compose-project-level","title":"Docker Compose (Project-level)","text":"<p>Manages the lifecycle of services in <code>docker-compose.yml</code>.</p> Command Description <code>docker compose up -d</code> Start/recreate all services in detached (background) mode. <code>docker compose down</code> Stop and remove all containers, networks. <code>docker compose down -v</code> DANGER: Stops, removes containers AND deletes standard (non-external) volumes. <code>docker compose pull</code> Pulls the latest images for all services defined in the file. <code>docker compose logs -f</code> Follow (stream) the logs for all services in the project."},{"location":"infrastructure/docker/docker/#docker-volume-data","title":"Docker Volume (Data)","text":"<p>Manages persistent data volumes.</p> Command Description <code>docker volume ls</code> Lists all named volumes on system. <code>docker volume create &lt;name&gt;</code> (Manual step for External Volumes) Creates a new named volume. <code>docker volume rm &lt;name&gt;</code> Deletes a named volume. Data will be lost. (Cannot delete if in use). <code>docker volume inspect &lt;name&gt;</code> Shows details about a volume, including its on-disk mountpoint."},{"location":"infrastructure/docker/docker/#6-workflows","title":"6. Workflows","text":""},{"location":"infrastructure/docker/docker/#61-deploying-a-new-stack","title":"6.1. Deploying a New Stack","text":"<ol> <li>Create Volumes: Manually create the volumes first.</li> </ol> <p><pre><code>docker volume create omada_data\ndocker volume create omada_logs\n</code></pre> OR </p> <p><code>mkdir /docker/project/testfolder1</code> - for bind mounts (if not already made)</p> <ol> <li>Deploy Stack:</li> </ol> <p><pre><code>docker compose up -d\n</code></pre> 3. Check it's running</p> <p><code>docker ps -a</code></p>"},{"location":"infrastructure/docker/docker/#62-backing-up-a-volume","title":"6.2. Backing Up a Volume","text":"<p>This command runs a temporary container to <code>tar</code> the contents of a volume into a backup file on the host.</p> <pre><code># Backup 'omada_data' to current directory\ndocker run --rm \\\n  -v omada_data:/data:ro \\\n  -v $(pwd):/backup \\\n  alpine \\\n  tar czf /backup/omada_backup.tar.gz -C /data .\n</code></pre>"},{"location":"infrastructure/docker/docker/#7-official-documentation-links","title":"7. Official Documentation Links","text":"<ul> <li>Docker Volumes: https://docs.docker.com/storage/volumes/</li> <li>Bind Mounts: https://docs.docker.com/storage/bind-mounts/</li> <li>Docker Compose File Reference: https://docs.docker.com/compose/compose-file/</li> <li>Compose <code>network_mode</code>: https://docs.docker.com/compose/compose-file/05-services/#network_mode</li> </ul>"},{"location":"infrastructure/docker/dockerinstall/","title":"Install Docker Engine on Debian","text":"<p>To get started with Docker Engine on Debian, make sure you meet the prerequisites, and then follow the installation steps.</p>"},{"location":"infrastructure/docker/dockerinstall/#prerequisites","title":"Prerequisites","text":""},{"location":"infrastructure/docker/dockerinstall/#firewall-limitations","title":"Firewall limitations","text":"<p>[!WARNING]</p> <p>Before you install Docker, make sure you consider the following security implications and firewall incompatibilities.</p> <ul> <li>If you use ufw or firewalld to manage firewall settings, be aware that   when you expose container ports using Docker, these ports bypass your   firewall rules. For more information, refer to   Docker and ufw.</li> <li>Docker is only compatible with <code>iptables-nft</code> and <code>iptables-legacy</code>.   Firewall rules created with <code>nft</code> are not supported on a system with Docker installed.   Make sure that any firewall rulesets you use are created with <code>iptables</code> or <code>ip6tables</code>,   and that you add them to the <code>DOCKER-USER</code> chain,   see Packet filtering and firewalls.</li> </ul>"},{"location":"infrastructure/docker/dockerinstall/#os-requirements","title":"OS requirements","text":"<p>To install Docker Engine, you need one of these Debian versions:</p> <ul> <li>Debian Trixie 13 (stable)</li> <li>Debian Bookworm 12 (oldstable)</li> <li>Debian Bullseye 11 (oldoldstable)</li> </ul> <p>Docker Engine for Debian is compatible with x86_64 (or amd64), armhf (arm/v7), arm64, and ppc64le (ppc64el) architectures.</p>"},{"location":"infrastructure/docker/dockerinstall/#uninstall-old-versions","title":"Uninstall old versions","text":"<p>Before you can install Docker Engine, you need to uninstall any conflicting packages.</p> <p>Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.</p> <p>The unofficial packages to uninstall are:</p> <ul> <li><code>docker.io</code></li> <li><code>docker-compose</code></li> <li><code>docker-doc</code></li> <li><code>podman-docker</code></li> </ul> <p>Moreover, Docker Engine depends on <code>containerd</code> and <code>runc</code>. Docker Engine bundles these dependencies as one bundle: <code>containerd.io</code>. If you have installed the <code>containerd</code> or <code>runc</code> previously, uninstall them to avoid conflicts with the versions bundled with Docker Engine.</p> <p>Run the following command to uninstall all conflicting packages:</p> <pre><code>$ sudo apt remove $(dpkg --get-selections docker.io docker-compose docker-doc podman-docker containerd runc | cut -f1)\n</code></pre> <p><code>apt</code> might report that you have none of these packages installed.</p> <p>Images, containers, volumes, and networks stored in <code>/var/lib/docker/</code> aren't automatically removed when you uninstall Docker. If you want to start with a clean installation, and prefer to clean up any existing data, read the uninstall Docker Engine section.</p>"},{"location":"infrastructure/docker/dockerinstall/#installation-methods","title":"Installation methods","text":"<p>You can install Docker Engine in different ways, depending on your needs:</p> <ul> <li> <p>Docker Engine comes bundled with   Docker Desktop for Linux. This is   the easiest and quickest way to get started.</p> </li> <li> <p>Set up and install Docker Engine from   Docker's <code>apt</code> repository.</p> </li> <li> <p>Install it manually and manage upgrades manually.</p> </li> <li> <p>Use a convenience script. Only   recommended for testing and development environments.</p> </li> </ul> <p>Apache License, Version 2.0. See LICENSE for the full license.</p>"},{"location":"infrastructure/docker/dockerinstall/#install-using-the-repository","title":"Install using the <code>apt</code> repository","text":"<p>Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker <code>apt</code> repository. Afterward, you can install and update Docker from the repository.</p> <ol> <li>Set up Docker's <code>apt</code> repository.</li> </ol> <pre><code># Add Docker's official GPG key:\nsudo apt update\nsudo apt install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\nsudo tee /etc/apt/sources.list.d/docker.sources &lt;&lt;EOF\nTypes: deb\nURIs: https://download.docker.com/linux/debian\nSuites: $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\nComponents: stable\nSigned-By: /etc/apt/keyrings/docker.asc\nEOF\n\nsudo apt update\n</code></pre> <p>[!NOTE]</p> <p>If you use a derivative distribution, such as Kali Linux, you may need to substitute the part of this command that's expected to print the version codename:</p> <pre><code>$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\n</code></pre> <p>Replace this part with the codename of the corresponding Debian release, such as <code>bookworm</code>.</p> <ol> <li>Install the Docker packages.</li> </ol> <p>Latest</p> <p>To install the latest version, run:</p> <pre><code>$ sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Specific version</p> <p>To install a specific version of Docker Engine, start by listing the    available versions in the repository:</p> <pre><code>$ apt list --all-versions docker-ce\n\ndocker-ce/bookworm 5:29.1.5-1~debian.12~bookworm &lt;arch&gt;\ndocker-ce/bookworm 5:29.1.4-1~debian.12~bookworm &lt;arch&gt;\n...\n</code></pre> <p>Select the desired version and install:</p> <pre><code>$ VERSION_STRING=5:29.1.5-1~debian.12~bookworm\n$ sudo apt install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <pre><code>&gt; [!NOTE]\n&gt;\n&gt; The Docker service starts automatically after installation. To verify that\n&gt; Docker is running, use:\n&gt; \n&gt; ```console\n&gt; $ sudo systemctl status docker\n&gt; ```\n&gt;\n&gt; Some systems may have this behavior disabled and will require a manual start:\n&gt;\n&gt; ```console\n&gt; $ sudo systemctl start docker\n&gt; ```\n</code></pre> <ol> <li>Verify that the installation is successful by running the <code>hello-world</code> image:</li> </ol> <pre><code>$ sudo docker run hello-world\n</code></pre> <p>This command downloads a test image and runs it in a container. When the    container runs, it prints a confirmation message and exits.</p> <p>You have now successfully installed and started Docker Engine.</p> <p>[!TIP]</p> <p>Receiving errors when trying to run without root?</p> <p>The <code>docker</code> user group exists but contains no users, which is why you\u2019re required  to use <code>sudo</code> to run Docker commands. Continue to Linux postinstall  to allow non-privileged users to run Docker commands and for other optional configuration steps.</p>"},{"location":"infrastructure/docker/dockerinstall/#upgrade-docker-engine","title":"Upgrade Docker Engine","text":"<p>To upgrade Docker Engine, follow step 2 of the installation instructions, choosing the new version you want to install.</p>"},{"location":"infrastructure/docker/dockerinstall/#install-from-a-package","title":"Install from a package","text":"<p>If you can't use Docker's <code>apt</code> repository to install Docker Engine, you can download the <code>deb</code> file for your release and install it manually. You need to download a new file each time you want to upgrade Docker Engine.</p> <ol> <li> <p>Go to <code>https://download.docker.com/linux/debian/dists/</code>.</p> </li> <li> <p>Select your Debian version in the list.</p> </li> <li> <p>Go to <code>pool/stable/</code> and select the applicable architecture (<code>amd64</code>,    <code>armhf</code>, <code>arm64</code>, or <code>s390x</code>).</p> </li> <li> <p>Download the following <code>deb</code> files for the Docker Engine, CLI, containerd,    and Docker Compose packages:</p> </li> <li> <p><code>containerd.io_&lt;version&gt;_&lt;arch&gt;.deb</code></p> </li> <li><code>docker-ce_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-ce-cli_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li><code>docker-buildx-plugin_&lt;version&gt;_&lt;arch&gt;.deb</code></li> <li> <p><code>docker-compose-plugin_&lt;version&gt;_&lt;arch&gt;.deb</code></p> </li> <li> <p>Install the <code>.deb</code> packages. Update the paths in the following example to    where you downloaded the Docker packages.</p> </li> </ol> <pre><code>$ sudo dpkg -i ./containerd.io_&lt;version&gt;_&lt;arch&gt;.deb \\\n  ./docker-ce_&lt;version&gt;_&lt;arch&gt;.deb \\\n  ./docker-ce-cli_&lt;version&gt;_&lt;arch&gt;.deb \\\n  ./docker-buildx-plugin_&lt;version&gt;_&lt;arch&gt;.deb \\\n  ./docker-compose-plugin_&lt;version&gt;_&lt;arch&gt;.deb\n</code></pre> <pre><code>&gt; [!NOTE]\n&gt;\n&gt; The Docker service starts automatically after installation. To verify that\n&gt; Docker is running, use:\n&gt; \n&gt; ```console\n&gt; $ sudo systemctl status docker\n&gt; ```\n&gt;\n&gt; Some systems may have this behavior disabled and will require a manual start:\n&gt;\n&gt; ```console\n&gt; $ sudo systemctl start docker\n&gt; ```\n</code></pre> <ol> <li>Verify that the installation is successful by running the <code>hello-world</code> image:</li> </ol> <pre><code>$ sudo docker run hello-world\n</code></pre> <p>This command downloads a test image and runs it in a container. When the    container runs, it prints a confirmation message and exits.</p> <p>You have now successfully installed and started Docker Engine.</p> <p>[!TIP]</p> <p>Receiving errors when trying to run without root?</p> <p>The <code>docker</code> user group exists but contains no users, which is why you\u2019re required  to use <code>sudo</code> to run Docker commands. Continue to Linux postinstall  to allow non-privileged users to run Docker commands and for other optional configuration steps.</p>"},{"location":"infrastructure/docker/dockerinstall/#upgrade-docker-engine_1","title":"Upgrade Docker Engine","text":"<p>To upgrade Docker Engine, download the newer package files and repeat the installation procedure, pointing to the new files.</p>"},{"location":"infrastructure/docker/dockerinstall/#install-using-the-convenience-script","title":"Install using the convenience script","text":"<p>Docker provides a convenience script at https://get.docker.com/ to install Docker into development environments non-interactively. The convenience script isn't recommended for production environments, but it's useful for creating a provisioning script tailored to your needs. Also refer to the install using the repository steps to learn about installation steps to install using the package repository. The source code for the script is open source, and you can find it in the <code>docker-install</code> repository on GitHub.</p> <p>Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:</p> <ul> <li>The script requires <code>root</code> or <code>sudo</code> privileges to run.</li> <li>The script attempts to detect your Linux distribution and version and   configure your package management system for you.</li> <li>The script doesn't allow you to customize most installation parameters.</li> <li>The script installs dependencies and recommendations without asking for   confirmation. This may install a large number of packages, depending on the   current configuration of your host machine.</li> <li>By default, the script installs the latest stable release of Docker,   containerd, and runc. When using this script to provision a machine, this may   result in unexpected major version upgrades of Docker. Always test upgrades in   a test environment before deploying to your production systems.</li> <li>The script isn't designed to upgrade an existing Docker installation. When   using the script to update an existing installation, dependencies may not be   updated to the expected version, resulting in outdated versions.</li> </ul> <p>[!TIP]</p> <p>Preview script steps before running. You can run the script with the <code>--dry-run</code> option to learn what steps the script will run when invoked:</p> <pre><code>$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh ./get-docker.sh --dry-run\n</code></pre> <p>This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:</p> <pre><code>$ curl -fsSL https://get.docker.com -o get-docker.sh\n$ sudo sh get-docker.sh\nExecuting docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737\n&lt;...&gt;\n</code></pre> <p>You have now successfully installed and started Docker Engine. The <code>docker</code> service starts automatically on Debian based distributions. On <code>RPM</code> based distributions, such as CentOS, Fedora or RHEL, you need to start it manually using the appropriate <code>systemctl</code> or <code>service</code> command. As the message indicates, non-root users can't run Docker commands by default.</p> <p>Use Docker as a non-privileged user, or install in rootless mode?</p> <p>The installation script requires <code>root</code> or <code>sudo</code> privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker without <code>root</code> privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).</p>"},{"location":"infrastructure/docker/dockerinstall/#install-pre-releases","title":"Install pre-releases","text":"<p>Docker also provides a convenience script at https://test.docker.com/ to install pre-releases of Docker on Linux. This script is equal to the script at <code>get.docker.com</code>, but configures your package manager to use the test channel of the Docker package repository. The test channel includes both stable and pre-releases (beta versions, release-candidates) of Docker. Use this script to get early access to new releases, and to evaluate them in a testing environment before they're released as stable.</p> <p>To install the latest version of Docker on Linux from the test channel, run:</p> <pre><code>$ curl -fsSL https://test.docker.com -o test-docker.sh\n$ sudo sh test-docker.sh\n</code></pre>"},{"location":"infrastructure/docker/dockerinstall/#upgrade-docker-after-using-the-convenience-script","title":"Upgrade Docker after using the convenience script","text":"<p>If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.</p>"},{"location":"infrastructure/docker/dockerinstall/#uninstall-docker-engine","title":"Uninstall Docker Engine","text":"<ol> <li>Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:</li> </ol> <pre><code>$ sudo apt purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras\n</code></pre> <ol> <li>Images, containers, volumes, or custom configuration files on your host    aren't automatically removed. To delete all images, containers, and volumes:</li> </ol> <pre><code>$ sudo rm -rf /var/lib/docker\n$ sudo rm -rf /var/lib/containerd\n</code></pre> <ol> <li>Remove source list and keyrings</li> </ol> <pre><code>$ sudo rm /etc/apt/sources.list.d/docker.sources\n$ sudo rm /etc/apt/keyrings/docker.asc\n</code></pre> <p>You have to delete any edited configuration files manually.</p>"},{"location":"infrastructure/docker/dockerinstall/#next-steps","title":"Next steps","text":"<ul> <li>Continue to Post-installation steps for Linux.</li> </ul>"},{"location":"infrastructure/filesystem/management/","title":"Filesystem Management Guide","text":""},{"location":"infrastructure/filesystem/management/#overview","title":"Overview","text":"<p>General idea on managing filesystems. This shows specifically <code>ext4</code> and <code>xfs</code> as that's what I used, but there are plently of other options. </p>"},{"location":"infrastructure/filesystem/management/#1-identification","title":"1. Identification","text":"<p>Before modifying any disks, identifying the correct device identifiers is critical to prevent data loss.</p> <pre><code>lsblk -o NAME,MODEL,SIZE,TYPE,FSTYPE\n</code></pre> <p>Reference Output:</p> <pre><code>NAME                      MODEL                            SIZE TYPE FSTYPE\nsda                       SanDisk SD8TB8U2               238.5G disk \n\u2514\u2500sda1                                                   238.5G part ext4\nsdb                       WDC WD120EFGX-68                10.9T disk \n\u2514\u2500sdb1                                                    10.9T part xfs\nsdc                       WDC WD120EFGX-68                10.9T disk \n\u2514\u2500sdc1                                                    10.9T part xfs\nsdd                       WDC WD120EFGX-68                10.9T disk \n\u2514\u2500sdd1                                                    10.9T part xfs\nnvme0n1                   WDC PC SN730 SDBQNTY-256G-1001 238.5G disk \n\u251c\u2500nvme0n1p1                                                  1M part \n\u251c\u2500nvme0n1p2                                                  2G part ext4\n\u2514\u2500nvme0n1p3                                              236.5G part LVM2_member\n  \u2514\u2500ubuntu--vg-ubuntu--lv                                  100G lvm  ext4\n</code></pre>"},{"location":"infrastructure/filesystem/management/#2-partitioning-formatting","title":"2. Partitioning &amp; Formatting","text":"<p>You can choose between a scriptable command-line approach or an interactive visual tool.</p>"},{"location":"infrastructure/filesystem/management/#option-a-command-line-scriptable","title":"Option A: Command Line (Scriptable)","text":""},{"location":"infrastructure/filesystem/management/#ssd-setup-ext4","title":"SSD Setup (ext4)","text":"<p>Target: <code>/dev/sda</code> (Appdata/Cache)</p> <pre><code># 1. Wipe old filesystem signatures\nsudo wipefs -a /dev/sda\n\n# Use parted for GPT specifically (Recommended for drives &gt;2TB)\nsudo parted -s /dev/sda mklabel gpt \n\n# 3. Format to EXT4\nsudo mkfs.ext4 -L \"appdata\" /dev/sda1\n</code></pre>"},{"location":"infrastructure/filesystem/management/#hdd-setup-xfs","title":"HDD Setup (XFS)","text":"<p>Target: <code>/dev/sdb</code>, <code>/dev/sdc</code>, <code>/dev/sdd</code> (Mass Storage)</p> <pre><code># Drive 1 (sdb)\nsudo wipefs -a /dev/sdb\nsudo parted -s /dev/sdb mkpart primary xfs 0% 100%\nsudo mkfs.xfs -f -L \"disk1\" /dev/sdb1\n\n# Drive 2 (sdc)\nsudo wipefs -a /dev/sdc\nsudo parted -s /dev/sdc mkpart primary xfs 0% 100%\nsudo mkfs.xfs -f -L \"disk2\" /dev/sdc1\n\n# Drive 3 (sdd)\nsudo wipefs -a /dev/sdd\nsudo parted -s /dev/sdd mkpart primary xfs 0% 100%\nsudo mkfs.xfs -f -L \"parity1\" /dev/sdd1\n</code></pre>"},{"location":"infrastructure/filesystem/management/#option-b-interactive-visual","title":"Option B: Interactive (Visual)","text":"<p>For a visual interface, use <code>cfdisk</code>.</p> <p>Formatting Required</p> <p><code>cfdisk</code> only creates the partition. You must still run the <code>mkfs</code> commands listed in Option A to format the filesystem after creating the partitions.</p> <ol> <li>Launch Tool: <code>sudo cfdisk /dev/sda</code> (Repeat for <code>sdb</code>, <code>sdc</code>, <code>sdd</code>).</li> <li>Label Type: Select gpt.</li> <li>Delete: Remove existing partitions if necessary.</li> <li>New: Select [ New ] -&gt; [ Enter ] (Use max size).</li> <li>Write: Select [ Write ] -&gt; Type <code>yes</code>.</li> <li>Quit: Select [ Quit ].</li> </ol>"},{"location":"infrastructure/filesystem/management/#3-mounting-persistence","title":"3. Mounting &amp; Persistence","text":""},{"location":"infrastructure/filesystem/management/#get-uuids","title":"Get UUIDs","text":"<p>Use UUIDs for mounting. Unlike <code>/dev/sdX</code> names, UUIDs do not change if drive order changes. You'll need to map them using these or they may not show on reboot</p> <pre><code>ls -l /dev/disk/by-uuid/\n</code></pre> <p>Output:</p> <pre><code>lrwxrwxrwx 1 root root 10 Dec  7 05:34 02de315f-8ced-4745-bb4f-2d24c46efa48 -&gt; ../../sdb1\nlrwxrwxrwx 1 root root 15 Dec  7 04:33 3c26dce0-7f10-426c-bd67-e9f2d77889ee -&gt; ../../nvme0n1p2\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 603999da-c2dd-484d-80bf-ab4977680e90 -&gt; ../../sdc1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 844cae61-9599-4a57-9431-18c0a33904e4 -&gt; ../../sda1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 9860056e-b314-4cb9-acc5-30e76281795b -&gt; ../../sdd1\nlrwxrwxrwx 1 root root 10 Dec  7 04:33 d5ac7faa-876c-43ff-b27a-94c53bf79bb2 -&gt; ../../dm-0\n</code></pre>"},{"location":"infrastructure/filesystem/management/#create-mount-points","title":"Create Mount Points","text":"<p>Create the directories where the drives will be accessed.</p> <pre><code>sudo mkdir -p /mnt/disk1\nsudo mkdir -p /mnt/disk2\nsudo mkdir -p /mnt/parity1\nsudo mkdir -p /mnt/appdata\n</code></pre>"},{"location":"infrastructure/filesystem/management/#edit-fstab","title":"Edit fstab","text":"<p>Add the following to the bottom of <code>/etc/fstab</code> to ensure drives mount automatically at boot.</p> <pre><code># ---------------------------------------------------------\n# DATA DRIVES (XFS) - 12TB WD Reds\n# ---------------------------------------------------------\n# Disk 1 (sdb1)\nUUID=02de315f-8ced-4745-bb4f-2d24c46efa48 /mnt/disk1 xfs defaults,noatime 0 0\n\n# Disk 2 (sdc1)\nUUID=603999da-c2dd-484d-80bf-ab4977680e90 /mnt/disk2 xfs defaults,noatime 0 0\n\n# Parity Drive (sdd1)\nUUID=9860056e-b314-4cb9-acc5-30e76281795b /mnt/parity1 xfs defaults,noatime 0 0\n\n# ---------------------------------------------------------\n# FAST STORAGE (EXT4) - 240GB SSD\n# ---------------------------------------------------------\n# Appdata/Cache (sda1)\nUUID=844cae61-9599-4a57-9431-18c0a33904e4 /mnt/appdata ext4 defaults,noatime 0 0\n</code></pre>"},{"location":"infrastructure/filesystem/management/#reload-configuration","title":"Reload Configuration","text":"<p>After editing <code>fstab</code>, verify the syntax and reload system daemons.</p> <pre><code>sudo findmnt --verify\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"infrastructure/filesystem/management/#4-verification","title":"4. Verification","text":"<p>Perform these checks before rebooting to ensure the configuration is valid.</p> <ol> <li> <p>Test Mount: <pre><code># No output is good news\nsudo mount -a\n</code></pre></p> </li> <li> <p>Check Sizing: <pre><code>df -h\n</code></pre></p> </li> <li> <p><code>disk1</code>, <code>disk2</code>, <code>parity1</code>: Should be ~11T</p> </li> <li> <p><code>appdata</code>: Should be ~220G</p> </li> <li> <p>Check Permissions: <pre><code>ls -ld /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p> </li> </ol> <p>If listed as <code>root</code>, change ownership to your specific user (e.g., <code>goose</code>): <pre><code>sudo chown -R goose:goose /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p>"},{"location":"infrastructure/filesystem/management/#5-docker-integration-reference","title":"5. Docker Integration Reference","text":"<p>Once mounted, pass these paths to containers via <code>compose.yaml</code>.</p>"},{"location":"infrastructure/filesystem/management/#filebrowser-file-management","title":"FileBrowser (File Management)","text":"<pre><code>services:\n  filebrowser:\n    volumes:\n      # Map Host Path : Container Path\n      - /mnt/nvme_storage:/mnt/nvme_storage\n</code></pre>"},{"location":"infrastructure/filesystem/management/#beszel-system-monitoring","title":"Beszel (System Monitoring)","text":"<pre><code>services:\n  beszel-agent:\n    volumes:\n      - /mnt/nvme_storage:/mnt/nvme_storage:ro\n    environment:\n      # Comma-separated list of mount points to monitor\n      - EXTRA_FILESYSTEMS=/mnt/nvme_storage\n</code></pre>"},{"location":"infrastructure/filesystem/management/#6-general-cli-cheat-sheet","title":"6. General CLI Cheat Sheet","text":""},{"location":"infrastructure/filesystem/management/#identification-hardware-info","title":"Identification &amp; Hardware Info","text":"Command Description <code>lsblk</code> List Block Devices. The best \"at a glance\" view of disks, partitions, and mount points. <code>lsblk -f</code> Lists devices plus filesystem types (ext4, ntfs) and UUIDs. <code>sudo fdisk -l</code> Detailed low-level partition table dump. Good for verifying sector sizes. <code>sudo lshw -class disk</code> Hardware details (model numbers, serial numbers, firmware). <code>sudo blkid</code> Prints the UUIDs and labels for all block devices."},{"location":"infrastructure/filesystem/management/#disk-usage-space","title":"Disk Usage &amp; Space","text":"Command Description <code>df -h</code> Disk Free. Shows total/used/avail space in \"Human Readable\" format (GB/TB). <code>du -sh /path/to/dir</code> Disk Usage. Calculates the size of a specific directory. <code>ncdu</code> Interactive Usage. A visual, navigable tool to find large files. (Requires install)."},{"location":"infrastructure/filesystem/management/#health-maintenance","title":"Health &amp; Maintenance","text":"Command Description <code>sudo smartctl -a /dev/sda</code> SMART Data. Checks physical health, temp, and error logs (Requires <code>smartmontools</code>). <code>sudo fsck /dev/sda1</code> File System Check. Repairs corruption. Only run on unmounted drives."},{"location":"infrastructure/filesystem/management/#mounting-operations","title":"Mounting Operations","text":"Command Description <code>sudo mount /dev/sdb1 /mnt/usb</code> Manually mounts a partition to a folder. <code>sudo umount /mnt/usb</code> Unmounts the drive safely. <code>sudo mount -a</code> Mounts everything listed in <code>/etc/fstab</code> that isn't already mounted."},{"location":"infrastructure/filesystem/mergerfs/","title":"MergerFS Configuration Guide","text":""},{"location":"infrastructure/filesystem/mergerfs/#overview","title":"Overview","text":"<p>MergerFS is a union filesystem designed to simplify storage management. It pools multiple storage devices (branches) into a single unified directory (mount point).</p> <p>Unlike RAID, MergerFS does not strip data across drives. Files exist intact on the individual backing drives.</p> <ul> <li>Flexibility: Drives of different sizes, filesystems, and speeds can be mixed.</li> <li>Resilience: If one drive fails, only the data on that specific drive is lost; the rest of the pool remains accessible.</li> <li>Dynamic: Drives can be added or removed from the pool without rebuilding the array.</li> </ul>"},{"location":"infrastructure/filesystem/mergerfs/#installation","title":"Installation","text":"<p>MergerFS is available in most standard Linux repositories, though building from source ensures the latest feature set.</p> <pre><code># Standard installation\nsudo apt install mergerfs fuse\n</code></pre> <p>For latest versions/source: GitHub Repository</p>"},{"location":"infrastructure/filesystem/mergerfs/#branch-setup-preparation","title":"Branch Setup &amp; Preparation","text":"<p>Warning</p> <p>Never write data directly to the <code>/mnt/disk1</code> folders if you can help it. Always write to <code>/mnt/storage</code>. If you modify files on the backing disks directly while MergerFS is running, you might see cache inconsistencies until you remount.</p> <p>Before creating the pool, the underlying mount points (branches) should be organized and secured.</p>"},{"location":"infrastructure/filesystem/mergerfs/#1-naming-convention","title":"1. Naming Convention","text":"<p>Using a consistent naming structure helps identify drive types within the pool.</p> <ul> <li>HDDs: <code>/mnt/hdd/Serial-Number</code></li> <li>SATA SSDs: <code>/mnt/ssd/Serial-Number</code></li> <li>NVMe: <code>/mnt/nvme/Serial-Number</code></li> <li>Remote Shares: <code>/mnt/remote/Share-Name</code></li> </ul> <p>Example Layout:</p> <pre><code>/mnt/\n\u251c\u2500\u2500 hdd/\n\u2502   \u251c\u2500\u2500 10T-01234567\n\u2502   \u2514\u2500\u2500 20T-12345678\n\u2514\u2500\u2500 nvme/\n    \u2514\u2500\u2500 1T-ABCDEFGH\n</code></pre>"},{"location":"infrastructure/filesystem/mergerfs/#2-permissions-locking-critical","title":"2. Permissions &amp; Locking (Critical)","text":""},{"location":"infrastructure/filesystem/mergerfs/#mount-points","title":"mount points","text":"<p>To ensure the directory is only used as a point to mount another filesystem it is good to lock it down as much as possible. Be sure to do this before mounting a filesystem to it.</p> <p>Run these commands for each underlying drive mount point:</p> <pre><code># 1. Set ownership to root\nsudo chown root:root /mnt/hdd/10T-XYZ\n\n# 2. Remove read/write/execute permissions (makes it inaccessible directly)\nsudo chmod 0000 /mnt/hdd/10T-XYZ\n\n# 3. Mark as a specific branch mount (prevents mounting if drive is missing)\nsudo setfattr -n user.mergerfs.branch_mounts_here\n</code></pre> <p>The extended attribute user.mergerfs.branch_mounts_here is used by the branches-mount-timeout option to recognize whether or not a mergerfs branch path points to the intended filesystem.</p> <p>The chattr is likely to only work on EXT{2,3,4} filesystems but will restrict even root from modifying the directory or its content but is still able to be a mount point.</p>"},{"location":"infrastructure/filesystem/mergerfs/#mounted-filesystems","title":"mounted filesystems","text":"<p>For those new to Linux, intending to be the primary individual logged into the system, or simply want to simplify permissions it is recommended to set the root of mounted filesystems like /tmp/ is set to. Owned by root, ugo+rwx and sticky bit set.</p> <p>This must be done after mounting the filesystem to the target mount point.</p> <pre><code>$ sudo chown root:root /mnt/hdd/10T-SERIALNUM\n$ sudo chmod 1777 /mnt/hdd/10T-SERIALNUM\n$ sudo setfattr -n user.mergerfs.branch /mnt/hdd/10T-SERIALNUM\n</code></pre>"},{"location":"infrastructure/filesystem/mergerfs/#pool-configuration","title":"Pool Configuration","text":""},{"location":"infrastructure/filesystem/mergerfs/#1-create-pool-directory","title":"1. Create Pool Directory","text":"<p>Info</p> <p>For Linux v6.6 and above (defaults - quick start)</p> <p>-cache.files=off</p> <p>-category.create=pfrd</p> <p>-func.getattr=newest</p> <p>-dropcacheonclose=false</p> <p>Create the unified mount point where applications will access the data.</p> <pre><code>sudo mkdir -p /mnt/storage\n</code></pre>"},{"location":"infrastructure/filesystem/mergerfs/#2-command-line-test","title":"2. Command Line Test","text":"<p>Test the pool creation manually before making it persistent.</p> <p>Note</p> <p>These settings are just what I used. Your situation may benefit from different layout. Read the docs to see what would work best. </p> <pre><code>mergerfs -o cache.files=off,category.create=mspmfs,func.getattr=newest,dropcacheonclose=false,minfreespace=200G,moveonenospc=true /mnt/disk*/mnt /mnt/storage\n</code></pre>"},{"location":"infrastructure/filesystem/mergerfs/#3-persistence-etcfstab","title":"3. Persistence (<code>/etc/fstab</code>)","text":"<p>Add the configuration to <code>/etc/fstab</code> to ensure the pool mounts at boot.</p> <p>Recommended Options (Linux Kernel v6.6+):</p> <ul> <li><code>cache.files=off</code>: Disables page caching (reduces RAM usage/complexity).</li> <li><code>category.create=mspmfs</code>: Most Space, Path Most Free Space. Writes new files to the drive with the most free space, unless the path already exists on another drive.</li> <li><code>func.getattr=newest</code>: Returns file attributes from the file with the newest <code>mtime</code>. Critical for apps like Plex/Kodi to detect changes.</li> <li><code>minfreespace=200G</code>: Prevents filling a drive completely; moves to the next drive when 200GB remains.</li> </ul> <p>Add to <code>/etc/fstab</code>:</p> <pre><code># &lt;branches&gt; &lt;mountpoint&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;\n/mnt/hdd/WD-B00WHLZD:/mnt/hdd/WD-B00WUHXD /mnt/storage mergerfs cache.files=off,category.create=mspmfs,func.getattr=newest,dropcacheonclose=false,minfreespace=200G,moveonenospc=true,fsname=mergerfs 0 0\n</code></pre>"},{"location":"infrastructure/filesystem/mergerfs/#4-verification","title":"4. Verification","text":"<p>Reload the system daemon and mount the pool.</p> <pre><code># Verify syntax\nsudo findmnt --verify\n\n# Reload fstab\nsudo systemctl daemon-reload\n\n# Mount all\nsudo mount -a\n\n# Check size (Should equal sum of all drives)\ndf -h\n</code></pre>"},{"location":"infrastructure/filesystem/mergerfs/#docker-integration","title":"Docker Integration","text":"<p>When configuring containers, always use the unified <code>/mnt/storage</code> path. Never bind volumes to the individual disks (e.g., <code>/mnt/disk1</code>), as this bypasses MergerFS logic.</p> <p>Example <code>compose.yaml</code>:</p> <pre><code>services:\n  plex:\n    image: linuxserver/plex\n    volumes:\n      # Correct: Uses the pool\n      - /mnt/storage/media/movies:/movies\n      - /mnt/storage/media/tv:/tv\n\n      # INCORRECT: Do not map directly to backing disks\n      # - /mnt/disk1/media/movies:/movies \n</code></pre>"},{"location":"infrastructure/filesystem/mergerfs/#maintenance-troubleshooting","title":"Maintenance &amp; Troubleshooting","text":""},{"location":"infrastructure/filesystem/mergerfs/#handling-cacheddeleted-files","title":"Handling Cached/Deleted Files","text":"<p>If a file is deleted from the pool but appears to persist (ghost file), or if changes aren't reflecting immediately, the filesystem cache may need clearing.</p> <pre><code>sync &amp;&amp; echo 3 | sudo tee /proc/sys/vm/drop_caches\n</code></pre> <ul> <li>Note: Deleting a file from the pool removes it from the underlying drive.</li> </ul>"},{"location":"infrastructure/filesystem/mergerfs/#common-issues","title":"Common Issues","text":"<ul> <li>Permissions: Run MergerFS as <code>root</code>. Running as a standard user can cause unpredictable behavior.</li> <li>Missing Files: If directories appear missing or permissions seem erratic, ensure the permissions on the underlying physical drives are identical. Use <code>mergerfs.fsck</code> to audit synchronization.</li> <li>Scanner Lag (Plex/Kodi): If media scanners miss new files, ensure <code>func.getattr=newest</code> is enabled in the options. This forces the pool to report the most recent modification time found across all branches.</li> </ul>"},{"location":"infrastructure/filesystem/snapraid/","title":"SnapRAID Configuration Guide","text":"<p>SnapRAID Official Docs</p>"},{"location":"infrastructure/filesystem/snapraid/#overview","title":"Overview","text":"<p>SnapRAID is a backup program for disk arrays. It stores parity information of the data to recover from up to six disk failures.</p> <p>Unlike traditional RAID 5 or 6, SnapRAID is not real-time; it is a snapshot-based system.</p> <ul> <li>Provide fault tolerance to protect against (inevitable) hard drive failure</li> <li>Checksum files to guard against bitrot</li> <li>Support hard drives of differing / mismatched sizes</li> <li>Enable incremental upgrading of hard drives in batches as small as one</li> <li>Each drive should have a separately readable filesystem with no striping of data</li> </ul>"},{"location":"infrastructure/filesystem/snapraid/#the-parity-concept","title":"The Parity Concept","text":"<p>SnapRAID requires dedicating specific disks to \"Parity.\"</p> <ul> <li>Parity Level: One parity disk protects against one drive failure. Two parity disks protect against two failures, and so on.</li> <li>Sizing: The parity disk must be equal to or larger than the largest single data disk in the array.</li> <li>Storage: Parity disks contain only the large <code>.parity</code> file; they cannot be used for standard data storage.</li> </ul>"},{"location":"infrastructure/filesystem/snapraid/#installation","title":"Installation","text":"<p>To ensure the latest features and compatibility, SnapRAID is often compiled from source.</p>"},{"location":"infrastructure/filesystem/snapraid/#compile-from-source","title":"Compile from Source","text":"<p>Run the following commands to download, compile, and install version 13.0.</p> <pre><code># Download source\nwget https://github.com/amadvance/snapraid/releases/download/v13.0/snapraid-13.0.tar.gz\n\n# Extract archive\ntar xzvf snapraid-13.0.tar.gz\n\n# Enter directory\ncd snapraid\n\n# Compile and Install\n./autogen.sh\n./configure\nmake\nmake check\nsudo make install\n\n# Verify installation\nsnapraid -V\n</code></pre>"},{"location":"infrastructure/filesystem/snapraid/#configuration","title":"Configuration","text":"<p>SnapRAID is configured via a single text file located at <code>/etc/snapraid.conf</code>. This file defines the parity volumes, content files, and data disks.</p>"},{"location":"infrastructure/filesystem/snapraid/#key-components","title":"Key Components","text":"<ol> <li>Parity Files: The destination for redundancy data.</li> <li>Content Files: These act as the \"index\" or \"database\" of the array. They contain checksums and file maps.</li> <li> <p>Critical: Multiple copies should be stored on separate physical disks (e.g., one on the boot drive, others on data drives) to ensure the index survives a disk failure.</p> </li> <li> <p>Data Disks: The actual storage drives containing the media/files to protect.</p> </li> </ol>"},{"location":"infrastructure/filesystem/snapraid/#configuration-example-etcsnapraidconf","title":"Configuration Example (<code>/etc/snapraid.conf</code>)","text":"<p>Create or edit the file using a text editor:</p> <pre><code># --- Parity Location --- \n# The parity file must be on the dedicated parity drive\nparity /mnt/parity1/snapraid.parity \n\n# --- Content files ---\n# Store copies on multiple physical drives for safety\ncontent /var/snapraid/snapraid.content\ncontent /mnt/disk1/snapraid.content\ncontent /mnt/disk2/snapraid.content\n\n# --- Data Disks ---\n# Assign a name (d1, d2) and a mount point for each disk\ndata d1 /mnt/disk1\ndata d2 /mnt/disk2\n\n# --- Excludes ---\n# Prevent temporary files from breaking the sync\nexclude *.unrecoverable\nexclude /tmp/\nexclude /lost+found/\n</code></pre>"},{"location":"infrastructure/filesystem/snapraid/#usage-maintenance","title":"Usage &amp; Maintenance","text":"<p>Because SnapRAID is snapshot-based, the array is not protected until a \"sync\" is performed.</p>"},{"location":"infrastructure/filesystem/snapraid/#syncing","title":"Syncing","text":"<p>Run the <code>sync</code> command to update the parity information. This reads data from the data disks and computes the parity.</p> <pre><code>snapraid sync\n</code></pre> <ul> <li>Note: The first sync may take several hours. Subsequent syncs only process changed data.</li> <li>Interruptible: The process can be stopped (<code>Ctrl+C</code>) and resumed later.</li> </ul>"},{"location":"infrastructure/filesystem/snapraid/#scrubbing","title":"Scrubbing","text":"<p>Scrubbing checks the data and parity for silent corruption (bitrot).</p> <pre><code>snapraid scrub\n</code></pre> <ul> <li>Default Behavior: Checks approximately 8% of the array that hasn't been scrubbed in the last 10 days.</li> <li>Custom Plan: To check a specific percentage (e.g., 5%) of blocks older than 20 days: <pre><code>snapraid -p 5 -o 20 scrub\n</code></pre></li> </ul>"},{"location":"infrastructure/filesystem/snapraid/#automation","title":"Automation","text":"<p>Since SnapRAID requires manual commands, it is highly recommended to automate these tasks using scripts. The SnapRAID AIO Script is a popular community solution for automating syncs, scrubs, and monitoring.</p>"},{"location":"infrastructure/filesystem/snapraid/#recovery","title":"Recovery","text":""},{"location":"infrastructure/filesystem/snapraid/#undeleting-files","title":"Undeleting Files","text":"<p>If a file is accidentally deleted, SnapRAID can restore it to its previous state (like a backup).</p> <p>Restore specific file:</p> <pre><code>snapraid fix -f /path/to/file\n</code></pre> <p>Restore specific directory:</p> <pre><code>snapraid fix -f /path/to/directory/\n</code></pre> <p>Restore only missing files: Use the <code>-m</code> flag to recover deleted files without overwriting existing changed files.</p> <pre><code>snapraid fix -m -f /path/to/directory/\n</code></pre>"},{"location":"infrastructure/filesystem/snapraid/#recovering-a-failed-drive","title":"Recovering a Failed Drive","text":"<p>In the event of a total disk failure, follow this procedure strictly.</p> <p>1. Halt Operations Disable any automated tasks (cron jobs, AIO scripts) and stop writing new data to the array.</p> <p>2. Reconfigure Install the replacement disk and mount it. Edit <code>/etc/snapraid.conf</code> to point the failed disk identifier to the new mount point.</p> <ul> <li>Example: Change <code>data d1 /mnt/failed_disk</code> to <code>data d1 /mnt/new_disk</code>.</li> </ul> <p>3. Fix (Restore Data) Run the fix command targeting the specific disk identifier (<code>-d</code>). Log the output to a file on a different drive for review.</p> <pre><code>snapraid -d d1 -l fix.log fix\n</code></pre> <ul> <li>Note: This process will read from all other disks to reconstruct the missing data. It runs at the speed of the slowest drive.</li> </ul> <p>4. Review Check <code>fix.log</code> for \"unrecoverable\" errors. If files were modified since the last sync, they may not be fully recoverable.</p> <p>5. Verification (Optional) Run a check on the restored disk to verify integrity.</p> <pre><code>snapraid -d d1 -a check\n</code></pre> <p>6. Resync Once verified, run a standard sync to update the array status.</p> <pre><code>snapraid sync\n</code></pre>"},{"location":"infrastructure/gettingstarted/nassetup/","title":"DIY NAS: MergerFS + SnapRAID","text":"<ul> <li>SnapRAID Manual</li> <li>MergerFS Documentation</li> </ul>"},{"location":"infrastructure/gettingstarted/nassetup/#you-can-find-more-in-depth-info-in-the-related-guides-here","title":"You can find more in depth info in the related guides here.","text":"<ul> <li> <p>This is just how I did it. There are plenty of other ways to do it using different configurations (ZFS, Proxmox, Unraid, TrueNAS, OMV, etc). All options have pros and cons. </p> </li> <li> <p>For my configuration I created an Ubuntu server with MergerFS + SnapRAID. </p> <p>MergerFS: Pool drives and read them as one drive. FUSE system. Various configurations, read the official docs. THe docs are very literal.</p> <p>SnapRAID: Create routine snapshots of drives to restore them in case of failure. No striping, easy to expand, straightforward to manage. </p> </li> </ul>"},{"location":"infrastructure/gettingstarted/nassetup/#mount-points","title":"Mount points","text":"<p>Branches Docs</p>"},{"location":"infrastructure/gettingstarted/nassetup/#naming-convention","title":"Naming Convention","text":"<p>HDDs: <code>/mnt/hdd</code> SATA SSDs: <code>/mnt/ssd</code> NVME/M2: <code>/mnt/nvme</code> REMOTE: <code>/mnt/remote</code></p>"},{"location":"infrastructure/gettingstarted/nassetup/#example","title":"Example","text":"<pre><code>$ ls -lh /mnt/\ntotal 16K\ndrwxr-xr-x 8 root root 4.0K Aug 18  2024 hdd\ndrwxr-xr-x 6 root root 4.0K Oct  8  2024 nvme\ndrwxr-xr-x 3 root root 4.0K Aug 24  2024 remote\ndrwxr-xr-x 3 root root 4.0K Jul 14  2024 ssd\n\n$ ls -lh /mnt/hdd/\ntotal 8K\nd--------- 2 root root 4.0K Apr 14 15:58 10T-01234567\nd--------- 2 root root 4.0K Apr 12 20:51 20T-12345678\n\n$ ls -lh /mnt/nvme/\ntotal 8K\nd--------- 2 root root 4.0K Apr 14 16:00 1T-ABCDEFGH\nd--------- 2 root root 4.0K Apr 14 23:24 1T-BCDEFGHI\n\n$ ls -lh /mnt/remote/\ntotal 8K\nd--------- 2 root root 4.0K Apr 12 20:23 foo-sshfs\nd--------- 2 root root 4.0K Apr 12 20:24 bar-nfs\n\n\n# You can find the serial number of a drive using lsblk\n$ lsblk -d -o NAME,PATH,SIZE,SERIAL\nNAME    PATH           SIZE SERIAL\nsda     /dev/sda       9.1T 01234567\nsdb     /dev/sdb      18.2T 12345678\nnvme0n1 /dev/nvme0n1 953.9G ABCDEFGH\nnvme1n1 /dev/nvme1n1 953.9G BCDEFGHI\n</code></pre>"},{"location":"infrastructure/gettingstarted/nassetup/#1-drive-preparation","title":"1. Drive Preparation","text":"<p>Before pooling, drives must be partitioned and formatted.</p> <ul> <li>Note: Identify your disks carefully using <code>lsblk</code> or <code>sudo fdisk -l</code>.</li> <li>Strategy: Use EXT4 for simplicity and reliability on Ubuntu.</li> </ul>"},{"location":"infrastructure/gettingstarted/nassetup/#formatting-ssd-as-ext4","title":"Formatting SSD as ext4","text":"<pre><code># 1. Create a GPT partition table (Destructive!)\nsudo wipefs -a /dev/sda\n\n# 2. Create the partition (100% of disk)\nsudo parted -a opt /dev/sdb mkpart primary ext4 0% 100%\n\n# 3. Format to EXT4\n# -m 0: Reserve 0% for root (max space for data)\n# -L: Label the disk (disk1, disk2, parity1) to make it easy to identify\nsudo mkfs.ext4 -m 0 -L disk1 /dev/sdb1\n</code></pre>"},{"location":"infrastructure/gettingstarted/nassetup/#formatting-3x-hdds-as-xfs","title":"Formatting 3x HDDs as XFS","text":"<pre><code># Drive 1 (sdb)\n# wipe fs\nsudo wipefs -a /dev/sdb\n# create partition\nsudo parted -s /dev/sdb mkpart primary xfs 0% 100%\n# list partition\nsudo fdisk -l /dev/sdb\n# format partition\nsudo mkfs.xfs -f -L \"disk1\" /dev/sdb1\n\n# Drive 2 (sdc)\nsudo wipefs -a /dev/sdc\nsudo parted -s /dev/sdc mkpart primary xfs 0% 100%\nsudo fdisk -l /dev/sdc\nsudo mkfs.xfs -f -L \"disk2\" /dev/sdc1\n\n# Drive 3 (sdd)\nsudo wipefs -a /dev/sdd\nsudo parted -s /dev/sdd mkpart primary xfs 0% 100%\nsudo fdisk -l /dev/sdd  \nsudo mkfs.xfs -f -L \"parity1\" /dev/sdd1\n</code></pre> <p>(Repeat for <code>disk2</code>, <code>parity1</code>, etc.)</p> <ul> <li> <p>Get UUIDs <pre><code>ls -l /dev/disk/by-uuid/\n</code></pre></p> </li> <li> <p>Output will be: <pre><code>lrwxrwxrwx 1 root root 10 Dec  7 05:34 02de315f-8ced-4745-bb4f-2d24c46efa48 -&gt; ../../sdb1\nlrwxrwxrwx 1 root root 15 Dec  7 04:33 3c26dce0-7f10-426c-bd67-e9f2d77889ee -&gt; ../../nvme0n1p2\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 603999da-c2dd-484d-80bf-ab4977680e90 -&gt; ../../sdc1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 844cae61-9599-4a57-9431-18c0a33904e4 -&gt; ../../sda1\nlrwxrwxrwx 1 root root 10 Dec  7 05:34 9860056e-b314-4cb9-acc5-30e76281795b -&gt; ../../sdd1\nlrwxrwxrwx 1 root root 10 Dec  7 04:33 d5ac7faa-876c-43ff-b27a-94c53bf79bb2 -&gt; ../../dm-0\n</code></pre></p> </li> </ul>"},{"location":"infrastructure/gettingstarted/nassetup/#mounting","title":"Mounting","text":"<ul> <li> <p>Create Mount points: <pre><code># Must make the folder to mount drives to\nsudo mkdir -p /mnt/disk1\nsudo mkdir -p /mnt/disk2\nsudo mkdir -p /mnt/parity1\nsudo mkdir -p /mnt/appdata\n\n# Mount Parition\nsudo mount /dev/sdb1 /mnt/parity1\nsudo mount /dev/sdc1 /mnt/disk1\nsudo mount /dev/sdd1 /mnt/disk2\n\n# Verify Mounting \ndf -h \n# Filesystem      Size  Used Avail Use% Mounted on\n# ...\n# /dev/sda1       7.3T   93M  6.9T   1% /mnt/mydrive\n</code></pre></p> </li> <li> <p>Edit fstab</p> </li> <li>Scroll to bottom of fstab and add:  <pre><code># ---------------------------------------------------------\n# DATA DRIVES (XFS) - 12TB WD Reds\n# ---------------------------------------------------------\n# Disk 1 (sdb1)\nUUID=02de315f-8ced-4745-bb4f-2d24c46efa48 /mnt/disk1 xfs defaults,noatime 0 0\n\n# Disk 2 (sdc1)\nUUID=603999da-c2dd-484d-80bf-ab4977680e90 /mnt/disk2 xfs defaults,noatime 0 0\n\n# Parity Drive (sdd1)\nUUID=9860056e-b314-4cb9-acc5-30e76281795b /mnt/parity1 xfs defaults,noatime 0 0\n\n# ---------------------------------------------------------\n# FAST STORAGE (EXT4) - 240GB SSD\n# ---------------------------------------------------------\n# Appdata/Cache (sda1)\nUUID=844cae61-9599-4a57-9431-18c0a33904e4 /mnt/appdata ext4 defaults,noatime 0 0\n</code></pre></li> <li> </li> </ul>"},{"location":"infrastructure/gettingstarted/nassetup/#note-must-run-after-editing-fstab","title":"NOTE:  MUST RUN AFTER EDITING fstab","text":"<pre><code># verify syntax \nsudo findmnt --verify\n\n# reload fstab\nsudo systemctl daemon-reload\n</code></pre>"},{"location":"infrastructure/gettingstarted/nassetup/#verify","title":"Verify","text":"<ul> <li> <p>Test this BEFORE reboot - no output is good <pre><code>sudo mount -a\n</code></pre></p> </li> <li> <p>Check correct sizing: <pre><code>df -h\n</code></pre></p> </li> </ul> <p>You should see:</p> <ul> <li> <p><code>disk1</code>, <code>disk2</code>, <code>parity1</code>: ~11T</p> </li> <li> <p><code>appdata</code>: ~220G</p> </li> <li> <p>Check ownership: <pre><code>ls -ld /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p> </li> <li> <p>If owner listed as <code>root</code> (and doesnt need to be) run : <pre><code>sudo chown -R goose:goose /mnt/disk1 /mnt/disk2 /mnt/parity1 /mnt/appdata\n</code></pre></p> </li> </ul>"},{"location":"infrastructure/gettingstarted/nassetup/#2-install-mergerfs","title":"2. Install MergerFS","text":"<p>Read the official docs. There is a lot to go over with mergerFS. </p> <pre><code>sudo apt update &amp;&amp; sudo apt install mergerfs -y\n</code></pre> <p>Configure Pool in Fstab Add this line to the bottom of <code>/etc/fstab</code>:</p> <pre><code># Syntax: /mnt/disk* (all mounts starting with disk) -&gt; /mnt/storage\n/mnt/hdd/WD-B00WHLZD:/mnt/hdd/WD-B00WUHXD /mnt/storage fuse.mergerfs cache.files=off,category.create=mspmfs,func.getattr=newest,dropcacheonclose=false,minfreespace=100G,moveonenospc=true,fsname=mergerfs 0 0\n</code></pre> <ul> <li>minfreespace=50G: Prevents filling a drive completely, which creates fragmentation issues.</li> <li>cache.files=off: Disables page caching (reduces RAM usage/complexity).</li> <li>category.create=mspmfs: Most Space, Path Most Free Space. Writes new files to the drive with the most free space, unless the path already exists on another drive.</li> <li>func.getattr=newest: Returns file attributes from the file with the newest mtime. Critical for apps like Plex/Kodi to detect changes.</li> <li>minfreespace=200G: Prevents filling a drive completely; moves to the next drive when 200GB remains.</li> </ul> <p>C. Activate</p> <pre><code>sudo mkdir -p /mnt/storage\nsudo mount -a\n</code></pre>"},{"location":"infrastructure/gettingstarted/nassetup/#3-snapraid","title":"3. SnapRAID","text":"<p>SnapRAID calculates parity (like RAID 5) but does it on a schedule (Snapshot), not in real-time. Ideally suited for media servers where files rarely change.</p> <p>A. Install SnapRAID Ubuntu repos often have outdated versions. It is safer to build from source or download the latest .deb.</p> <pre><code># Check version in repo or download from official site\nsudo apt install snapraid -y\nsnapraid --version\n</code></pre> <p>**B. Configure <code>/etc/snapraid.conf**</code> Backup the default and create your own: <code>sudo nano /etc/snapraid.conf</code></p> <pre><code># 1. Parity Location (The dedicated large drive)\nparity /mnt/parity1/snapraid.parity\n\n# 2. Content Files (Index of all your files)\n# Save multiple copies on different physical disks!\ncontent /var/snapraid.content\ncontent /mnt/disk1/.snapraid.content\ncontent /mnt/disk2/.snapraid.content\n\n# 3. Data Disks ( The drives you want to protect)\ndata d1 /mnt/disk1/\ndata d2 /mnt/disk2/\n\n# 4. Excludes (Temp files/Trash)\nexclude *.unrecoverable\nexclude /tmp/\nexclude /lost+found/\n</code></pre> <p>C. Initialize Run the first sync to calculate parity. This will take hours depending on data size.</p> <pre><code>sudo snapraid sync\n</code></pre>"},{"location":"infrastructure/gettingstarted/nassetup/#5-automation-maintenance","title":"5. Automation &amp; Maintenance","text":"<p>SnapRAID is manual by default. You must automate it to stay protected.</p> <p>A. Basic Crontab Edit cron: <code>sudo crontab -e</code> Add a nightly sync and weekly scrub (checks for bit-rot).</p> <pre><code># Sync every night at 3 AM\n0 3 * * * snapraid sync &gt;&gt; /var/log/snapraid.log\n\n# Scrub (check data integrity) every Sunday at 5 AM\n0 5 * * 0 snapraid scrub -p 5 &gt;&gt; /var/log/snapraid_scrub.log\n</code></pre> <ul> <li>-p 5: Scrubs 5% of the array each week, ensuring a full check every ~5 months.</li> </ul>"},{"location":"infrastructure/gettingstarted/nassetup/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>\"Disk is full\" during sync: Your parity drive must be equal to or larger than your single largest data drive.</li> <li>Data Recovery: If <code>disk1</code> fails, replace it, mount to <code>/mnt/disk1</code>, and run <code>snapraid fix -d d1</code>.</li> </ul>"},{"location":"infrastructure/gettingstarted/pcsetup/","title":"Server Setup","text":"<ul> <li>Ubuntu Server 24.04 LTS Release Notes</li> <li>Ubuntu Server Network Configuration</li> <li>OpenSSH Server Documentation</li> </ul>"},{"location":"infrastructure/gettingstarted/pcsetup/#1-preparation-media-creation","title":"1. Preparation &amp; Media Creation","text":"<p>Before touching the hardware, the installation media must be prepared on a separate computer.</p> <p>A. Download ISO Get the Ubuntu Server 24.04 LTS (Long Term Support) ISO. Use whatever OS you want. I just used Ubuntu for this guide.</p> <ul> <li>Note: Do not use the \"Desktop\" version for a server. The Server ISO is optimized for performance and headless operation (no monitor).</li> </ul> <p>B. Flash to USB Use BalenaEtcher, Rufus, <code>dd</code>, or another application to write the ISO to a USB drive (4GB+).</p>"},{"location":"infrastructure/gettingstarted/pcsetup/#2-bios-uefi-configuration","title":"2. BIOS / UEFI Configuration","text":"<p>Mini PCs (like the Lenovo M920q) often ship with settings optimized for Windows office use. These must be adjusted for a Linux server.</p> <ol> <li>Enter BIOS: Insert the USB, power on, and rapidly tap the setup key (usually <code>F1</code>, <code>F2</code>, or <code>F12</code> for Lenovo/Dell/HP).</li> <li>Secure Boot: Set to Disabled. This ensures compatibility with third-party drivers (like Proxmox or specific NIC drivers) later on.</li> <li>Power Behavior: Look for \"After Power Loss\" or \"AC Recovery\" and set to Power On. This ensures the server reboots automatically after a power outage.</li> <li>Boot Order: Move \"USB HDD\" or your flash drive to the top of the list.</li> </ol>"},{"location":"infrastructure/gettingstarted/pcsetup/#3-installation-process","title":"3. Installation Process","text":"<p>Boot from the USB. The installer (Subiquity) uses a text-based UI. Navigate with <code>Arrow Keys</code>, select with <code>Enter</code>, and toggle options with <code>Space</code>.</p> <p>A. General Settings</p> <ul> <li>Language/Keyboard: Select your defaults.</li> <li>Base Install: Choose \"Ubuntu Server\" (standard). The \"Minimized\" version is too stripped down for a beginner homelab.</li> </ul> <p>B. Network (Critical Step) By default, the server asks for a DHCP address. Set a Static IP now to avoid connection issues later.</p> <ol> <li>Select your Ethernet interface (e.g., <code>eth0</code> or <code>eno1</code>).</li> <li>Change \"IPv4 Method\" from DHCP to Manual.</li> <li>Subnet: Usually <code>192.168.1.0/24</code> (Check your router).</li> <li>Address: Pick an IP outside your router's DHCP range (e.g., <code>192.168.1.50</code>).</li> <li>Gateway: Your router's IP (e.g., <code>192.168.1.1</code>).</li> <li>Name Servers: <code>1.1.1.1</code> (Cloudflare) or <code>8.8.8.8</code> (Google).</li> </ol> <p>C. Storage</p> <ul> <li>Select \"Use an entire disk\".</li> <li>LVM: Keep \"Set up this disk as an LVM group\" checked. This allows for flexible partition resizing later. Not required, but it can help. Verify compatability with desired filesystem to be used.</li> </ul> <p>D. Profile &amp; SSH</p> <ul> <li>Identity: Set your hostname (e.g., <code>my-node-01</code>) and username.</li> <li>SSH Setup: Check the box [ ] Install OpenSSH server.</li> <li>Do not skip this. Without it, you cannot connect to the headless server remotely.</li> </ul>"},{"location":"infrastructure/gettingstarted/pcsetup/#4-post-install-configuration","title":"4. Post-Install Configuration","text":"<p>Once installed, remove the USB and press Enter to reboot. From this point on, perform all steps via SSH from your main computer.</p> <p>A. Connect via SSH Open your terminal (or PowerShell on Windows) and connect:</p> <pre><code>ssh username@192.168.1.50\n</code></pre> <p>(Replace with the IP you set in Step 3B).</p> <p>B. Update System Update the package repositories and upgrade installed packages:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre> <p>C. Verify Static IP (Netplan) If you skipped the static IP step during install, you must configure <code>netplan</code>. Edit the config: <code>sudo nano /etc/netplan/00-installer-config.yaml</code></p> <pre><code>network:\n  ethernets:\n    eno1: # Check your interface name with 'ip a'\n      dhcp4: false\n      addresses:\n        - 192.168.1.50/24\n      routes:\n        - to: default\n          via: 192.168.1.1\n      nameservers:\n        addresses: [1.1.1.1, 8.8.8.8]\n  version: 2\n</code></pre> <p>Apply changes: <code>sudo netplan apply</code>.</p>"},{"location":"infrastructure/gettingstarted/pcsetup/#5-quality-of-life-security","title":"5. Quality of Life &amp; Security","text":"<p>A. Firewall (UFW) Enable the firewall but allow SSH first to prevent locking yourself out.</p> <pre><code>sudo ufw allow ssh\nsudo ufw enable\n</code></pre> <p>Response: Command may disrupt existing ssh connections. Proceed with operation (y|n)? Type <code>y</code>.</p> <p>B. Prevent Sleep (Laptop/Mini PC Specific) Ensure the system doesn't suspend when idle.</p> <pre><code>sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target\n</code></pre>"},{"location":"infrastructure/gettingstarted/pcsetup/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>\"Permission Denied (publickey)\": You may have enabled \"Import SSH Identity\" during install but didn't provide a key. Use password authentication initially or add the <code>-o PubkeyAuthentication=no</code> flag to debug.</li> <li>No Internet: Check your gateway settings in <code>netplan</code>. Try <code>ping 8.8.8.8</code>. If that works but <code>ping google.com</code> fails, check your <code>nameservers</code>.</li> <li>Fan Noise: On some Mini PCs (Lenovo Tiny/Dell Micro), fan control packages like <code>lm-sensors</code> and <code>fancontrol</code> may be needed if the BIOS default is too aggressive.</li> </ul>"},{"location":"infrastructure/git/git/","title":"Git Reference &amp; Version Control","text":""},{"location":"infrastructure/git/git/#overview","title":"Overview","text":"<p>Git is a distributed version control system used to track changes in source code. In a homelab context, it serves as the backbone of \"Infrastructure as Code\" (IaC). By storing <code>docker-compose.yml</code> and configuration files in Git, the entire server state is backed up, versioned, and easily reproducible.</p> <p>This guide covers the initialization of a local repository, security practices for managing secrets, and daily workflows using both the Command Line Interface (CLI) and VS Code.</p> <p>I don't know much about the other stuff in git. I just use the basics, but this gets me by. </p>"},{"location":"infrastructure/git/git/#1-initialization-one-time-setup","title":"1. Initialization (One-Time Setup)","text":"<p>Perform these steps when setting up the machine for the first time to establish the repository.</p>"},{"location":"infrastructure/git/git/#11-installation-identity","title":"1.1. Installation &amp; Identity","text":"<p>First, ensure Git is installed and configure the user identity. This information is embedded in every commit.</p> <pre><code># Install Git\nsudo apt update &amp;&amp; sudo apt install git -y\n\n# Configure Global Identity\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\n# Set default branch to 'main' (Modern Standard)\ngit config --global init.defaultBranch main \n</code></pre>"},{"location":"infrastructure/git/git/#12-initialize-repo","title":"1.2. Initialize Repo","text":"<p>Turn the Docker directory into a tracked repository.</p> <pre><code>cd ~/homelab/docker/\ngit init\n</code></pre>"},{"location":"infrastructure/git/git/#13-the-ignore-rules-critical","title":"1.3. The Ignore Rules (Critical)","text":"<p>A <code>.gitignore</code> file prevents sensitive data (secrets) and system clutter from being uploaded to the remote server.</p> <p>File Location: <code>~/homelab/docker/.gitignore</code></p> <pre><code># --- Secrets (NEVER COMMIT) ---\n**/.env\n**/.env.*\n**/secrets/\nid_rsa\n*.pem\n\n# --- System Junk ---\n.DS_Store\nThumbs.db\n**/.git/   # Ignore nested git repos (prevents submodule errors)\n\n# --- Docker Data (If mapped locally) ---\n**/data/\n**/mysql/\n**/influxdb/ \n\n# --- Custom Ignores ---\n**/.pastebin\n**/.pastebin/\n</code></pre>"},{"location":"infrastructure/git/git/#14-connect-to-remote-github","title":"1.4. Connect to Remote (GitHub)","text":"<p>Link the local folder to a GitHub repository for off-site backup.</p> <pre><code># 1. Link Remote\ngit remote add origin https://github.com/YourUser/homelab-backup.git\n\n# 2. First Push\ngit add .\ngit commit -m \"Initial homelab commit\"\ngit push -u origin main\n</code></pre>"},{"location":"infrastructure/git/git/#2-daily-workflows-cli","title":"2. Daily Workflows (CLI)","text":""},{"location":"infrastructure/git/git/#21-the-config-change-routine","title":"2.1. The \"Config Change\" Routine","text":"<p>Execute this routine whenever a <code>compose.yaml</code> or configuration file is edited.</p> <ol> <li> <p>Check Status: <pre><code>git status\n</code></pre></p> </li> <li> <p>Verify: Ensure no <code>.env</code> files appear in the untracked list (Red).</p> </li> <li> <p>Stage &amp; Commit: <pre><code>git add .\ngit commit -m \"Added Tautulli to media stack\"\n</code></pre></p> </li> <li> <p>Push: <pre><code>git push\n</code></pre></p> </li> </ol>"},{"location":"infrastructure/git/git/#22-disaster-recovery-rebuild","title":"2.2. Disaster Recovery (Rebuild)","text":"<p>If the host OS drive fails, use this workflow to restore the stack on a fresh installation.</p> <ol> <li> <p>Clone Configs: <pre><code>git clone https://github.com/YourUser/homelab-backup.git ~/homelab/docker\n</code></pre></p> </li> <li> <p>Restore Secrets (Manual): Since <code>.env</code> files are ignored for security, they must be manually recreated from a password manager (e.g., Bitwarden). <pre><code>cd ~/homelab/docker/core\nnano .env \n# Paste API keys/passwords here\n</code></pre></p> </li> <li> <p>Launch: <pre><code>docker compose up -d\n</code></pre></p> </li> </ol>"},{"location":"infrastructure/git/git/#3-security-secret-detection","title":"3. Security: Secret Detection","text":"<p>Objective: Prevent accidental leakage of passwords, API keys, or <code>.env</code> contents into the public Git history.</p>"},{"location":"infrastructure/git/git/#31-visual-inspection","title":"3.1. Visual Inspection","text":"<p>Before staging files, review changes to ensure no hardcoded credentials were added.</p> Command Description <code>git diff</code> Shows line-by-line changes for unstaged files. <code>git diff --staged</code> Shows changes for files already staged but not committed."},{"location":"infrastructure/git/git/#32-automated-scanning-gitleaks-via-docker","title":"3.2. Automated Scanning (Gitleaks via Docker)","text":"<p>Use Gitleaks to scan the directory for high-entropy strings (random characters that look like API keys) without installing new software on the host.</p> <pre><code># Run inside ~/homelab/docker\ndocker run --rm \\\n  -v $(pwd):/path \\\n  zricethezav/gitleaks:latest \\\n  detect --source=\"/path\" -v\n</code></pre> <ul> <li>Success: \"No leaks found.\"</li> <li>Failure: Output lists the specific secret and file location.</li> </ul>"},{"location":"infrastructure/git/git/#4-vs-code-integration","title":"4. VS Code Integration","text":""},{"location":"infrastructure/git/git/#41-visualizing-diffs","title":"4.1. Visualizing Diffs","text":"<p>To view diffs directly in VS Code from the terminal:</p> <p>One-off:</p> <pre><code>git diff | code -\n</code></pre> <p>Permanent Configuration:</p> <pre><code>git config --global diff.tool vscode\ngit config --global difftool.vscode.cmd 'code --wait --diff $LOCAL $REMOTE'\n# Usage:\ngit difftool\n</code></pre>"},{"location":"infrastructure/git/git/#42-file-explorer-indicators","title":"4.2. File Explorer Indicators","text":"<p>Understanding the colored badges in the VS Code file explorer.</p> Badge Color (Gruvbox) Meaning Action Needed U Green Untracked File is new. Needs <code>git add</code>. M Blue Modified File is edited but not staged. A Green Added File is staged and ready to commit. D Red Deleted File removed locally. - Gray Ignored Matches <code>.gitignore</code> rule."},{"location":"infrastructure/git/git/#43-gui-workflow","title":"4.3. GUI Workflow","text":"<p>The \"Happy Path\" using the Source Control Tab (<code>Ctrl+Shift+G</code>):</p> <ol> <li>Edit &amp; Save: Make changes and save (<code>Ctrl+S</code>).</li> <li>Review: Click the file under Changes to see the Side-by-Side Diff.</li> <li>Stage: Click the <code>+</code> (Plus) icon next to the file.</li> <li>Commit: Type a message in the input box  Click Commit.</li> <li>Sync: Click Sync Changes (Auto Pulls &amp; Pushes).</li> </ol>"},{"location":"infrastructure/git/git/#5-troubleshooting","title":"5. Troubleshooting","text":""},{"location":"infrastructure/git/git/#51-submodule-error-gray-folder-on-github","title":"5.1. \"Submodule\" Error (Gray Folder on GitHub)","text":"<p>Symptom: A project (like a theme) was <code>git clone</code>d inside the main repo. GitHub sees the nested <code>.git</code> folder and treats it as a dead link (submodule).</p> <p>Fix:</p> <pre><code># 1. Remove the nested .git folder\nrm -rf path/to/subfolder/.git\n\n# 2. Remove the folder from Git's index (cache) only\ngit rm --cached path/to/subfolder\n\n# 3. Re-add the folder as standard files\ngit add .\ngit commit -m \"Converted submodule to standard files\"\n</code></pre>"},{"location":"infrastructure/git/git/#52-credentials-issues-https","title":"5.2. Credentials Issues (HTTPS)","text":"<p>Symptom: Git prompts for a username/password on every push.</p> <p>Fix (Cache Credentials):</p> <pre><code># Cache credentials in memory for 1 hour (3600 seconds)\ngit config --global credential.helper 'cache --timeout=3600'\n</code></pre>"},{"location":"infrastructure/git/git/#6-command-cheat-sheet","title":"6. Command Cheat Sheet","text":""},{"location":"infrastructure/git/git/#basic-operations","title":"Basic Operations","text":"Action Command Check State <code>git status</code> (Always run this first) View Changes <code>git diff</code> Stage Files <code>git add .</code> (Stage all) or <code>git add &lt;filename&gt;</code> Commit <code>git commit -m \"Your message here\"</code> Push <code>git push origin main</code> Pull Updates <code>git pull</code> Undo File Edit <code>git restore &lt;file&gt;</code> (Discards local changes) View History <code>git log --oneline --graph --all</code>"},{"location":"infrastructure/git/git/#emergency-fixes","title":"Emergency Fixes","text":"Scenario Workflow / Command Typos in last commit <code>git add .</code> <code>git commit --amend -m \"New Message\"</code> <code>git push -f</code> Leaked Secret <code>git rm --cached .env</code> <code>echo \".env\" &gt;&gt; .gitignore</code> <code>git commit -m \"rm secret\"</code> Reset to Remote <code>git fetch origin</code> <code>git reset --hard origin/main</code> (Destructive: makes local match cloud)"},{"location":"infrastructure/shell/zsh/","title":"Zsh &amp; Terminal Customization","text":""},{"location":"infrastructure/shell/zsh/#1-installation","title":"1. Installation","text":"<p>Install the shell and git, which is required for downloading the theme and plugins.</p> <pre><code># Update package list and install\nsudo apt update &amp;&amp; sudo apt install zsh git -y\n\n# Verify installation\nzsh --version\n</code></pre>"},{"location":"infrastructure/shell/zsh/#2-oh-my-zsh-framework","title":"2. Oh My Zsh Framework","text":"<p>Oh My Zsh (OMZ) is a configuration framework that manages the Zsh environment.</p> <p>Install via Curl:</p> <pre><code>sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> <p>Note: If prompted to change the default shell to <code>zsh</code>, type <code>Y</code> and press <code>Enter</code>.</p>"},{"location":"infrastructure/shell/zsh/#3-powerlevel10k-theme","title":"3. Powerlevel10k Theme","text":"<p>Powerlevel10k is a high-performance theme that adds context-aware prompts (e.g., Git status, Docker context, execution time).</p> <p>A. Download the Theme Clone the repository into the custom themes directory:</p> <pre><code>git clone --depth=1 https://github.com/romkatv/powerlevel10k.git \"${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k\"\n</code></pre> <p>**B. Configure <code>.zshrc**</code> Open the configuration file: <code>nano ~/.zshrc</code></p> <p>Locate the line <code>ZSH_THEME=\"robbyrussell\"</code> and replace it with:</p> <pre><code>ZSH_THEME=\"powerlevel10k/powerlevel10k\"\n</code></pre> <p>C. Apply and Configure Apply the changes to start the configuration wizard:</p> <pre><code>source ~/.zshrc\n</code></pre> <p>Follow the on-screen prompts to select the visual style.</p> <p>Important: Font Requirement If icons appear as square placeholders, the client machine (the computer you are typing on) is missing a Nerd Font. Install MesloLGS NF on your local machine and set it as the terminal font. Do not install this on the server.</p>"},{"location":"infrastructure/shell/zsh/#4-essential-plugins","title":"4. Essential Plugins","text":"<p>These plugins add syntax highlighting and predictive text based on history.</p> <p>A. Install Plugins Clone the repositories into the plugins directory:</p> <ul> <li> <p>zsh-autosuggestions: <pre><code>git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\n</code></pre></p> </li> <li> <p>zsh-syntax-highlighting: <pre><code>git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting\n</code></pre></p> </li> </ul> <p>B. Activate Plugins Open the config file: <code>nano ~/.zshrc</code></p> <p>Locate the line <code>plugins=(git)</code> and update it to:</p> <pre><code>plugins=(git zsh-autosuggestions zsh-syntax-highlighting)\n</code></pre>"},{"location":"infrastructure/shell/zsh/#5-custom-aliases","title":"5. Custom Aliases","text":"<p>Append the following block to the bottom of <code>~/.zshrc</code> to create shortcuts for common Docker and system tasks.</p> <pre><code># --- Docker &amp; Homelab Aliases ---\nalias dcp='docker compose ps'\nalias dcl='docker compose logs -f'  # Follow logs\nalias dcd='docker compose down'\nalias dcup='docker compose up -d'\nalias dcr='docker compose restart'\n\n# --- System Management ---\nalias update='sudo apt update &amp;&amp; sudo apt upgrade -y'\nalias ports='sudo ss -tulpn | grep LISTEN' # uses ss instead of netstat\nalias reload='source ~/.zshrc'\n\n# --- Navigation ---\nalias ..='cd ..'\nalias ...='cd ../..'\nalias ll='ls -lah'\n</code></pre>"},{"location":"infrastructure/shell/zsh/#6-finalize","title":"6. Finalize","text":"<p>Reload the configuration one last time to apply plugins and aliases:</p> <pre><code>source ~/.zshrc\n</code></pre>"},{"location":"infrastructure/shell/zsh/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>\"Command not found\": Ensure <code>source ~/.zshrc</code> was executed after editing the file.</li> <li>Wrong Shell: If the prompt remains in Bash after a reboot, force the default shell change: <pre><code>chsh -s $(which zsh)\n</code></pre></li> </ul> <p>Note: A logout/login is required for <code>chsh</code> to take effect.</p>"},{"location":"monitoring/beszel/","title":"Beszel","text":""},{"location":"monitoring/beszel/#github","title":"Github","text":"<p>Beszel is a lightweight server monitoring platform that includes Docker statistics, historical data, and alert functions.</p> <p>It has a friendly web interface, simple configuration, and is ready to use out of the box. It supports automatic backup, multi-user, OAuth authentication, and API access.</p> <p></p>"},{"location":"monitoring/beszel/#features","title":"Features","text":"<ul> <li>Lightweight: Smaller and less resource-intensive than leading solutions.</li> <li>Simple: Easy setup with little manual configuration required.</li> <li>Docker stats: Tracks CPU, memory, and network usage history for each container.</li> <li>Alerts: Configurable alerts for CPU, memory, disk, bandwidth, temperature, load average, and status.</li> <li>Multi-user: Users manage their own systems. Admins can share systems across users.</li> <li>OAuth / OIDC: Supports many OAuth2 providers. Password auth can be disabled.</li> <li>Automatic backups: Save to and restore from disk or S3-compatible storage.</li> </ul>"},{"location":"monitoring/beszel/#architecture","title":"Architecture","text":"<p>Beszel consists of two main components: the hub and the agent.</p> <ul> <li>Hub: A web application built on PocketBase that provides a dashboard for viewing and managing connected systems.</li> <li>Agent: Runs on each system you want to monitor and communicates system metrics to the hub.</li> </ul>"},{"location":"monitoring/beszel/#getting-started","title":"Getting started","text":"<p>Other documentation is available on offical website, beszel.dev.</p> <p>Install hub and local agent. The hub is central monitoring for multiple machines, this only needs to be on a single machine. The agent is used to monitor each machine. </p> <p>compose.yaml <pre><code># --- BESZEL HUB (The Dashboard) ---\nbeszel-hub:\n  image: henrygd/beszel # https://github.com/henrygd/beszel\n  container_name: beszel-hub\n  networks:\n    - proxy_net  # To access via NPM\n  ports:\n    - \"8090:8090\"   # Exposed locally for initial setup\n  volumes:\n    - ${APPDATA}/beszel/data:/beszel_data\n  restart: unless-stopped\n\n# --- BESZEL AGENT (The Sensor) ---\nbeszel-agent:\n  image: henrygd/beszel-agent # https://github.com/henrygd/beszel\n  container_name: beszel-agent\n  restart: unless-stopped\n  network_mode: \"host\"  #  Must be on host to read actual CPU/Disk/Network\n\n  # GPU monitoring \n  deploy:\n    resources:\n      reservations:\n        devices:\n          - driver: nvidia\n            count: all\n            capabilities:\n              - utility  \n\n  devices:\n    - /dev/nvme0:/dev/nvme0 # Root Drive\n    - /dev/sda:/dev/sda # Appdata Drive\n    - /dev/sdb:/dev/sdb # Parity Drive  \n    - /dev/sdc:/dev/sdc # Data Drive 1 \n    - /dev/sdd:/dev/sdd # Data Drive 2 \n  cap_add:\n    - SYS_RAWIO # required for S.M.A.R.T. data\n    - SYS_ADMIN # required for NVMe S.M.A.R.T. data\n\n  volumes:\n    - ./beszel_agent_data:/var/lib/beszel-agent\n    - /var/run/docker.sock:/var/run/docker.sock:ro\n\n    #  Map disk mounts for stroage info. For disk I/O info: /extra-filesystems/sdX = sdX must match disk from `cat /proc/diskstats`  -- double underscores adds the customs names. \n    - /mnt/hdd/WD-12345:/extra-filesystems/sdc__data_disk_1:ro \n    - /mnt/hdd/WD-67890/extra-filesystems/sdd__data_disk_2:ro \n  environment:\n    - PORT=45876\n    - KEY=${BESZEL_KEY} \n    - EXTRA_FILESYSTEMS=/mnt/root_drive\n</code></pre></p>"},{"location":"monitoring/beszel/#supported-metrics","title":"Supported metrics","text":"<ul> <li>CPU usage - Host system and Docker / Podman containers.</li> <li>Memory usage - Host system and containers. Includes swap and ZFS ARC.</li> <li>Disk usage - Host system. Supports multiple partitions and devices.</li> <li>Disk I/O - Host system. Supports multiple partitions and devices.</li> <li>Network usage - Host system and containers.</li> <li>Load average - Host system.</li> <li>Temperature - Host system sensors.</li> <li>GPU usage / power draw - Nvidia, AMD, and Intel.</li> <li>Battery - Host system battery charge.</li> <li>Containers - Status and metrics of all running Docker / Podman containers.</li> <li>S.M.A.R.T. - Host system disk health.</li> </ul>"},{"location":"monitoring/dozzle/","title":"Dozzle","text":"<p>Github  </p> <p>Official Site  </p> <p>Dozzle is a small lightweight application with a web based interface to monitor Docker logs. It doesn\u2019t store any log files. It is for live monitoring of your container logs only.</p>"},{"location":"monitoring/dozzle/#features","title":"Features","text":"<ul> <li>Intelligent fuzzy search for container names \ud83e\udd16</li> <li>Search logs using regex \ud83d\udd26</li> <li>Search logs using SQL queries \ud83d\udcca</li> <li>Small memory footprint \ud83c\udfce</li> <li>Split screen for viewing multiple logs</li> <li>Live stats with memory and CPU usage</li> <li>Multi-user authentication with support for proxy forward authorization \ud83d\udea8</li> <li>Swarm mode support \ud83d\udc33</li> <li>Agent mode for monitoring multiple Docker hosts \ud83d\udd75\ufe0f\u200d\u2642\ufe0f</li> <li>Dark mode \ud83c\udf19</li> </ul>"},{"location":"monitoring/dozzle/#running-dozzle","title":"Running Dozzle","text":"<p>Dozzle will be available at http://localhost:8080/.</p> <p>Here is the Docker Compose file:</p> <pre><code>services:\n  dozzle:\n    container_name: dozzle\n    image: amir20/dozzle:latest\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    ports:\n      - 8080:8080\n</code></pre> <p>For advanced options like authentication, remote hosts or common questions see documentation at dozzle.dev.</p>"},{"location":"monitoring/dozzle/#remote-hosts","title":"Remote Hosts","text":"<p>Add the line <code>command: agent</code> to your remote host.</p> <pre><code>services:\n  dozzle-agent:\n    image: amir20/dozzle:latest\n    command: agent \n    ports:\n      - 7007:7007\n    healthcheck:\n      test: [\"CMD\", \"/dozzle\", \"healthcheck\"]\n      interval: 5s\n      retries: 5\n      start_period: 5s\n      start_interval: 5s\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    restart: unless-stopped\n</code></pre> <p>On your main host, add the line <code>command: --remote-agent REMOTEIP:PORT</code></p> <pre><code>dozzle:\n    image: amir20/dozzle:latest # https://github.com/amir20/dozzle\n    container_name: dozzle\n    command: --remote-agent 192.168.0.11:7007 # connect to remote agent\n    ports:\n      - \"8081:8080\" # Web UI on port 8081\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    environment:\n      - DOZZLE_LEVEL=info \n    networks: \n      - proxy_net # to allow npm access (using container name only)\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"/dozzle\", \"healthcheck\"]\n      interval: 5s\n      retries: 5\n      start_period: 5s\n      start_interval: 5s\n</code></pre>"},{"location":"monitoring/homepage/","title":"Homepage","text":"<p>Github  </p> <p>A modern, fully static, fast, secure fully proxied, highly customizable application dashboard with integrations for over 100 services and translations into multiple languages. Easily configured via YAML files or through docker label discovery. </p>"},{"location":"monitoring/homepage/#features","title":"Features","text":"<p>With features like quick search, bookmarks, weather support, a wide range of integrations and widgets, an elegant and modern design, and a focus on performance, Homepage is your ideal start to the day and a handy companion throughout it.</p> <pre><code>Fast - The site is statically generated at build time for instant load times.\nSecure - All API requests to backend services are proxied, keeping your API keys hidden. Constantly reviewed for security by the community.\nFor Everyone - Images built for AMD64, ARM64.\nFull i18n - Support for over 40 languages.\nService &amp; Web Bookmarks - Add custom links to the homepage.\nDocker Integration - Container status and stats. Automatic service discovery via labels.\nService Integration - Over 100 service integrations, including popular starr and self-hosted apps.\nInformation &amp; Utility Widgets - Weather, time, date, search, and more.\nAnd much more...\n</code></pre>"},{"location":"monitoring/homepage/#running-homepage-via-docker","title":"Running Homepage via docker","text":"<pre><code>services:\n  homepage:\n    image: ghcr.io/gethomepage/homepage:latest\n    container_name: homepage\n    environment:\n      HOMEPAGE_ALLOWED_HOSTS: gethomepage.dev # required, may need port. See gethomepage.dev/installation/#homepage_allowed_hosts\n      PUID: 1000 # optional, your user id\n      PGID: 1000 # optional, your group id\n    ports:\n      - 3000:3000\n    volumes:\n      - /path/to/config:/app/config # Make sure your local config directory exists\n      - /var/run/docker.sock:/var/run/docker.sock:ro # optional, for docker integrations\n    restart: unless-stopped\n</code></pre>"},{"location":"monitoring/homepage/#configuration","title":"Configuration","text":"<p>Homepage can integrate ALOT. </p> <p>Refer to the homepage documentation website for more information. It has everything.</p>"},{"location":"monitoring/homepage/#basic-config-file-usage-is","title":"Basic config file usage is:","text":"<p><code>services.yaml</code></p> <p>This will actually define the main app blocks on the page. This is where you define the URL, container, icons, and widget info. </p> <p><code>settings.yaml</code></p> <p>This is the main way to layout the page. Rows, columns, sections, title, weather, etc.</p> <p><code>widgets.yaml</code></p> <p>This is the non-app/container widgets. Search , glances monitor, etc.</p> <p><code>docker.yaml</code></p> <p>Defines your docker socket for integration - same for proxmox, kube, etc.</p> <p><code>bookmarks.yaml</code></p> <p>Smaller bookmark tabs. Youtube, github, whatever you want. No widgets. </p> <p><code>custom.css</code></p> <p>Makes your page fancy. I hate CSS but this is mine, there is probably a much better way to do it but oh well. Gruvbox dark theme:</p> <pre><code>/*==================================\n  HOMEPAGE CSS - GRUVBOX DARK \n==================================*/\n\n/*==================================\n  1. FONT IMPORTS\n==================================*/\n@import url('https://fonts.googleapis.com/css2?family=Fira+Mono:wght@400;500;700&amp;display=swap');\n\n/*==================================\n  2. CSS VARIABLES\n==================================*/\n:root {\n  --my-font: \"Fira Mono\", monospace;\n\n  /* Gruvbox Dark Palette */\n  --gb-bg:       #1d2021;\n  --gb-card:     #32302f;\n  --gb-cream:    #ebdbb2; /* Main Text */\n\n  /* Prism Accents */\n  --gb-yellow:   #fabd2f; /* Titles/Borders */\n  --gb-green:    #98971a; /* Headers */\n  --gb-purple:   #d3869b; /* Hovers */\n  --gb-blue:     #83a598; /* Meta/Inputs */\n  --gb-orange:   #fe8019; /* Icons */\n  --gb-red:      #fb4934; /* Alerts */\n\n  /* Settings */\n  --border-width: 2px;\n  --border-radius: 6px; \n}\n\n/*==================================\n  3. BASE STYLES\n==================================*/\n/* Apply background and font to the document root */\nbody, html, #__next {\n  background-color: var(--gb-bg);\n  color: var(--gb-cream); /* Default text is Cream*/\n  font-family: var(--my-font);\n  -webkit-font-smoothing: antialiased;\n}\n\n/* Headers / Group Titles */\nh1, h2, h3, .group-title {\n  color: var(--gb-green);\n  font-weight: 700;\n  text-transform: uppercase;\n  letter-spacing: 1px;\n}\n\n/*==================================\n  4. CARDS (Services &amp; Bookmarks)\n==================================*/\n/* Target the cards specifically */\n.services-item, \n.bookmark-item, \n.widget {\n  background-color: var(--gb-card);\n  border: var(--border-width) solid var(--gb-yellow);\n  border-radius: var(--border-radius);\n  box-shadow: 4px 4px 0px #1d2021; /* Hard shadow for \"retro\" feel */\n  transition: all 0.2s cubic-bezier(0.25, 0.46, 0.45, 0.94);\n  color: var(--gb-cream);\n}\n\n/* Card Titles - Force Yellow */\n.services-item .text-base,\n.bookmark-item .text-base,\n.service-name {\n  color: var(--gb-yellow);\n  font-weight: 700;\n}\n\n/* Card Descriptions - Force Cream/Blue */\n.services-item .text-xs, \n.service-description {\n  color: var(--gb-cream);\n  opacity: 0.8;\n}\n\n/* Hover Effects */\n.services-item:hover, \n.bookmark-item:hover,\n.widget:hover {\n  border-color: var(--gb-purple);\n  transform: translate(-2px, -2px);\n  box-shadow: 6px 6px 0px var(--gb-purple);\n  z-index: 10;\n}\n\n/* Remove internal transparency/backgrounds */\n.services-item a, \n.services-item div, \n.widget div {\n   background: transparent;\n}\n\n/*==================================\n  5. UI COMPONENTS\n==================================*/\n/* Icons */\n.service-icon img, \n.service-icon svg,\n.bookmark-icon img,\n.bookmark-icon svg {\n   /* filter: sepia(100%) hue-rotate(-30deg) saturate(300%); Trick to tint images orange */\n   color: var(--gb-cream);\n}\n\n/* Search Bar */\ninput {\n  background-color: var(--gb-card);\n  border: 2px solid #3c3836;\n  color: var(--gb-cream);\n  font-family: var(--my-font);\n  border-radius: 20px;\n}\n\ninput:focus {\n  outline: none;\n  border-color: var(--gb-orange);\n}\n\ninput::placeholder {\n  color: var(--gb-blue);\n  opacity: 0.5;\n}\n\n\n/*top bar text color*/\n  .dark\\:text-theme-200:is(.dark *), .dark\\:text-theme-200\\/50:is(.dark *) {\n    color: var(--gb-cream);\n  }\n\n/* contatier stat text */\n.text-center,\nspan[class*=\"all\"] { color: var(--gb-cream) !important; }\n\n/* ICONS AT TOP BAR */\nsvg.w-5.h-5 {\n   color: var(--gb-yellow);\n}\n\n/*==================================\n  7. UTILITIES / FOOTER\n==================================*/\nfooter, .settings-toggle {\n   display: none;\n}\n\n@media screen and (max-width: 768px) {\n  #myTab {\n    display: flex;\n    flex-wrap: wrap;\n    justify-content: center;\n    gap: 10px;\n  }\n}\n</code></pre>"},{"location":"networking/cloudflare/tunnels/","title":"Cloudflare Tunnels","text":"<p>Cloudflare docs </p> <p>This covers creating a cloudflare tunnel in docker, serving a container with it, and securing it using an access policy. This is assuming you already have a domain and CNAME DNS record setup with cloudflare. </p>"},{"location":"networking/cloudflare/tunnels/#creating-a-tunnel","title":"Creating A Tunnel","text":"<p>First, you need to generate a tunnel and get its Tunnel Token.</p> <ol> <li> <p>Login to the Cloudflare Zero Trust Dashboard</p> </li> <li> <p>Navigate to Networks &gt; Tunnels.</p> </li> <li> <p>Click Create a tunnel, select cloudflared, and give it a name.</p> </li> <li> <p>On the \"Install connector\" page, select Docker.</p> </li> <li> <p>Copy the Token from the provided command (it's the long string after --token).</p> </li> </ol>"},{"location":"networking/cloudflare/tunnels/#deploy-the-connector","title":"Deploy the Connector","text":"<p>There are multiple ways to deploy a tunnel. This is using a docker <code>compose.yaml</code> file. This allows you to easily manage the container as well as the tunnel together or seperately. </p> <pre><code>services:\n  cloudflared:\n    image: cloudflare/cloudflared:latest\n    container_name: cloudflared\n    restart: unless-stopped\n    environment:\n      - TUNNEL_TOKEN=your_token_here  # Paste your token from step 1\n    command: tunnel --no-autoupdate run\n    networks:\n      - tunnel-net\n\n  my-app:\n    image: your-app-image:latest\n    container_name: my-app\n    networks:\n      - tunnel-net\n    # No ports need to be exposed to the host!\n\nnetworks:\n  tunnel-net:\n    name: tunnel-net\n</code></pre> <p>NOTE: The <code>cloudflared</code> container must be on the same Docker network as the service you want to expose.</p>"},{"location":"networking/cloudflare/tunnels/#route-traffic-public-hostname","title":"Route Traffic (Public Hostname)","text":"<p>Now that the connector is running, tell Cloudflare where to send the traffic:</p> <ol> <li> <p>Back in the Cloudflare Dashboard, go to the Public Hostname tab for your tunnel.</p> </li> <li> <p>Click Add a public hostname.</p> </li> <li> <p>Fill in your details:</p> <pre><code>Subdomain: app\n\nDomain: yourdomain.com\n\nService Type: HTTP\n\nURL: my-app:80 (Use the Docker container name and internal port).\n</code></pre> </li> <li> <p>Save the configuration.</p> </li> </ol>"},{"location":"networking/cloudflare/tunnels/#secure-tunnel","title":"Secure Tunnel","text":"<p>Using an access policy, you can secure the initial access to the tunnel. Multiple options are available including email, Indent providers (github, google, etc) and more. Im using an email for simplicity here. </p>"},{"location":"networking/cloudflare/tunnels/#create-the-access-application","title":"Create the Access Application","text":"<ol> <li> <p>In the Zero Trust Dashboard, navigate to Access &gt; Applications.</p> </li> <li> <p>lick Add an application and select Self-hosted.</p> </li> <li> <p>Application Configuration:</p> <p>Application name: (e.g., My App Protection)</p> <p>Session Duration: How long a user stays logged in.</p> <p>Application domain: Enter the Subdomain and Domain that matches exactly what you set up in your Tunnel (e.g., app.yourdomain.com).</p> </li> <li> <p>Click Next.</p> </li> </ol>"},{"location":"networking/cloudflare/tunnels/#create-access-policy","title":"Create Access Policy","text":"<ol> <li> <p>This defines who is allowed to get through the lock.</p> <p>Policy Name: (e.g., Allow Casey Only)</p> <p>Action: Ensure this is set to Allow.</p> <p>Assign a group (Optional): You can skip this for a simple rule.</p> <p>Configure rules:</p> <pre><code>Include: Select Emails or Email ending in.\n\nRequire: Select Emails.\n\nValue: Enter your specific email address.\n</code></pre> </li> <li> <p>Click Next through the Setup page and then click Add application.</p> </li> </ol> <p>Your app, served at app.domain.com should now be forwarded via a cloudflare tunnel, and secure with an inital check by cloudflare. </p>"},{"location":"networking/npm/overview/","title":"NGINX Proxy Mananger","text":""},{"location":"networking/npm/overview/#github","title":"Github","text":""},{"location":"networking/npm/overview/#overview","title":"Overview","text":"<p>NGINX Proxy Mananger is a user-friendly, web-based interface for configuring Nginx as a reverse proxy, simplifying the management of multiple web services on a single IP address, especially for home labs and self-hosted applications, by offering easy setup for proxy hosts, Let's Encrypt SSL certificates, redirects, access controls, and stream proxying, all within Docker containers. It removes the need for manual Nginx configuration, allowing quick deployment and management of secure web access to internal services through a simple dashboard. </p> <p>Instead of remembering 8000 different <code>IP-ADDR:PORT</code> combos, you can use NPM to set a reverse proxy to <code>service.domain.me</code>. </p>"},{"location":"networking/npm/overview/#installation","title":"Installation","text":""},{"location":"networking/npm/overview/#docker-compose","title":"Docker Compose","text":"<pre><code>services:\n  npm:\n    image: jc21/nginx-proxy-manager:latest\n    container_name: npm\n    restart: unless-stopped\n    ports:\n      - \"80:80\"     # HTTP\n      - \"81:81\"     # Admin UI\n      - \"443:443\"   # HTTPS\n    environment:\n      DB_SQLITE_FILE: \"/data/database.sqlite\"\n    volumes:\n      - ${APPDATA}/ingress/npm/data:/data\n      - ${APPDATA}/ingress/npm/letsencrypt:/etc/letsencrypt\n</code></pre>"},{"location":"networking/npm/overview/#usage","title":"Usage","text":"<ul> <li> <p>Navigate to you <code>IP-ADDR:PORT</code> address, using the Admin UI port set in your <code>compose.yaml</code> above. </p> </li> <li> <p>Login with:</p> <ul> <li>EMAIL: EMAIL@example.com</li> <li>Password: changeme</li> </ul> <p>(probably best to change the password immediately.)</p> </li> <li> <p>Go to SSL Certificates, \"Add Certificates\", select \"Let's Encrypt\". </p> </li> <li> <p>Enter your DDNS address, valid email, enable \"Use a DNS challenge\", choose your DNS provider, add your DDNS token, accept TOS and save.</p> </li> </ul> <p>Note</p> <p>You can get a DDNS from multiple providers including DuckDNS, cloudflare, or porkbun. </p> <p>To set a Host: </p> <ul> <li> <p>Select \"Hosts\" &gt; \"Proxy Hosts\" </p> </li> <li> <p>Add Proxy Host</p> </li> <li> <p>Under Domain Names input desired domain</p> <ul> <li> <p>service.domain.com</p> </li> <li> <p>or whatever you want it to be called</p> </li> </ul> </li> <li> <p>Scheme: HTTP (the reverse proxy w/ your SSL cert will forward to HTTPS).</p> </li> <li> <p>Forward your hostname / IP of the hosting machine/service. </p> </li> <li> <p>Forward Port - use port defined in service. This would be the host port defined in a container. </p> <ul> <li>e.g. a <code>compose.yaml</code> with: <pre><code>ports:\n  - '3005:8080'\n</code></pre> Would use port 3005 as the forwarded IP, not 8080. </li> </ul> </li> <li> <p>Choose your ACL rule. </p> </li> <li> <p>Choose \"Block COmmon Exploits\" and \"Websockets support\" </p> </li> <li> <p>Select the SSL tab at the top</p> <ul> <li> <p>Select the SSL cert made earlier. </p> </li> <li> <p>Force SSL and HTTP/2 support. </p> </li> </ul> </li> <li> <p>Save</p> </li> </ul> <p>Now you can access <code>service.domain.com</code> instead of <code>192.168.3.20:3005</code>.</p>"},{"location":"networking/npm/pihole/","title":"NPM with Pi-Hole","text":"<p>Turns out setting up Pi-Hole with NPM requires extra work and took me entirely too goddamn long to figure that out, so here is the fix. Found this on reddit thread.</p> <p>Nginx Setup</p> <p>On the nginx web gui, add a new Proxy Host.</p> <pre><code>Details\n\n    Domain names = pihole.mydomain.com\n\n    Scheme = http\n\n    Forward Hostname/IP = &lt;pihole_ip&gt;\n\n    Forward Port = 80\n\nSSL\n\nYour Let's Encrypt cert. *.mydomain.com\n\nForce SSL = Yes\n\nHTTP/2 Support = Yes\n</code></pre> <p>In the custom confg (this is the magic) <pre><code> location / {\n proxy_pass http://&lt;pihole_ip&gt;:80/admin/;\n proxy_set_header Host $host;\n proxy_set_header X-Real-IP $remote_addr;\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n proxy_hide_header X-Frame-Options;\n proxy_set_header X-Frame-Options \"SAMEORIGIN\";\n proxy_read_timeout 90;\n }\n\n location /admin/ {\n proxy_pass http://&lt;pihole_ip&gt;:80/admin/;\n proxy_set_header Host $host;\n proxy_set_header X-Real-IP $remote_addr;\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n proxy_hide_header X-Frame-Options;\n proxy_set_header X-Frame-Options \"SAMEORIGIN\";\n proxy_read_timeout 90;\n }\n\n location /api/ {\n proxy_pass http://&lt;pihole_ip&gt;:80/api/;\n proxy_set_header Host $host;\n proxy_set_header X-Real-IP $remote_addr;\n proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n proxy_hide_header X-Frame-Options;\n proxy_set_header X-Frame-Options \"SAMEORIGIN\";\n proxy_read_timeout 90;\n }\n</code></pre> It seems you have to define /, /admin/, and /api/ locations with the full URL with no nginx variables.</p> <p>With this setup, I am able to access Pihole via pihole.mydomain.com on my local network with the full dashboard and graphs working. 10</p> <p>Reddit link</p>"},{"location":"networking/opnsense/opnsense/","title":"OPNSense","text":"<p>OPNSense docs  </p> <p>OPNsense\u00ae is an open source, easy-to-use and easy-to-build FreeBSD based firewall and routing platform.</p> <p>OPNsense includes most of the features available in expensive commercial firewalls, and more in many cases. It brings the rich feature set of commercial offerings with the benefits of open and verifiable sources.</p>"},{"location":"networking/opnsense/opnsense/#installation","title":"Installation","text":"<p>For my installation I used an M920q. This would require 2 things in order to allow for further network connectivity. </p> <ul> <li> <p>Proprietary Lenovo PCIe Riser Card - P/N: 01AJ940</p> </li> <li> <p>INTEL I350T4 1GbE Quad Port Ethernet Server Adapter</p> </li> </ul> <p>For the software installation, the standard installation method can be followed here.</p> <p>Note</p> <p>Pay attention to the inital network interface gievn after installation. This will be needed to access later. </p>"},{"location":"networking/opnsense/opnsense/#initial-configuration","title":"Initial Configuration","text":"<p>After installation, the system will prompt you for the interface assignment. If you ignore this, then the default settings will be applied. Installation ends with the login prompt.</p> <p>By default you have to log in to enter the console.</p> <p>Welcome message</p> <pre><code>* * * Welcome to OPNsense [OPNsense 15.7.25 (amd64/OpenSSL) on OPNsense * * *\n\nWAN (em1)     -&gt; v4/DHCP4: 192.168.2.100/24\nLAN (em0)     -&gt; v4: 192.168.1.1/24\n\nFreeBSD/10.1 (OPNsense.localdomain) (ttyv0)\n\nlogin:\n</code></pre> <p>Tip</p> <p>A user can login to the console menu with his credentials. The default credentials after a fresh install are username \u201croot\u201d and password \u201copnsense\u201d.</p> <p>VLANs and assigning interfaces</p> <p>If you choose to do manual interface assignment or when no config file can be found then you are asked to assign Interfaces and VLANs. VLANs are optional. If you do not need VLANs then choose no. You can always configure VLANs at a later time.</p> <p>LAN, WAN and optional interfaces</p> <p>The first interface is the LAN interface. Type the appropriate interface name, for example \u201cem0\u201d. The second interface is the WAN interface. Type the appropriate interface name, e.g. \u201cem1\u201d . Possible additional interfaces can be assigned as OPT interfaces. If you assigned all your interfaces you can press [ENTER] and confirm the settings. OPNsense will configure your system and present the login prompt when finished.</p> <p>Important</p> <p>The interface is especially important to note as the WAN/LAN port configuration needs to be followed to correctly interface to router/network. </p> <p>Console</p> <p>The console menu shows 13 options.</p> <pre><code>0)     Logout                              7)      Ping host\n1)     Assign interfaces                   8)      Shell\n2)     Set interface(s) IP address         9)      pfTop\n3)     Reset the root password             10)     Filter logs\n4)     Reset to factory defaults           11)     Restart web interface\n5)     Reboot system                       12)     Upgrade from console\n6)     Halt system                         13)     Restore a configuration\n</code></pre> <p>opnsense-update</p> <p>OPNsense features a command line interface (CLI) tool \u201copnsense-update\u201d. Via menu option 8) Shell, the user can get to the shell and use opnsense-update.</p> <p>For help, type man opnsense-update and press [Enter].</p> <p>Upgrade from console</p> <p>The other method to upgrade the system is via console option 12) Upgrade from console</p> <p>GUI</p> <p>An update can be done through the GUI via <code>System \u2023 Firmware \u2023 Updates.</code></p>"},{"location":"networking/opnsense/opnsense/#migration-to-opnsense","title":"Migration to OPNSense","text":"<p>To allow for ease of transitioning to OPNSense from an existing router/firewall (in my case a TP-Link ER605), it is best to setup a few things before installtion the OPNSense router. This can be done with the setup wizard, as well as direct settings.</p> <p>Note</p> <p>The default IP for access is 192.168.1.1. Wizard can be re-run under `System \u2023 Configuration \u2023 Wizard.</p> <ul> <li> <p>1) Under <code>Interfaces \u2023 [WAN]</code>: Ensure your IPv4 configuration Type is set to <code>DHCP</code> (same for IPv6) [if applicable].</p> </li> <li> <p>2) Under <code>Interfaces \u2023 [LAN] \u2023 Static IPv4 configuration</code>: Set the IPv4 address to the same IP as current router. </p> </li> <li> <p>3) Remove existing router, install new OPNSense router. May need flush DNS cache following. </p> </li> </ul>"},{"location":"networking/opnsense/opnsense/#tailscale-dns-migration","title":"Tailscale DNS migration","text":"<ul> <li> <p>Ensure DNS Nameserver is no longer set to pi-hole/previous DNS sinkhole. If using OPNSense as primary DNS, set the Global Nameserver under Tailscale Admin Console &gt; DNS.</p> </li> <li> <p>Reconnect to tailscale using <code>sudo tailscale up --reset --accept-dns=false</code> - this will wipe all previous flags and allow use of the OPNSense DNS. </p> </li> </ul> <p>To install tailscale on OPNSense</p> <p>Tailscale docs   (old)</p> <ul> <li> <p>Go to System &gt; Firmware &gt; Plugins. (Enable community plugins)</p> </li> <li> <p>Install os-tailscale.</p> </li> <li> <p>Once installed, it will appear in VPN &gt; Tailscale. </p> </li> <li> <p>Generate an Auth Key (Tailscale admin &gt; settings &gt; keys)</p> </li> <li> <p>Paste the key, leave server blank </p> </li> <li> <p>Advertise Exit Node - route internet traffic through OPNsense. </p> </li> <li> <p>Accept Subnet Routes - LAN Access</p> </li> <li> <p>Advertised Routes tab &gt; Add your subnet to advertise (e.g. 192.168.1.0/24)</p> </li> <li> <p>In the TS admin console, should see the new machine with the \"subnets\" flag. Hit edit and approve the route. </p> </li> <li> <p>Advertise your LAN subnet (192.168.1.0/24) from OPNsense.</p> </li> </ul>"},{"location":"networking/opnsense/opnsenseadguard/","title":"Setting up AdGuard Home on OPNSense","text":"<p>This guide is long as shit, so just refer to it:</p> <p>https://windgate.net/setup-adguard-home-opnsense-adblocker/</p>"},{"location":"networking/opnsense/opnsensetailscale/","title":"Tailscale &amp; OPNsense Configuration","text":"<p>This covers how to allow remote access to local network via Tailscale (Subnet Routing) and resolve local domains (Split DNS) via AdGuard Home/Unbound.</p>"},{"location":"networking/opnsense/opnsensetailscale/#fix-ip-access-subnet-routing","title":"Fix IP Access (Subnet Routing)","text":"<ul> <li>Makes local IPs (e.g., <code>192.168.0.2:3000</code>) accessible from tailscale remote clients </li> </ul>"},{"location":"networking/opnsense/opnsensetailscale/#1-advertise-the-route-opnsense","title":"1. Advertise the Route (OPNsense)","text":"<ol> <li>Navigate to VPN &gt; Tailscale &gt; Settings.</li> <li>Check Advertise Routes.</li> <li>Enter your LAN subnet (e.g., <code>192.168.0.0/24</code>).</li> <li>Click Save.</li> </ol> <p>CLI Alternative</p> <p>If you are using the CLI instead of the plugin, run: <pre><code>tailscale up --advertise-routes=192.168.0.0/24\n</code></pre></p>"},{"location":"networking/opnsense/opnsensetailscale/#2-approve-the-route-tailscale-console","title":"2. Approve the Route (Tailscale Console)","text":"<ol> <li>Open the Tailscale Admin Console.</li> <li>Find your OPNsense machine in the list.</li> <li>Click the Three Dots (...) &gt; Edit Route Settings.</li> <li>Toggle ON the subnet route (<code>192.168.0.0/24</code>) you previously advertised.</li> </ol>"},{"location":"networking/opnsense/opnsensetailscale/#3-allow-traffic-opnsense-firewall","title":"3. Allow Traffic (OPNsense Firewall)","text":"<p>Without this step, OPNsense will drop packets originating from the Tailscale interface.</p> <ol> <li>Go to Firewall &gt; Rules &gt; Tailscale.<ul> <li>Note: This interface might be named differently if you manually assigned it.</li> </ul> </li> <li>Add a new rule:<ul> <li>Action: Pass</li> <li>Source: <code>Tailscale net</code> (or <code>any</code>)</li> <li>Destination: <code>LAN net</code> (or <code>any</code>)</li> </ul> </li> <li>Click Save and Apply Changes.</li> </ol>"},{"location":"networking/opnsense/opnsensetailscale/#configure-split-dns","title":"Configure Split DNS","text":"<ul> <li> <p>Make local domains (e.g., <code>service.example.com</code>) resolve to the internal Nginx Proxy Manager IP over VPN.</p> </li> <li> <p>Configuring the Unbound/AGH DNS as the global DNS will also block all ads/trackers etc across the whole tailnet. </p> </li> </ul>"},{"location":"networking/opnsense/opnsensetailscale/#1-get-opnsense-tailscale-ip","title":"1. Get OPNsense Tailscale IP","text":"<ol> <li>In OPNsense, go to Interfaces &gt; Overview.</li> <li>Copy the Tailscale IP address (starts with <code>100.x.y.z</code>).</li> </ol>"},{"location":"networking/opnsense/opnsensetailscale/#2-configure-split-dns-tailscale-console","title":"2. Configure Split DNS (Tailscale Console)","text":"<ol> <li>Go to Tailscale Admin Console &gt; DNS.</li> <li>Scroll to Nameservers.</li> <li>Click Add Nameserver &gt; Custom.</li> <li>Configure the following:<ul> <li>Nameserver: Paste the OPNsense Tailscale IP (<code>100.x.y.z</code>) from Step 1.</li> <li>Restrict to domain: Toggle ON.</li> <li>Domain: Enter your local domain (e.g., <code>example.com</code>).</li> </ul> </li> <li>Click Save.</li> </ol> <p>Result</p> <p>When you type <code>example.com</code>, Tailscale will query OPNsense. For all other traffic (e.g., google.com), it will use the client's default DNS.</p>"},{"location":"networking/opnsense/opnsensetailscale/#3-allow-dns-queries-to-npm-for-local-domains-adguard-home","title":"3. Allow DNS Queries to NPM for local domains (AdGuard Home)","text":"<p>Your Adguard Home / Unbound DNS resolver will, by default and by design, look to upstream interent for domain resolution when requested. AdGuard needs to know that <code>*.example.com</code> should be redirected to your Nginx Proxy Manager (NPM) internal IP. For this you need a DNS rewrite in Adguard Home.</p> <ol> <li> <p>Open your AdGuard Home dashboard.</p> </li> <li> <p>Go to Filters &gt; DNS Rewrites.</p> </li> <li> <p>Click Add DNS rewrite.</p> </li> <li> <p>Domain: *.example.com (The * is a wildcard, so it covers service.example.com, nas.example.com, etc.)</p> </li> </ol> <p>Note</p> <p>If AdGuard complains about the wildcard, just add example.com and individual subdomains, but wildcards usually work.</p> <ol> <li> <p>Enter: 192.168.0.x (Enter the LAN IP address of the device running Nginx Proxy Manager).</p> </li> <li> <p>Click Save.</p> </li> </ol>"},{"location":"networking/opnsense/opnsensetailscale/#3a-allow-dns-queries-to-npm-for-local-domains-unbound","title":"3A. Allow DNS Queries to NPM for local domains (Unbound)","text":"<p>If using unbound to forward local requests: </p> <ol> <li> <p>Go to Services &gt; Unbound DNS &gt; Overrides.</p> </li> <li> <p>Click + to add a new override.</p> </li> <li> <p>Domain: example.com</p> </li> <li> <p>IP: 192.168.0.x (Nginx Proxy Manager IP).</p> </li> <li> <p>Description: Local NPM. (or whatever)</p> </li> <li> <p>Click Save and Apply.</p> </li> </ol> <p>Note</p> <p>You usually have to add separate entries for subdomains (e.g., service) unless you check \"Host Overrides\" settings for wildcard support. Adguard works better, found these steps while fixing something so I figured I'd add it here. </p>"},{"location":"networking/opnsense/opnsensetailscale/#interfaces-rebinds","title":"Interfaces &amp; Rebinds","text":"<p>If it still isn't working, check these common configuration errors:</p> <ol> <li>Interface Binding: Ensure AdGuard Home is actually listening on the Tailscale interface (or \"All Interfaces\").</li> <li>Bootstrap DNS: In AdGuard Home (Settings &gt; DNS Settings), set \"Bootstrap DNS servers\" to your OPNsense LAN IP or <code>127.0.0.1</code>.</li> <li>DNS Rebind Check:<ul> <li>Go to System &gt; Settings &gt; General.</li> <li>Ensure DNS Rebind Check is unchecked.</li> <li>Alternatively: Add your specific domain to the whitelist.</li> </ul> </li> </ol> <p>DNS Rebind Protection</p> <p>If \"DNS Rebind Check\" is active, OPNsense may block the DNS response because it resolves a public domain to a private RFC1918 IP address.</p>"},{"location":"networking/opnsense/opnsensetailscale/#verification","title":"Verification","text":"<p>To verify the setup is working:</p> <ol> <li>Disconnect your phone from WiFi (switch to 5G/LTE).</li> <li>Enable the Tailscale VPN.</li> <li>Attempt to access a service via IP: <code>http://192.168.0.2:3000</code> (Should load).</li> <li>Attempt to access a service via Domain: <code>http://service.example.com</code> (Should load).</li> </ol> <p>```</p>"},{"location":"networking/vlan/vlan/","title":"VLANs - OPNSense &amp; Omada Controller","text":"<p>OPNSense Docs  </p> <p>Zenarmor Tutorial  </p> <p>Home Network Guy Youtube  </p> <p>I am only using IPv4 for this guide. Not familiar enough with IPv6, and I don't feel like dealing with for now. I'll come around to it (maybe).</p> <p>This is made for DNSMasq and Unbound DNS specifically, but same ideas apply if using something else. Same for omada controller (for the most part) </p>"},{"location":"networking/vlan/vlan/#vlan-structure","title":"VLAN Structure","text":"<p>I currently have my network setup as this:</p> <p>You can set this however works for you. </p> <ul> <li> <p>VLAN 10: Management (Mgmt) - 10.10.10.x</p> <p>Devices: Omada Switches, WAPs, Omada Controller, Proxmox Web GUI, iDRAC/IPMI.</p> <p>Rules: strict access; only your main PC can reach this.</p> </li> <li> <p>VLAN 20: Trusted LAN - 10.10.20.x</p> <p>Devices: Personal, known devices. </p> </li> <li> <p>VLAN 30: Lab / Server - 10.10.30.x</p> <p>Devices: VMs, Docker Containers. </p> </li> <li> <p>VLAN 50: IoT - 10.10.50.x</p> <p>Devices: Smart plugs, Alexa, Thermostats, random IoT stuff. </p> <p>Rules: Can access Internet, CANNOT access other VLANs.</p> </li> <li> <p>VLAN 99: Guest - 10.10.99.x</p> <p>Devices: Random, new, unkown devices. </p> </li> </ul>"},{"location":"networking/vlan/vlan/#vlan-creation-in-opnsense","title":"VLAN Creation in OPNSense","text":""},{"location":"networking/vlan/vlan/#wan-settings","title":"WAN Settings","text":"<p>Go to Interfaces &gt; [WAN]</p> <ul> <li>IPv4 Configuration Type = DHCP</li> </ul> <p>This will allow your OPNSense router to be able to hand out DHCP addresses to the rest of the network - assuming your ISP allows this.</p>"},{"location":"networking/vlan/vlan/#create-a-new-vlan","title":"Create a New VLAN","text":"<p>Go to Interfaces &gt; Devices &gt; VLAN. Configure each VLAN as:</p> <p>Note</p> <p>I am using the management interface for the examples, same ideas apply for the others. </p> <ul> <li> <p><code>Device</code>: [blank - will auto create name, can be custom if wanted.]</p> </li> <li> <p><code>Parent</code>: igb0 (12.34.56.mac.address) (physical interface that will carry traffic)</p> </li> <li> <p><code>VLAN tag</code>: 10 </p> </li> <li> <p><code>VLAN Priority:</code> Best effort (0, default) </p> </li> <li> <p><code>Description:</code> MANAGEMENT (or whatever you want)</p> </li> <li> <p>Repeat this for all other VLANs</p> </li> </ul>"},{"location":"networking/vlan/vlan/#interface-assignment","title":"Interface Assignment","text":"<p>Go to Interface &gt; Assignments</p> <ul> <li> <p>Assign each new VLAN (device).</p> </li> <li> <p>Match the description name </p> </li> </ul>"},{"location":"networking/vlan/vlan/#vlan-ip-configuration-settings","title":"VLAN IP Configuration Settings","text":"<p>Go to each new interface - [MANAGMENT], [TRSUTED]. etc.</p> <ul> <li> <p>Enable each interface</p> </li> <li> <p>Prevent interface removal </p> </li> <li> <p>IPv4 Configuration Type = Static IPv4</p> </li> <li> <p>Static IPv4 Configuration - this will assign the gateway IP address for your LAN. </p> <ul> <li>IPv4 Address = 10.10.10.1/24 (be sure to change this to /24).</li> </ul> </li> <li> <p>(Usually for LAN/VLANs) leave Block private networks and Block bogon networks unchecked. Enable them only on WAN-like interfaces.</p> </li> </ul>"},{"location":"networking/vlan/vlan/#dhcp-dns-settings","title":"DHCP / DNS Settings","text":""},{"location":"networking/vlan/vlan/#dhcp","title":"DHCP","text":"<p>1. Go to Services &gt;  Dnsmasq DNS &amp; DHCP &gt; General</p> <p>Important</p> <p>Make sure the VLAN interfaces are selected uunder Interface.</p> <p>2. Go to Services &gt;  Dnsmasq DNS &amp; DHCP &gt; DHCP Ranges</p> <ul> <li> <p>Press the + button </p> </li> <li> <p>Select desired interface</p> </li> <li> <p>Start Address = 10.10.10.100</p> </li> <li> <p>End Address = 10.10.10.200</p> </li> <li> <p>Repeat this for all other interfaces, changing 3rd octect.</p> <ul> <li> <p>MANAGEMENT = 10.10.10.100 - 10.10.10.200</p> </li> <li> <p>TRUSTED = 10.10.20.100 - 10.10.20.200</p> </li> <li> <p>ETC.</p> </li> </ul> </li> </ul>"},{"location":"networking/vlan/vlan/#dns","title":"DNS","text":"<p>1. Go to Services &gt; Unbound DNS &gt; General </p> <ul> <li> <p>Listen port: 53</p> </li> <li> <p>Enable DNSSEC Support: Enabled (if upstream DNS supports)</p> </li> <li> <p>Register DHCP Static Mappings: Enabled</p> <ul> <li>This will register all static DHCP reservations. This will auto create a DNS hostname. Handy.</li> </ul> </li> <li> <p>Do not register A/AAAA records: Enabled.</p> <ul> <li> <p>By default, OPNSense will register all IPs for all interfaces to routers hostname (e.g. OPNSense.internal). Hostname lookup will return all IPs, exposing all IPs and gets in the way.</p> </li> <li> <p>Enabling this also allows you to access router web interface more easily from seperate VLAN if using a DNS override to lookup hostname. </p> </li> </ul> </li> <li> <p>Flush DNS Cache during reload: Enabled </p> <ul> <li>Allows changes to apply immediately after reload, instead of waiting for new lease. </li> </ul> </li> </ul>"},{"location":"networking/vlan/vlan/#firewall-rules","title":"Firewall Rules","text":"<p>Tip</p> <p>Basic setup for the rules is: all VLANs can access internet, but not eachother unless specified.</p> <p>1. Go to Firewall &gt; Aliases</p> <p>Aliases will group together IPs and networks, much easier to create rules</p> <ul> <li>Create new Alias by pressing + button. Configure as: </li> </ul> <p><pre><code>Name: PrivateNetworks\n\nType: Network(s) \n\nContent: 10.0.0./8 172.16.0.0/12 192.168.0.0/16 \n    # All private IP ranges\n\nDescription: All private IP addresses\n</code></pre> 2. Go to Firewall &gt; Groups</p> <ul> <li>Configure as:</li> </ul> <pre><code>Name: private\n    # This will generate a network name as \"private net\" like other interfaces, easier to manange. \n\nMembers: LAN and all VLANs - NOT WAN\n\n(no) GUI Groups: Enabled \n    # Will not group them under Interfaces page.\n\nDescription: All private interfaces. \n</code></pre> <p>3. Go to Firewall &gt; Rules </p> <p>This will create 2 rules. One to allow internet access without other VLAN access. The other will </p> <ul> <li>Select MANAGEMENT interface</li> </ul> <p>Note</p> <p>By default on a new interface, all access is blocked. Need to make rules to allow. </p> <ul> <li>Configure first rule as: </li> </ul> <p><pre><code>Action: Pass\n\nInterface: MANAGMENT (should auto fill)\n\nTCP/IP: IPv4 \n\nProtocol: any\n\nSource: MANAGEMENT net\n\nDestination: private net\n\nDestination / Invert: ENABLED # IMPORTANT \n    # This will allow access to internet, but blocking all other private IPs. Inverting the rule above. \n\nDestination Port Range: any | any (can only select if other protocols selected)\n\nDescription: Internet access - block local network. \n</code></pre> * Because first rule blocks all private networks, the MANAGEMENT interface's IP address (10.10.10.1), the second rule will correct that for selected interface only. </p> <ul> <li>Configure second rule as:</li> </ul> <pre><code>Action: Pass\n\nInterface: MANAGMENT (should auto fill)\n\nTCP/IP: IPv4 \n\nProtocol: UDP\n\nSource: MANAGEMENT net\n\nDestination: MANAGMENT address \n\nDestination / Invert: disabled\n\nDestination Port Range: DNS | DNS \n\nDescription: Allow DNS on MANAGMENT network \n</code></pre> <p>Important</p> <p>Repeat this for all other interfaces. Ensuring to change the selected net and addresses for each. For first rule, \"private net\" should be destination on all. You can clone the rules and make it faster. </p>"},{"location":"networking/vlan/vlan/#switch-configuration-omada-controller","title":"Switch Configuration - Omada Controller","text":""},{"location":"networking/vlan/vlan/#vlan-profile-creation","title":"VLAN Profile Creation","text":"<ul> <li> <p>Open the Omada Controller WebUI </p> </li> <li> <p>Go to Settings &gt; Wired and Wireless Networks &gt; LAN</p> </li> <li> <p>Select Create New LAN </p> </li> <li> <p>Configure as:</p> <ul> <li><code>Name</code>: VLAN Name </li> <li><code>Purpose</code>: VLAN</li> <li><code>VLAN</code>: Enter the tag defined in your OPNSense configuration above</li> <li>Other options availabe as needed for different configs are listed. </li> </ul> </li> </ul>"},{"location":"networking/vlan/vlan/#vlan-assignment","title":"VLAN assignment","text":"<ul> <li>Select Devices on sidebar</li> <li>Select Ports<ul> <li>Click the edit icon on the port you want to configure</li> <li>Set the profile to the VLAN profile created in Omada. </li> </ul> </li> <li>This should be working correctly now. </li> </ul>"},{"location":"networking/vlan/vlan/#testing","title":"Testing","text":"<p>Testing confirms whether the VLAN works as expected before deploying it widely.</p> <ol> <li>Connect a device (e.g., laptop or VM) to a switch port assigned to the VLAN.</li> <li>Ensure it receives an IP address from the correct VLAN subnet.</li> <li>Test connectivity (e.g., ping the gateway or browse the internet).</li> <li>Verify that firewall rules are functioning as intended. This helps identify and fix any misconfigurations early.</li> </ol> <p>Linux - <code>ip a</code> should show newly assigned, correct, subnet</p> <p>Windows - <code>ipconfig</code> </p>"},{"location":"overview/overview/","title":"Overview","text":"<p>This is just a reference for all my project stuff. Most of this is just for personal reference and is a total work in progress but it may be useful to someone, who knows. </p>"},{"location":"overview/overview/#current-software","title":"Current software","text":"<p>A list of self-hosted services and docker containers I've run/am running. I'm sure a lot is missing from this list, but other pages above have more than what is listed here.</p>"},{"location":"overview/overview/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Media &amp; Streaming</li> <li>Automation &amp; Downloaders</li> <li>Home Automation &amp; IoT</li> <li>Monitoring &amp; Maintenance</li> <li>Networking &amp; Security</li> <li>Dashboards &amp; Utilities</li> </ul>"},{"location":"overview/overview/#media-sharing","title":"Media &amp; Sharing","text":"<p>Software for organizing, streaming, and managing media content.</p> <ul> <li>Plex - Client-server media player system to stream personal media collections.</li> <li>Samba - The standard Windows interoperability suite of programs for Linux and Unix, used here for network file sharing.</li> <li>copyparty - Portable file server with accelerated resumable uploads, deduplication, WebDAV, FTP, zeroconf, media indexer, video thumbnails, audio transcoding, and write-only folders, in a single file with no mandatory dependencies. </li> <li>Nextcloud - Access and share your files, calendars, contacts, mail and more from any device, on your terms</li> <li>Immich - Photo and video backup solution directly from your mobile phone.</li> </ul>"},{"location":"overview/overview/#automation-downloaders","title":"Automation &amp; Downloaders","text":"<p>Tools for automating media acquisition and managing download queues.</p> <ul> <li>Sonarr - PVR for Usenet and BitTorrent users. It monitors for multiple RSS feeds for new episodes of your favorite shows.</li> <li>Radarr - A fork of Sonarr but for movies. It monitors feeds for new movies and interfaces with clients and indexers.</li> <li>Prowlarr - An indexer manager/proxy built on the popular *arr .net/react stack to integrate with your various PVR apps.</li> <li>Overseerr - A request management and media discovery tool for the Plex ecosystem.</li> <li>qBittorrent - An open-source software alternative to \u00b5Torrent, providing a lightweight BitTorrent client.</li> </ul>"},{"location":"overview/overview/#home-automation-iot","title":"Home Automation &amp; IoT","text":"<p>Services for controlling smart devices and managing local network hardware.</p> <ul> <li>Home Assistant - Open source home automation that puts local control and privacy first.</li> <li>Zigbee2MQTT - Bridges events and allows you to control your Zigbee devices via MQTT.</li> <li>Mosquitto - An open source (EPL/EDL) message broker that implements the MQTT protocol versions 5.0, 3.1.1 and 3.1.</li> <li>Omada Controller - Management software for TP-Link EAP, JetStream switches, and Omada gateways.</li> </ul>"},{"location":"overview/overview/#monitoring-maintenance","title":"Monitoring &amp; Maintenance","text":"<p>Tools to track system health, updates, and logs.</p> <ul> <li>Beszel - A lightweight server monitoring hub and agent with historical data, Docker stats, and alerts.</li> <li>Glances - A cross-platform system monitoring tool written in Python.</li> <li>Uptime Kuma - A fancy self-hosted monitoring tool to check uptime for HTTP(s) / TCP / DNS.</li> <li>Scrutiny - Hard Drive S.M.A.R.T Monitoring, Historical Trends &amp; Real World Failure Thresholds.</li> <li>Dozzle - Real-time log viewer for Docker containers.</li> <li>Watchtower - A process for automating Docker container base image updates.</li> </ul>"},{"location":"overview/overview/#networking-access","title":"Networking &amp; Access","text":"<p>Services handling routing, proxies, and secure access.</p> <ul> <li>Nginx Proxy Manager - A simple way to expose your web services securely and manage SSL certificates.</li> <li>Cloudflared - The command-line client for Cloudflare Tunnel, connecting your local network to the Cloudflare edge.</li> <li>Socket Proxy - A security-enhanced proxy for the Docker socket to control access to the Docker API.</li> </ul>"},{"location":"overview/overview/#dashboards-utilities","title":"Dashboards &amp; Utilities","text":"<p>General productivity tools and start pages.</p> <ul> <li>Homepage - A modern, fully static, fast, secure, and highly customizable application dashboard.</li> <li>IT-Tools - A collection of handy tools for developers and IT people.</li> <li>Bento4 / BentoPDF - Likely a suite of tools for processing media or PDF files (Implied from container name).</li> <li>CouchDB - A database that uses JSON for documents, JavaScript for MapReduce indexes, and regular HTTP for an API. (Used for Obsidian LiveSync or similar backends).</li> </ul>"},{"location":"services/homeassisstant/homeassisstant/","title":"Home Assistant &amp; Zigbee Stack","text":"<p>Home Assistant Github  </p> <p>Z2MQTT Github </p> <p>Eclipse Mosquitto Github </p>"},{"location":"services/homeassisstant/homeassisstant/#overview","title":"Overview","text":"<p>This integrates Home Assistant, Zigbee2MQTT (Z2M), and an MQTT Broker (Mosquitto) to create a local smarthome. </p> <p>I am specifically configuring this for the Sonoff ZBDongle-MG24 (Model \"E\" or \"MG24\"), which runs on the Silicon Labs EFR32MG24 chip. This requires the specific <code>ember</code> driver in Zigbee2MQTT, unlike the older Texas Instruments-based dongles.</p> <p>The stack runs via Docker Compose, with Home Assistant running in <code>host</code> networking mode for optimal device discovery.</p> <p>I'm a big fan of the zigbee system as each device acts as a repeater for the other devices. Also, working of seperate protocol, your devices will continue working even without WiFi.</p>"},{"location":"services/homeassisstant/homeassisstant/#installation","title":"Installation","text":""},{"location":"services/homeassisstant/homeassisstant/#1-hardware-preparation","title":"1. Hardware Preparation","text":"<p>Identify the Dongle The Sonoff MG24 uses a different driver protocol than older models. Before configuring software, ensure the dongle is recognized.</p> <ol> <li>Plug the dongle into a USB 2.0 port (or use a USB 2.0 extension cable to avoid USB 3.0 interference).</li> <li> <p>Run the following to identify your specific device ID: <pre><code>ls -l /dev/serial/by-id/\n</code></pre></p> </li> <li> <p>Copy the output string. It usually looks like <code>usb-ITEAD_SONOFF_Zigbee_...</code> or similar. You will need this for your Compose file.</p> </li> </ol>"},{"location":"services/homeassisstant/homeassisstant/#2-directory-structure","title":"2. Directory Structure","text":"<p>Create a persistent directory structure to store your configurations and data.</p> <pre><code>mkdir -p ~/homelab/smarthome/{homeassistant,zigbee2mqtt/data,mosquitto/config,mosquitto/data,mosquitto/log}\n</code></pre>"},{"location":"services/homeassisstant/homeassisstant/#3-docker-compose","title":"3. Docker Compose","text":"<p>Create your <code>compose.yaml</code> file in <code>~/homelab/smarthome/</code>.</p> <p>Port Mapping</p> <p>This configuration maps the Zigbee2MQTT frontend to port 8082 to avoid conflicts, and maps the USB device. Ensure the <code>devices</code> section matches the ID you found in Step 1.</p> <pre><code>### --- Smart Home Stack: MQTT Broker, Zigbee2MQTT, Home Assistant --- ###\n\nservices:\n  # MQTT Broker\n  mosquitto:\n    container_name: mosquitto\n    image: eclipse-mosquitto\n    restart: unless-stopped\n    ports:\n      - \"1883:1883\"\n      - \"9001:9001\"\n    volumes:\n      - ./mosquitto/config:/mosquitto/config\n      - ./mosquitto/data:/mosquitto/data\n      - ./mosquitto/log:/mosquitto/log\n\n  # Zigbee2MQTT\n  zigbee2mqtt:\n    container_name: zigbee2mqtt\n    image: koenkk/zigbee2mqtt\n    restart: unless-stopped\n    depends_on:\n      - mosquitto\n    volumes:\n      - ./zigbee2mqtt/data:/app/data\n      - /run/udev:/run/udev:ro\n    environment:\n      - TZ=America/Chicago # Update to your timezone\n    devices:\n      # Map the USB dongle directly. UPDATE THIS to match your 'ls -l' output\n      - /dev/serial/by-id/usb-youthings_Zigbee_Adapter-if00:/dev/ttyACM0\n    ports:\n      - \"8082:8080\" # Web Interface\n\n  # Home Assistant \n  homeassistant:\n    container_name: homeassistant\n    image: lscr.io/linuxserver/homeassistant:latest\n    network_mode: host # Critical for Home Assistant discovery\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=America/Chicago\n    volumes:\n      - ./homeassistant:/config\n    restart: unless-stopped\n    depends_on:\n      - mosquitto\n      - zigbee2mqtt\n</code></pre>"},{"location":"services/homeassisstant/homeassisstant/#configuration","title":"Configuration","text":"<p>You must create the configuration files for Mosquitto and Zigbee2MQTT before starting the containers, or they will likely crash on boot.</p>"},{"location":"services/homeassisstant/homeassisstant/#1-mosquitto-broker","title":"1. Mosquitto Broker","text":"<p>Create the file: <code>~/homelab/smarthome/mosquitto/config/mosquitto.conf</code></p> <pre><code>persistence true\npersistence_location /mosquitto/data/\nlog_dest file /mosquitto/log/mosquitto.log\n\n# Listen on all interfaces\nlistener 1883\n\n# Allow anonymous for internal local network setup\nallow_anonymous true\n</code></pre>"},{"location":"services/homeassisstant/homeassisstant/#2-zigbee2mqtt","title":"2. Zigbee2MQTT","text":"<p>Create the file: <code>~/homelab/smarthome/zigbee2mqtt/data/configuration.yaml</code></p> <p>Driver Selection</p> <p>The <code>adapter: ember</code> setting is critical for the Sonoff MG24. If you omit this, Z2M will try to use the default driver and fail to start.</p> <pre><code># Home Assistant Integration\nhomeassistant: true\n\n# MQTT Settings\nmqtt:\n  base_topic: zigbee2mqtt\n  server: 'mqtt://mosquitto:1883'\n\n# Serial Settings for Sonoff MG24 (Ember Driver)\nserial:\n  # This path inside the container is mapped from your host in the compose file\n  port: /dev/ttyACM0 \n  adapter: ember\n\n# Frontend Settings (GUI)\nfrontend:\n  port: 8080\n\n# Zigbee Network\npermit_join: false\n</code></pre>"},{"location":"services/homeassisstant/homeassisstant/#setup-device-pairing","title":"Setup &amp; Device Pairing","text":"<p>Once the files are created, launch the stack:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"services/homeassisstant/homeassisstant/#pairing-devices-example-thirdreality-zl1-bulb","title":"Pairing Devices (Example: ThirdReality ZL1 Bulb)","text":"<ol> <li>Access Z2M: Open <code>http://&lt;server-ip&gt;:8082</code>.</li> <li>Permit Join: Click Permit Join (All) in the top bar to allow new devices.</li> <li>Reset Bulb:</li> <li>Screw in the ThirdReality ZL1 bulb.</li> <li>Power Cycle 5 Times: Turn the switch OFF and ON 5 times quickly (approx. 1 second per toggle).</li> <li>Sequence: ON -&gt; OFF -&gt; ON -&gt; OFF -&gt; ON -&gt; OFF -&gt; ON -&gt; OFF -&gt; ON.</li> <li> <p>Confirmation: The bulb will flash a sequence of colors (Warm White -&gt; Cool White -&gt; Red -&gt; Green -&gt; Blue) and stay solid Warm White.</p> </li> <li> <p>Rename: Once it appears in the Z2M dashboard, click the blue \"Edit\" icon. Give it a friendly name (e.g., \"Office_Lamp\") and ensure \"Update Home Assistant entity ID\" is checked.</p> </li> </ol>"},{"location":"services/homeassisstant/homeassisstant/#connecting-to-home-assistant","title":"Connecting to Home Assistant","text":"<ol> <li>Access HA: Open <code>http://&lt;server-ip&gt;:8123</code>.</li> <li>Integrations: Go to Settings &gt; Devices &amp; Services.</li> <li>Discovery: HA should auto-discover \"MQTT\". If not, click Add Integration &gt; MQTT.</li> <li>Broker Settings:</li> <li>Broker: <code>localhost</code> (Since HA is on the host network).</li> <li>Port: <code>1883</code>.</li> <li> <p>Auth: Leave blank (I enabled <code>allow_anonymous</code>).</p> </li> <li> <p>Verify: Your \"Office_Lamp\" should now appear as a device in Home Assistant.</p> </li> </ol>"},{"location":"services/homeassisstant/homeassisstant/#maintenance","title":"Maintenance","text":""},{"location":"services/homeassisstant/homeassisstant/#updates","title":"Updates","text":"<p>To update the stack, pull the latest images and recreate the containers. Your data is safe in the persistent volumes.</p> <pre><code>docker compose pull\ndocker compose up -d\n</code></pre>"},{"location":"services/homeassisstant/homeassisstant/#troubleshooting","title":"Troubleshooting","text":"<p>If Zigbee2MQTT fails to start, check the logs immediately:</p> <pre><code>docker compose logs zigbee2mqtt\n</code></pre> <ul> <li>Error: \"Error: Error: No such file or directory...\": Check your USB mapping in <code>compose.yaml</code>. The ID <code>usb-youthings...</code> or <code>usb-ITEAD...</code> must match exactly what <code>ls -l /dev/serial/by-id/</code> shows.</li> <li>Error: \"Adapter failed to start\": Ensure <code>adapter: ember</code> is in your <code>configuration.yaml</code>.</li> </ul>"},{"location":"services/immich/immich/","title":"Immich Photo Backup","text":""},{"location":"services/immich/immich/#overview","title":"Overview","text":"<p>Immich is a self-hosted, high-performance photo and video backup solution that aims to directly replace Google Photos. Unlike traditional file managers, Immich relies heavily on machine learning to provide features like facial recognition, object detection (e.g., \"search for 'cat'\"), and map-based exploration.</p> <p>The architecture is microservice-based, meaning it runs several distinct containers (Server, Machine Learning, Postgres, Redis) that work in concert to handle massive upload queues and background processing tasks.</p>"},{"location":"services/immich/immich/#installation","title":"Installation","text":""},{"location":"services/immich/immich/#docker-compose","title":"Docker Compose","text":"<p>Immich is designed to be run via Docker Compose. While there is a monolithic \"all-in-one\" container available, it is not recommended for production or heavy use. This guide uses the official <code>ghcr.io</code> images.</p>"},{"location":"services/immich/immich/#prerequisites","title":"Prerequisites","text":"<p>Because Immich uses modular configuration files for hardware acceleration, you must download two helper files into your directory before running the compose file.</p> <pre><code>wget [https://github.com/immich-app/immich/releases/latest/download/hwaccel.ml.yml](https://github.com/immich-app/immich/releases/latest/download/hwaccel.ml.yml)\nwget [https://github.com/immich-app/immich/releases/latest/download/hwaccel.transcoding.yml](https://github.com/immich-app/immich/releases/latest/download/hwaccel.transcoding.yml)\n</code></pre>"},{"location":"services/immich/immich/#compose-configuration","title":"Compose Configuration","text":"<p>Create a <code>.env</code> file in the same directory to handle your storage locations and database passwords.</p> <p>Hardware Note</p> <p>This configuration is optimized for an Intel i5-8500T using Intel QuickSync and OpenVINO. If you have a dedicated GPU, you will need to adjust the <code>service: quicksync</code> and <code>service: openvino</code> lines.</p> <pre><code>#\n# WARNING: To install Immich, follow our guide: [https://docs.immich.app/install/docker-compose](https://docs.immich.app/install/docker-compose)\n#\n# Make sure to use the docker-compose.yml of the current release:\n#\n# [https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml](https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml)\n#\n# The compose file on main may not be compatible with the latest release.\n\nname: immich\n\nservices:\n  immich-server:\n    container_name: immich_server\n    image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release}\n    extends:\n      file: hwaccel.transcoding.yml\n      service: quicksync # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding\n    volumes:\n      # Do not edit the next line. If you want to change the media storage location on your system, edit the value of UPLOAD_LOCATION in the .env file\n      - ${UPLOAD_LOCATION}:/data\n      - /etc/localtime:/etc/localtime:ro\n    env_file:\n      - .env\n    ports:\n      - '2283:2283'\n    depends_on:\n      - redis\n      - database\n    restart: always\n    healthcheck:\n      disable: false\n\n  immich-machine-learning:\n    container_name: immich_machine_learning\n    # For hardware acceleration, add one of -[armnn, cuda, rocm, openvino, rknn] to the image tag.\n    # Example tag: ${IMMICH_VERSION:-release}-cuda\n    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}-openvino\n    extends: # uncomment this section for hardware acceleration - see [https://docs.immich.app/features/ml-hardware-acceleration](https://docs.immich.app/features/ml-hardware-acceleration)\n      file: hwaccel.ml.yml\n      service: openvino # set to one of [armnn, cuda, rocm, openvino, openvino-wsl, rknn] for accelerated inference - use the `-wsl` version for WSL2 where applicable\n    volumes:\n      - model-cache:/cache\n    env_file:\n      - .env\n    restart: always\n    healthcheck:\n      disable: false\n\n  redis:\n    container_name: immich_redis\n    image: docker.io/valkey/valkey:9@sha256:fb8d272e529ea567b9bf1302245796f21a2672b8368ca3fcb938ac334e613c8f\n    healthcheck:\n      test: redis-cli ping || exit 1\n    restart: always\n\n  database:\n    container_name: immich_postgres\n    image: ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23\n    environment:\n      POSTGRES_PASSWORD: ${DB_PASSWORD}\n      POSTGRES_USER: ${DB_USERNAME}\n      POSTGRES_DB: ${DB_DATABASE_NAME}\n      POSTGRES_INITDB_ARGS: '--data-checksums'\n      # Uncomment the DB_STORAGE_TYPE: 'HDD' var if your database isn't stored on SSDs\n      # DB_STORAGE_TYPE: 'HDD'\n    volumes:\n      # Do not edit the next line. If you want to change the database storage location on your system, edit the value of DB_DATA_LOCATION in the .env file\n      - ${DB_DATA_LOCATION}:/var/lib/postgresql/data\n    shm_size: 128mb\n    restart: always\n\nvolumes:\n  model-cache:\n</code></pre>"},{"location":"services/immich/immich/#initial-setup","title":"Initial Setup","text":"<p>After ensuring the helper YAML files and <code>.env</code> are present, run <code>docker compose up -d</code>.</p> <p>Navigate to <code>http://your.server.ip:2283</code> to see the \"Getting Started\" wizard.</p> <ol> <li>Click Getting Started.</li> <li>Create Admin Account: Enter your email, name, and password.</li> <li>Log in: You will be taken to the main timeline.</li> </ol>"},{"location":"services/immich/immich/#hardware-acceleration","title":"Hardware Acceleration","text":""},{"location":"services/immich/immich/#video-transcoding-intel-quicksync","title":"Video Transcoding (Intel QuickSync)","text":"<p>Why use it: When you view a 4K video on a phone over a slow connection, Immich must transcode (shrink) that video in real-time. Doing this on the CPU will pin your processor to 100% usage and create stuttering. QuickSync offloads this to the iGPU.</p> <p>How to configure: The <code>hwaccel.transcoding.yml</code> in the compose file handles the backend permissions, but you must enable it in the UI.</p> <ol> <li>Go to Administration (top right) -&gt; Settings -&gt; Video Transcoding.</li> <li>Find Hardware Acceleration.</li> <li>Select Quick Sync from the dropdown.</li> <li>Click Save.</li> </ol>"},{"location":"services/immich/immich/#machine-learning-openvino","title":"Machine Learning (OpenVINO)","text":"<p>Why use it: Immich scans every photo for faces and objects. On a raw CPU, scanning 1,000 photos can take hours. OpenVINO optimizes these mathematical operations for Intel CPUs, significantly increasing indexing speed.</p> <p>How to configure: There is no UI toggle for this. It is controlled entirely by the image tag (<code>-openvino</code>) and the <code>extends</code> block in the compose file.</p> <p>Verification: Run the following to check if it loaded correctly:</p> <pre><code>docker compose logs immich-machine-learning | grep \"OpenVINO\"\n</code></pre> <ul> <li>Success: You will see a line stating <code>Loaded execution provider: OpenVINO</code>.</li> <li>Failure: If you only see <code>CPUExecutionProvider</code>, check that your image tag ends in <code>-openvino</code>.</li> </ul>"},{"location":"services/immich/immich/#database-storage","title":"Database &amp; Storage","text":""},{"location":"services/immich/immich/#postgresql-with-pgvector","title":"PostgreSQL with pgvector","text":"<p>Immich requires a specific version of Postgres equipped with <code>pgvector</code>. This extension allows the database to store \"vector embeddings\"\u2014mathematical representations of images. This enables smart searches (e.g., searching for \"red truck\" finds trucks without manual tags).</p> <p>Performance Tip</p> <p>Unlike Nextcloud, you rarely touch the DB config manually. However, ensure <code>DB_DATA_LOCATION</code> in your <code>.env</code> points to fast storage (SSD/NVMe). Vector searches on HDDs are noticeably slower.</p>"},{"location":"services/immich/immich/#storage-template","title":"Storage Template","text":"<p>By default, Immich uploads files into a generic folder structure. If you ever want to access your files directly on the disk (outside of Immich), this is difficult to navigate.</p> <p>The Fix: Configure this before uploading your library.</p> <ol> <li>Go to Administration -&gt; Settings -&gt; Storage Template.</li> <li>Enabled: Toggle ON.</li> <li>Template: Select or customize your structure.</li> <li>Recommendation: <code>{{y}}/{{y}}-{{MM}}-{{dd}}/{{filename}}</code></li> <li> <p>Result: <code>/2024/2024-12-25/IMG_1234.jpg</code></p> </li> <li> <p>Click Save.</p> </li> </ol>"},{"location":"services/immich/immich/#post-install-monitoring","title":"Post-Install Monitoring","text":""},{"location":"services/immich/immich/#gpu-utilization","title":"GPU Utilization","text":"<p>Since you are using hardware acceleration, verify the load is hitting the GPU and not the CPU.</p> <p>The Tool: <code>intel_gpu_top</code></p> <ol> <li> <p>Install on Host: <pre><code>sudo apt install intel-gpu-tools\n</code></pre></p> </li> <li> <p>Run Monitor: <pre><code>sudo intel_gpu_top\n</code></pre></p> </li> <li> <p>Test: Open Immich and scroll rapidly through the timeline (triggers thumbnail generation) or play a large video. You should see the <code>Video</code> or <code>Render</code> bars spike.</p> </li> </ol> <p>Sources:</p> <ul> <li>Immich Hardware Acceleration Guide</li> <li>Recommended Compose File</li> </ul>"},{"location":"services/immich/takeoutimport/","title":"Importing your photos from Google Takeout","text":""},{"location":"services/immich/takeoutimport/#creating-takeout-link","title":"Creating takeout link","text":"<ul> <li> <p>Go to https://takeout.google.com/</p> </li> <li> <p>Deselect all, then select Google Photos. Scroll to bottom and select \"next step\"</p> </li> <li> <p>Get the links via email, choose 50GB file size. </p> </li> </ul> <p>Note</p> <p>It may take awhile to generate the link</p>"},{"location":"services/immich/takeoutimport/#downloading-the-files","title":"Downloading the files","text":""},{"location":"services/immich/takeoutimport/#standard-method","title":"Standard method","text":"<p>Download the links like any other download, save to wherever works, and import to immich using <code>immich-go</code> </p>"},{"location":"services/immich/takeoutimport/#headless-download","title":"Headless download","text":"<p>Note</p> <p>you can only download a link 5 times. After that, you must regenerate the takeout link again.</p> <ul> <li> <p>Go to the generated takeout link. </p> </li> <li> <p>Press <code>F12</code> to open the developer tools of browser, then choose 'network'</p> </li> <li> <p>Start a download on browser, then pause it.</p> </li> <li> <p>Find the download from the domain <code>https://takeout-download.usercontent.google.com</code> - IT MUST BE THIS DOMAIN</p> </li> <li> <p>Right click -&gt; 'Copy as cURL'</p> </li> <li> <p>Create a ssh session using <code>tmux</code> or <code>screen</code> to keep download going if terminal dies.</p> </li> <li> <p>This will make a long string, paste to a text editor and add <code>-O</code> (- Capital O). This will write to file instead of stdout.</p> </li> <li> <p>This will begin download. Then after all <code>.zip</code> files are there, import using <code>immich-go</code></p> </li> </ul>"},{"location":"services/nextcloud/nextcloud/","title":"Nextcloud Storage","text":"<p>NextCloud Github  </p> <p>Nextcloud linuxserverio image  </p>"},{"location":"services/nextcloud/nextcloud/#overview","title":"Overview","text":"<p>Nextcloud is a self-hosted, open-source productivity platform that functions as a private alternative to services like Google Drive or Dropbox. It provides file storage, synchronization, and sharing, but can be expanded via \"apps\" to include contacts, calendars, office suites, and more.</p> <p>While the core functionality manages files, the architecture requires a web server, PHP runtime, a database for metadata, and a caching mechanism for performance.</p>"},{"location":"services/nextcloud/nextcloud/#installation","title":"Installation","text":""},{"location":"services/nextcloud/nextcloud/#docker-compose","title":"Docker Compose","text":"<p>While there are multiple ways to install Nextcloud, this guide uses Docker Compose for easier configuration and maintenance.</p> <p>Image Choice</p> <p>I read a lot of people having issues with the official nextcloud docker image so I used the LinuxServer.io image. It is apparently generally more stable. </p>"},{"location":"services/nextcloud/nextcloud/#compose-configuration","title":"Compose Configuration","text":"<p>Create a <code>compose.yaml</code> file. You should also create a <code>.env</code> file in the same directory to store your secrets (passwords and users).</p> <pre><code>services:\n  nextcloud:\n    image: lscr.io/linuxserver/nextcloud:latest\n    container_name: nextcloud\n    environment:\n      - PUID=1000   # Change to your user ID (run 'id' in terminal)\n      - PGID=1000   # Change to your group ID\n      - TZ=America/Chicago\n    volumes:\n      - /mnt/appdata/nextcloud/config:/config # Configuration files (appdata ssd)\n      - /mnt/storage/nextcloud:/data  # This is where your actual files/photos will live\n    ports:\n      # - 443:443 # Default HTTPS port (uncomment if you want to use it)\n      - 4043:443 \n    depends_on:\n      - nextcloud_db\n      - nextcloud_redis\n    restart: unless-stopped\n\n  nextcloud_db:\n    image: postgres:15-alpine\n    container_name: nextcloud_db\n    environment:\n      - POSTGRES_USER=${POSTGRES_USER}\n      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}  # CHANGE THIS in .env!\n      - POSTGRES_DB=${POSTGRES_DB} \n    volumes:\n      - /mnt/appdata/nextcloud/db_data:/var/lib/postgresql/data\n    restart: unless-stopped\n\n  nextcloud_redis:\n    image: redis:alpine\n    container_name: nextcloud_redis\n    restart: unless-stopped\n\nvolumes:\n  db_data:\n</code></pre>"},{"location":"services/nextcloud/nextcloud/#initial-setup","title":"Initial Setup","text":"<p>After creating your <code>compose.yaml</code> and <code>.env</code> files, run <code>docker compose up -d</code> to start the containers.</p> <p>Navigate to <code>https://your.server.ip:4043</code> to access the setup wizard.</p> <ol> <li>Create Admin Account: Enter your desired username and password.</li> <li>Database Configuration: Click \"Storage &amp; Database\" to expand the options.</li> <li>Select Database: Choose PostgreSQL.</li> </ol> <p>Database Hostname</p> <p>When asked for \"Database host\", do not use <code>localhost</code> or an IP. Use the container name: <code>nextcloud_db</code>.</p> <p>Ensure the user, password, and database name match exactly what you defined in your <code>.env</code> file.</p>"},{"location":"services/nextcloud/nextcloud/#database-caching","title":"Database &amp; Caching","text":""},{"location":"services/nextcloud/nextcloud/#postgresql","title":"PostgreSQL","text":"<p>Why use it: By default, Nextcloud can run on SQLite (file-based). However, SQLite performs poorly with concurrent connections and slows down significantly as your file count grows. PostgreSQL is an enterprise-class database that handles concurrent connections efficiently, providing stability for metadata operations.</p> <p>How to configure: This is handled during the initial setup wizard (see above).</p>"},{"location":"services/nextcloud/nextcloud/#redis","title":"Redis","text":"<p>Why use it: Redis is an in-memory data structure store used for: 1.  Memory Caching: Speeds up the web interface by storing frequently accessed data in RAM. 2.  Transactional File Locking: Prevents file corruption when multiple devices (phone, desktop) access the same file simultaneously.</p> <p>How to configure: The <code>nextcloud_redis</code> container is running, but Nextcloud needs to be told to use it.</p> <ol> <li>Locate your <code>config.php</code> file (mapped in your volume, e.g., <code>/mnt/appdata/nextcloud/config/www/nextcloud/config/config.php</code>).</li> <li>Add or modify the following lines inside the <code>$CONFIG = array ( ... );</code> block:</li> </ol> <pre><code>  'memcache.local' =&gt; '\\\\OC\\\\Memcache\\\\APCu',\n  'memcache.distributed' =&gt; '\\\\OC\\\\Memcache\\\\Redis',\n  'memcache.locking' =&gt; '\\\\OC\\\\Memcache\\\\Redis',\n  'redis' =&gt; \n  array (\n    'host' =&gt; 'nextcloud_redis',\n    'port' =&gt; 6379,\n  ),\n</code></pre>"},{"location":"services/nextcloud/nextcloud/#performance-tuning","title":"Performance Tuning","text":""},{"location":"services/nextcloud/nextcloud/#a-php-memory-limit","title":"A. PHP Memory Limit","text":"<p>By default, the container often limits PHP to 512MB, which causes sluggishness or crashes during heavy photo uploads.</p> <p>The Fix: Set the limit via environment variables in your <code>compose.yaml</code>:</p> <p><pre><code>services:\n  nextcloud:\n    # ... existing config ...\n    environment:\n      - PHP_MEMORY_LIMIT=2G\n      - PHP_UPLOAD_LIMIT=16G # Adjust based on your needs\n</code></pre> Run <code>docker compose up -d</code> to apply changes.</p>"},{"location":"services/nextcloud/nextcloud/#b-preview-generator","title":"B. Preview Generator","text":"<p>Nextcloud generates image thumbnails \"on the fly\" when you open a folder. If you open a folder with 1,000 photos, the server will freeze trying to generate them all at once.</p> <p>The Fix: Install the Preview Generator app to pre-render thumbnails in the background.</p> <ol> <li>Install App: Go to Apps (top right menu) &gt; Search \"Preview Generator\" &gt; Download &amp; Enable.</li> <li> <p>Run Initial Scan:</p> <p>Tip</p> <p>This command can take a long time. It is recommended to run this inside a <code>screen</code> or <code>tmux</code> session so it doesn't fail if your SSH connection drops.</p> <pre><code>docker exec -it nextcloud occ preview:generate-all -vvv\n</code></pre> </li> <li> <p>Add to Cron:     To ensure new photos get thumbnails automatically, open your host crontab (<code>crontab -e</code>) and add this line to run every 10 minutes:</p> <pre><code>*/10 * * * * docker exec -u abc nextcloud occ preview:pre-generate\n</code></pre> </li> </ol> <p>Sources:</p> <ul> <li> <p>Nextcloud Database Configuration</p> </li> <li> <p>Nextcloud Caching &amp; Locking</p> </li> </ul>"},{"location":"services/omada/omada/","title":"TP-Link Omada Controller","text":""},{"location":"services/omada/omada/#overview","title":"Overview","text":"<p>The Omada Software Defined Networking (SDN) platform is TP-Link's centralized management solution. It manages all your TP-Link hardwaer from a single interface.</p> <p>I have not gotten too into the deeper settings of omada, but this is basics from what I did. </p>"},{"location":"services/omada/omada/#installation","title":"Installation","text":""},{"location":"services/omada/omada/#1-volume-creation","title":"1. Volume Creation","text":"<p>The <code>compose.yaml</code> configuration defines the storage volumes as <code>external: true</code>. This means Docker will not create them automatically. You must create them manually before launching the container, or the stack will fail to start.</p> <pre><code>docker volume create omada_data\ndocker volume create omada_logs\ndocker volume create omada_work\n</code></pre>"},{"location":"services/omada/omada/#2-docker-compose","title":"2. Docker Compose","text":"<p>This setup uses the community-standard <code>mbentley/omada-controller</code> image. The container is configured with <code>network_mode: host</code> to ensure the controller can easily discover devices on the local network via Layer 2 broadcasts.</p> <p><code>compose.yaml</code></p> <pre><code>### Omada Controller - TP-Link / Omada network controller ###\n\nservices:\n  omada-controller:\n    image: mbentley/omada-controller:latest\n    container_name: omada-controller\n    restart: always\n    network_mode: host\n    environment:\n      - MANAGE_HTTP_PORT=8088\n      - MANAGE_HTTPS_PORT=8043\n      - PORTAL_HTTP_PORT=8088\n      - PORTAL_HTTPS_PORT=8843\n      - SHOW_SERVER_LOGS=true\n      - SHOW_MONGODB_LOGS=false\n      - TZ=America/Chicago\n    # networks: \n    #   - proxy_net  \n    volumes:   \n      - omada_data:/opt/tplink/EAPController/data\n      - omada_logs:/opt/tplink/EAPController/logs\n      - omada_work:/opt/tplink/EAPController/work\n\nvolumes:\n  omada_data:\n    external: true\n  omada_logs:\n    external: true\n  omada_work:\n    external: true\n\n# # --- Network Definitions --- #\n# networks: \n#   proxy_net: \n#     external: true\n</code></pre> <p>Start the controller:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"services/omada/omada/#initial-setup-adoption","title":"Initial Setup &amp; Adoption","text":"<p>Once the controller is running, follow this workflow to bring the network online.</p>"},{"location":"services/omada/omada/#1-access-the-interface","title":"1. Access the Interface","text":"<ul> <li>URL: <code>https://&lt;server-ip&gt;:8043</code></li> <li>Note: You may receive a security warning because the SSL certificate is self-signed. This is normal; proceed past it.</li> <li>First Login: The default is often <code>admin</code> / <code>admin</code>, but the setup wizard will force a password change immediately.</li> </ul>"},{"location":"services/omada/omada/#2-device-adoption","title":"2. Device Adoption","text":"<p>The controller does not automatically manage devices plugged into the network; they must be explicitly \"Adopted\".</p> <ol> <li>Go to the Devices tab (Access Point icon).</li> <li>Devices (Gateway, Switches, APs) should be listed as \"Pending\".</li> <li>Click the Adopt button on the right.</li> <li>The status will cycle: Provisioning -&gt; Configuring -&gt; Connected.</li> </ol>"},{"location":"services/omada/omada/#3-wanlan-configuration","title":"3. WAN/LAN Configuration","text":"<p>If using an Omada Gateway (like the ER605):</p> <ul> <li>Navigate to Settings (Gear icon) &gt; Wired Networks &gt; Internet.</li> <li>Configure the WAN type provided by the ISP (DHCP, PPPoE, or Static).</li> </ul>"},{"location":"services/omada/omada/#configuration-features","title":"Configuration Features","text":""},{"location":"services/omada/omada/#wireless-networks-ssids","title":"Wireless Networks (SSIDs)","text":"<ul> <li>Location: Settings &gt; Wireless Networks.</li> <li>Usage: Create multiple SSIDs here (e.g., Main, IoT, Guest).</li> <li>Guest Network: Checking the \"Guest Network\" box automatically applies isolation rules, preventing those clients from accessing the internal LAN.</li> </ul>"},{"location":"services/omada/omada/#vlans-virtual-lans","title":"VLANs (Virtual LANs)","text":"<p>To segment traffic (e.g., separating IoT devices from the main PC):</p> <ol> <li>Create Interface: Settings &gt; Wired Networks &gt; LAN. Create a new Interface with a VLAN ID (e.g., <code>20</code>).</li> <li>Tag Ports: Ensure managed switches utilize the correct profiles to pass this VLAN tag to the APs.</li> <li>Assign to Wi-Fi: In Wireless settings, assign an SSID to use that specific VLAN ID.</li> </ol>"},{"location":"services/omada/omada/#acls-access-control-lists","title":"ACLs (Access Control Lists)","text":"<ul> <li>Location: Settings &gt; Network Security &gt; ACL.</li> <li>Usage: Use \"Switch ACLs\" or \"Gateway ACLs\" to block traffic between VLANs (e.g., Block the IoT VLAN from accessing the Main VLAN).</li> </ul>"},{"location":"services/omada/omada/#maintenance","title":"Maintenance","text":"<ul> <li>Backups: Go to Settings &gt; Maintenance &gt; Backup &amp; Restore. Set up \"Auto Backup\" to save the config locally. Since the <code>omada_data</code> volume is mapped, these backups persist on the disk.</li> <li>Roaming: Enable Fast Roaming (802.11r) in Site Settings to help devices switch between APs smoothly. Note: Test this first, as some older IoT devices struggle with it.</li> <li>AI Optimization: The \"AI WLAN Optimization\" tool scans the RF environment and automatically adjusts channels and transmit power to reduce interference. Run this once after the initial deployment.</li> </ul>"},{"location":"services/pihole/pihole/","title":"Pi-hole DNS Blocker","text":""},{"location":"services/pihole/pihole/#overview","title":"Overview","text":"<p>The Pi-hole\u00ae is a DNS sinkhole that protects your devices from unwanted content without installing any client-side software.</p>"},{"location":"services/pihole/pihole/#installation-methods","title":"Installation Methods","text":""},{"location":"services/pihole/pihole/#option-1-docker-compose-recommended","title":"Option 1: Docker Compose (Recommended)","text":"<p>This is the preferred method for this homelab setup. It integrates easily with existing Docker networks and storage.</p> <pre><code># More info at [https://github.com/pi-hole/docker-pi-hole/](https://github.com/pi-hole/docker-pi-hole/) and [https://docs.pi-hole.net/](https://docs.pi-hole.net/)\nservices:\n  pihole:\n    container_name: pihole\n    image: pihole/pihole:latest\n    ports:\n      # DNS Ports\n      - \"53:53/tcp\"\n      - \"53:53/udp\"\n      # Default HTTP Port\n      - \"80:80/tcp\"\n      # Default HTTPs Port. FTL will generate a self-signed certificate\n      - \"443:443/tcp\"\n      # Uncomment the below if using Pi-hole as your DHCP Server\n      #- \"67:67/udp\"\n      # Uncomment the line below if you are using Pi-hole as your NTP server\n      #- \"123:123/udp\"\n    environment:\n      # Set the appropriate timezone for your location from\n      # [https://en.wikipedia.org/wiki/List_of_tz_database_time_zones](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones), e.g:\n      TZ: 'Europe/London'\n      # Set a password to access the web interface. Not setting one will result in a random password being assigned\n      FTLCONF_webserver_api_password: 'correct horse battery staple'\n      # If using Docker's default `bridge` network setting the dns listening mode should be set to 'ALL'\n      FTLCONF_dns_listeningMode: 'ALL'\n    # Volumes store your data between container upgrades\n    volumes:\n      # For persisting Pi-hole's databases and common configuration file\n      - './etc-pihole:/etc/pihole'\n      # Uncomment the below if you have custom dnsmasq config files that you want to persist. Not needed for most starting fresh with Pi-hole v6. If you're upgrading from v5 you and have used this directory before, you should keep it enabled for the first v6 container start to allow for a complete migration. It can be removed afterwards. Needs environment variable FTLCONF_misc_etc_dnsmasq_d: 'true'\n      #- './etc-dnsmasq.d:/etc/dnsmasq.d'\n    cap_add:\n      # See [https://github.com/pi-hole/docker-pi-hole#note-on-capabilities](https://github.com/pi-hole/docker-pi-hole#note-on-capabilities)\n      # Required if you are using Pi-hole as your DHCP server, else not needed\n      - NET_ADMIN\n      # Required if you are using Pi-hole as your NTP client to be able to set the host's system time\n      - SYS_TIME\n      # Optional, if Pi-hole should get some more processing time\n      - SYS_NICE\n    restart: unless-stopped\n</code></pre> <p>Note On Capabilities</p> <p>FTLDNS expects capabilities like <code>CAP_NET_BIND_SERVICE</code> (binding port 53) and <code>CAP_NET_ADMIN</code> (DHCP).</p> <pre><code>The docker image automatically grants these if available. We explicitly add `NET_ADMIN` in the compose file above to ensure smooth operation.\n</code></pre>"},{"location":"services/pihole/pihole/#option-2-automated-install-bare-metal","title":"Option 2: Automated Install (Bare Metal)","text":"<p>If you prefer to install directly on the OS:</p> <pre><code>curl -sSL [https://install.pi-hole.net](https://install.pi-hole.net) | bash\n</code></pre> <p>Security Warning</p> <p>Piping to <code>bash</code> is a controversial topic. It prevents you from reading code before it runs. If you prefer to review the code first, use the methods below.</p> <p>Alternatives:</p> <ul> <li> <p>Clone &amp; Run: <pre><code>git clone --depth 1 [https://github.com/pi-hole/pi-hole.git](https://github.com/pi-hole/pi-hole.git) Pi-hole\ncd \"Pi-hole/automated install/\"\nsudo bash basic-install.sh\n</code></pre></p> </li> <li> <p>Download Manually: <pre><code>wget -O basic-install.sh [https://install.pi-hole.net](https://install.pi-hole.net)\nsudo bash basic-install.sh\n</code></pre></p> </li> </ul>"},{"location":"services/pihole/pihole/#configuration-network-setup","title":"Configuration &amp; Network Setup","text":"<p>Once installed, you must configure your network to actually use the Pi-hole.</p>"},{"location":"services/pihole/pihole/#router-configuration","title":"Router Configuration","text":"<p>Configure your router's DHCP settings to use the Pi-hole's IP address as the Primary DNS Server. This ensures all connected devices are automatically protected.</p>"},{"location":"services/pihole/pihole/#host-configuration-the-server-itself","title":"Host Configuration (The Server itself)","text":"<p>The host machine running the Pi-hole container does not automatically use it.</p> <p>Method 1: DHCPCD (Standard Linux) Add to <code>/etc/dhcpcd.conf</code>:</p> <pre><code>static domain_name_servers=127.0.0.1\n</code></pre> <p>Warning</p> <p>If your Pi-hole host uses itself as upstream DNS and Pi-hole fails, the host loses DNS. This can prevent repair attempts (e.g., <code>pihole -r</code>) because the internet connection will appear \"down.\"</p> <p>Method 2: Permissions Pi-hole v6 uses a new API. To allow your user to run CLI commands without a password:</p> <pre><code># Debian/Ubuntu/RasPi OS\nsudo usermod -aG pihole $USER\n\n# Alpine\nsudo addgroup pihole $USER\n</code></pre>"},{"location":"services/pihole/pihole/#advanced-local-dns-wildcards","title":"Advanced: Local DNS &amp; Wildcards","text":"<p>To make your local services reachable via a custom domain (e.g., <code>service.vanth.me</code>) while on your LAN, we use <code>dnsmasq</code> to intercept requests and point them to your Reverse Proxy (Nginx Proxy Manager).</p> <ol> <li>Create Config: Create <code>/etc/dnsmasq.d/99-wildcard.conf</code> inside the container (or mapped volume): <pre><code>address=/.vanth.me/192.168.0.116\n</code></pre></li> </ol> <p>(Replace <code>192.168.0.116</code> with your Nginx Proxy Manager Host IP) 2. Activate: In Pi-hole Settings &gt; Misc &gt; Enable misc.etc_dnsmasq_d. 3. Flush: Run <code>pihole restartdns</code> in the container or <code>ipconfig /flushdns</code> on your client machine.</p>"},{"location":"services/pihole/pihole/#tailscale-subnet-router-integration","title":"Tailscale Subnet Router Integration","text":"<p>This setup allows you to access your home network (and use your Pi-hole for DNS) from anywhere in the world, without opening ports on your firewall.</p>"},{"location":"services/pihole/pihole/#1-enable-ip-forwarding","title":"1. Enable IP Forwarding","text":"<p>You must allow the host to route traffic.</p> <pre><code># Enable IPv4 forwarding immediately\nsudo sysctl -w net.ipv4.ip_forward=1\n\n# Make the change permanent across reboots\necho 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf\n</code></pre>"},{"location":"services/pihole/pihole/#2-advertise-subnet-routes","title":"2. Advertise Subnet Routes","text":"<p>Tell Tailscale that this machine can route traffic to your LAN (192.168.0.x).</p> <pre><code># Advertise the 192.168.0.0/24 subnet\nsudo tailscale up --advertise-routes=192.168.0.0/24\n</code></pre>"},{"location":"services/pihole/pihole/#3-approve-route","title":"3. Approve Route","text":"<ol> <li>Go to the Tailscale Admin Console.</li> <li>Find your router node (the machine you just configured).</li> <li>Click the ... menu -&gt; Edit route settings.</li> <li>Approve the <code>192.168.0.0/24</code> route.</li> </ol>"},{"location":"services/pihole/pihole/#4-how-it-works","title":"4. How it works","text":"<ol> <li>You set Pi-hole as your Nameserver in Tailscale DNS settings.</li> <li>Your remote phone queries <code>service.vanth.me</code>.</li> <li>Pi-hole resolves this to the LAN IP (<code>192.168.0.116</code>).</li> <li>Because the Subnet Route is active, your phone sends the traffic through the Tailscale tunnel to your home server.</li> </ol> <p>```</p>"},{"location":"services/servarr/gluetun/","title":"Gluetun - ProtonVPN + Qbittorrent","text":"<p>Gluetun  </p> <p>Gluetun + ProtonVPN  </p> <p>Gluetun is a VPN client in a thin Docker container for multiple VPN providers, written in Go, and using OpenVPN or Wireguard, DNS over TLS, with a few proxy servers built-in. </p>"},{"location":"services/servarr/gluetun/#gluetun-configuration-for-protonvpn-qbittorrent","title":"Gluetun Configuration for ProtonVPN + QBittorrent","text":"<p>Reddit Guide  </p> <p>For this I'm using a Wireguard setup instead of OpenVPN.     * It is lighter, faster, and easier to configure. </p> <p>In ProtonVPN webpage:      * go into the Downloads section and create a new WireGuard configuration. Select Router, no filtering, and \"NAT-PMP (Port Forwarding)\". Deselect VPN accelerator. When you click Create, a popup of the config will display. Copy the PrivateKey.</p> <p>Use this <code>compose.yaml</code> to setup the gluetun container for ProtonVPN and bind it to QB. The <code>.env</code> is below it. No need to configure the <code>compose.yaml</code>. It can all be done via the <code>.env</code>.</p> <p><pre><code>services:\n  gluetun:\n    image: qmcgaw/gluetun:v3\n    container_name: gluetun\n    cap_add:\n      - NET_ADMIN\n    devices:\n      - /dev/net/tun:/dev/net/tun\n    ports:\n      - 8080:8080/tcp # qbittorrent\n    environment:\n      - TZ=${TZ}\n      - UPDATER_PERIOD=24h\n      - VPN_SERVICE_PROVIDER=protonvpn\n      - VPN_TYPE=${VPN_TYPE}\n      - BLOCK_MALICIOUS=off\n      - OPENVPN_USER=${OPENVPN_USER}\n      - OPENVPN_PASSWORD=${OPENVPN_PASSWORD}\n      - OPENVPN_CIPHERS=AES-256-GCM\n      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}\n      - PORT_FORWARD_ONLY=on\n      - VPN_PORT_FORWARDING=on\n      - VPN_PORT_FORWARDING_UP_COMMAND=/bin/sh -c 'wget -O- --retry-connrefused --post-data \"json={\\\"listen_port\\\":{{PORTS}}}\" http://127.0.0.1:8080/api/v2/app/setPreferences 2&gt;&amp;1'\n      - SERVER_COUNTRIES=${SERVER_COUNTRIES}\n    volumes:\n      - ${MEDIA_DIR}/gluetun/config:/gluetun\n    restart: unless-stopped\n\n  qbittorrent:\n    image: lscr.io/linuxserver/qbittorrent:latest\n    container_name: qbittorrent\n    depends_on:\n      gluetun:\n        condition: service_healthy\n    environment:\n      - PUID=1000\n      - PGID=1000\n      - TZ=${TZ}\n      - WEBUI_PORT=8080\n    volumes:\n      - ${MEDIA_DIR}/qbittorrent/config:/config\n      - ${MEDIA_DIR}/qbittorrent/downloads:/downloads\n    restart: unless-stopped\n    network_mode: \"service:gluetun\"\n</code></pre> .env</p> <pre><code># Fill in either the OpenVPN or Wireguard sections. The choice of vpn is made with VPN_TYPE. Choose 'wireguard' or 'openvpn'. The settings for the other vpn type will be ignored. \n# Alter the TZ, MEDIA_DIR, and SERVER_COUNTRIES to your preference. Run 'docker run --rm -v eraseme:/gluetun qmcgaw/gluetun format-servers -protonvpn' to get a list of server countries\n\n# Base config\nTZ=Australia/Brisbane\nMEDIA_DIR=/media\n\n# Gluetun config\nVPN_TYPE=wireguard #openvpn\nSERVER_COUNTRIES=Albania,Algeria,Angola,Argentina,Australia,Austria,Azerbaijan\n\n# OpenVPN config\nOPENVPN_USER=username+pmp\nOPENVPN_PASSWORD=password\n\n# Wireguard config (example key)\nWIREGUARD_PRIVATE_KEY=wOEI9rqqbDwnN8/Bpp22sVz48T71vJ4fYmFWujulwUU=\n</code></pre> <p>Run <code>docker compose up -d</code> to start it. </p> <p>Note</p> <p>This WILL fail to set the port on first run. Fix below.</p> <p>Login to the Qbittorrent WebUI.      * Settings &gt; WebUI tab     * Set user/pass     * Check 'Bypass authentication for clients on localhost'     * Save. </p> <p>Restart the stack, check the container logs, and it should work. </p> <p>You can test a download from here https://webtorrent.io/free-torrents.</p>"},{"location":"services/servarr/qbittorrent/","title":"qBittorrent","text":"<p>TRaSH's Guide   - this is copy from here</p> <p>Note</p> <p>Settings that aren't covered means you can change them to your own liking or just leave them on default.</p>"},{"location":"services/servarr/qbittorrent/#downloads","title":"Downloads","text":"<p><code>Tools</code> =&gt; <code>Options</code> =&gt; <code>Downloads</code> (Or click on the cogwheel to access the options)</p>"},{"location":"services/servarr/qbittorrent/#when-adding-a-torrent","title":"When adding a torrent","text":"<ol> <li> <p>For consistency with other torrents, we recommend leaving this on <code>Original</code>.</p> <p>Suggested: <code>Original</code></p> </li> <li> <p>Delete the .torrent file after it has been added to qBittorrent.</p> <p>Suggested: <code>Personal preference</code></p> </li> <li> <p>Pre-allocated disk space for the added torrents limits fragmentation and also makes sure if you use a cache drive or a feeder disk that the space is available.</p> <p>Suggested: <code>Personal Preferences</code></p> <p>Important: Disable Pre-allocation in qBittorrent if you're using unRaid with a cache drive</p> <p>Go to qBittorrent \u2192 Options \u2192 Downloads and disable this option:</p> <p><code>Pre-allocate disk space for all files</code></p> <p>When this option is enabled, it keeps the reserved space locked (in use) until you quit qBittorrent.</p> </li> </ol>"},{"location":"services/servarr/qbittorrent/#saving-management","title":"Saving Management","text":"<ol> <li> <p>Make sure this is set to <code>Automatic</code>. Your downloads will not go into the category folder otherwise.</p> <p>Suggested: <code>Automatic</code></p> </li> <li> <p>This helps you to manage your file location based on categories.</p> <p>Suggested: <code>Enabled</code></p> </li> <li> <p>Same as <code>Step 2</code></p> <p>Suggested: <code>Enabled</code></p> </li> <li> <p>Your download root path (Download folder/location).</p> <p>Read the <code>ATTENTION</code> block below</p> </li> <li> <p>If you enable this, your incomplete downloads will be placed in this directory until completed. This could be useful if you want your downloads to use a separate SSD/Feeder disk[^1], but this also results in extra unnecessary moves or in worse cases a slower and more I/O intensive copy + delete.</p> <p>Suggested: <code>Personal preference</code></p> </li> </ol>"},{"location":"services/servarr/qbittorrent/#attention","title":"ATTENTION","text":"<p>{! include-markdown \"../../../includes/downloaders/warning-path-location.md\" !}</p>"},{"location":"services/servarr/qbittorrent/#connection","title":"Connection","text":""},{"location":"services/servarr/qbittorrent/#listening-port","title":"Listening Port","text":"<ol> <li> <p>Set this to TCP for the best performance</p> <p>Suggested: <code>TCP</code></p> </li> <li> <p>Your port used for incoming connections, this is the port you opened in your router/firewall or port forwarded at your VPN provider to make sure you're connectable.</p> <p>Suggested: <code>The port you opened in your router/firewall or port forwarded at your VPN provider</code></p> </li> <li> <p>This should be disabled in your router for several security reasons.</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>Make sure this is disabled so you don't mess up the forwarded port.</p> <p>Suggested: <code>Disabled</code></p> </li> </ol>"},{"location":"services/servarr/qbittorrent/#connections-limits","title":"Connections Limits","text":"<p>The best settings for this depend on many factors so we won't be covering this.</p> <p>Suggested: <code>personal preference based on your setup and connection.</code></p>"},{"location":"services/servarr/qbittorrent/#proxy-server","title":"Proxy Server","text":"<p>This is where you would add for example your SOCKS5 settings from your VPN provider.</p> <p>Suggested: <code>I personally don't recommend this insecure option being it's unencrypted and only spoofs your IP.</code></p>"},{"location":"services/servarr/qbittorrent/#speed","title":"Speed","text":""},{"location":"services/servarr/qbittorrent/#global-rate-limits","title":"Global Rate Limits","text":"<p>Here you can set your global rate limits, meaning your maximum download/upload speed used by qBittorrent. (For all torrents)</p> <p>The best settings depend on many factors.</p> <ul> <li>Your ISP speed.</li> <li>Your hardware used.</li> <li> <p>Bandwidth needed by other services in your home network.</p> <p>Suggested: <code>For a home connection that you use with others it's best practice to set the upload/download rate to about 70-80% of your maximum upload/download speed.</code></p> </li> </ul>"},{"location":"services/servarr/qbittorrent/#alternative-rate-limits","title":"Alternative Rate Limits","text":"<p>When enabled, it basically does the same as above, but with the option to set up a schedule.</p> <p>Examples:</p> <ul> <li>Limit your upload/download rate during the daytime when you make the most use of it, and unlimited it during nighttime when no one is using the connection.</li> <li> <p>If you have an internet connection that's limited during specific hours (unlimited bandwidth during the night, but limited during the day)</p> <p>Suggested: <code>Personal preference</code></p> </li> </ul>"},{"location":"services/servarr/qbittorrent/#rate-limits-settings","title":"Rate Limits Settings","text":"<p>Not going to cover the technical part of what it does, but the following settings are recommended for best speeds (in most cases).</p> <ol> <li> <p>Prevents you from being flooded if the uTP protocol is used for any reason.</p> <p>Suggested: <code>Enabled</code></p> </li> <li> <p>Apply rate limit to transport overhead</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>Apply rate limit to peers on LAN</p> <p>Suggested: <code>Enabled</code></p> </li> </ol>"},{"location":"services/servarr/qbittorrent/#bittorrent","title":"Bittorrent","text":""},{"location":"services/servarr/qbittorrent/#privacy","title":"Privacy","text":"<ol> <li> <p>These settings are mainly used for public trackers (and should be enabled for them) and not for private trackers, decent private trackers use a private flag where they ignore these settings.</p> <p>Suggested: <code>Personal preference</code></p> </li> <li> <p>Recommended setting <code>Allow encryption</code> rather than enforcing it allows more peers to connect and is recommended on underpowered systems as it will allow for lower overhead.</p> <p>Suggested: <code>Allow encryption</code></p> </li> <li> <p>Anonymous mode hides the client's (qBittorrent) fingerprint from the peer-ID, sets the \u2018User-Agent\u2019 to Null and doesn\u2019t share your IP address directly with trackers (though peers will still see your IP address). If using private trackers, it's recommended to <code>disable</code> this. We also got reports from people who are using this that they had worse speeds.</p> <p>Suggested: <code>Disabled</code></p> </li> </ol>"},{"location":"services/servarr/qbittorrent/#torrent-queueing","title":"Torrent Queueing","text":"<p>These options allow you to control the number of active torrents being downloaded and uploaded.</p> <p>Suggested: <code>personal preference based on your setup and connection.</code></p>"},{"location":"services/servarr/qbittorrent/#seeding-limits","title":"Seeding Limits","text":"<ol> <li> <p>Your maximum seeding ratio preference. (When both ratio and seeding time are enabled it will trigger the action on whatever happens first.)</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>Your maximum seeding time preference (When both ratio and seeding time are enabled it will trigger the action on whatever happens first.)</p> <p>Suggested: <code>Disabled</code></p> </li> <li> <p>What to do when ratio or seeding time is reached.</p> <p>Suggested: <code>Paused and Disabled</code></p> </li> </ol> <p>Tip</p> <p>We recommend using the seeding goals in your Starr Apps indexer settings (enable advanced), or use qBit Manage</p>"},{"location":"services/servarr/qbittorrent/#automatically-add-these-trackers-to-new-downloads","title":"Automatically add these trackers to new downloads","text":"<p>Recommendation: <code>Disabled</code></p> <p>Warning</p> <p> NEVER USE THIS OPTION ON (Semi-)PRIVATE TRACKERS </p>"},{"location":"services/servarr/qbittorrent/#web-ui","title":"Web UI","text":""},{"location":"services/servarr/qbittorrent/#authentication","title":"Authentication","text":"<ol> <li>When enabled there will be no authentication required for clients on localhost.</li> <li>When enabled there will be no authentication required for clients in the <code>step.3</code> whitelist.</li> <li>Add all IP subnets that you want to bypass authentication.</li> </ol>"},{"location":"services/servarr/qbittorrent/#security","title":"Security","text":"<ol> <li> <p>In some cases when this is enabled it could result in issues.</p> <p>Suggested: <code>Disabled</code></p> </li> </ol>"},{"location":"services/servarr/radarr/","title":"Radarr","text":"<p>Servarr Wiki   - This is just a copy from here</p> <p>This page is still in progress and not complete. Contributions are welcome</p> <p>For a more detailed breakdown of all the settings, check Radarr =&gt;Settings</p> <p>In this guide we will try to explain the basic setup you need to do to get started with Radarr. We're going to skip some options that you may see on the screen. If you want to dive deeper into those, please see the appropriate page in the FAQ and docs for a full explanation.</p> <p>Important</p> <p>Please note that within the screenshots and GUI settings in <code>orange</code> are advanced options, so you will need to click <code>Show Advanced</code> at the top of the page to make them visible.</p>"},{"location":"services/servarr/radarr/#startup","title":"Startup","text":"<p>After installation and starting up, you open a browser and go to <code>http://{your_ip_here}:7878</code></p> <p></p>"},{"location":"services/servarr/radarr/#media-management","title":"Media Management","text":"<p>First we\u2019re going to take a look at the <code>Media Management</code> settings where we can setup our preferred naming and file management settings.</p> <p><code>Settings</code> =&gt; <code>Media Management</code></p> <p></p>"},{"location":"services/servarr/radarr/#movie-naming","title":"Movie Naming","text":"<ol> <li>Enable/Disable Renaming of your movies (as opposed to leaving the names that are currently there or as they were when you downloaded them).</li> <li>If you want illegal characters replaced or removed (<code>\\ / : * ? \" &lt; &gt; | ~ # % &amp; + { }</code>).</li> <li>This setting will dictate how Radarr handles colons within the movie file.</li> <li>Here you will select the naming convention for the actual movie files.</li> <li>(Advanced Option) This is where you will set the naming convention for the folder that contains the video file.</li> </ol> <p>If you want a recommended naming scheme and examples take a look TRaSH's Recommended Naming Schemes.</p>"},{"location":"services/servarr/radarr/#importing","title":"Importing","text":"<ol> <li>(Advanced Option) Enable <code>Use Hard links instead of Copy</code> more info how and why with examples TRaSH's Hard links Guide.</li> <li>(Advanced Option) Import matching extra files (subtitles, nfo, etc) after importing a file.</li> </ol>"},{"location":"services/servarr/radarr/#file-management","title":"File Management","text":"<ol> <li>Movies deleted from disk are automatically unmonitored in Radarr.<ul> <li>You may want to delete a movie but do not want Radarr to re-download the movie. You would use this option.</li> </ul> </li> <li>(Advanced Option) Designate a location for deleted files to go to (just in case you want to retrieve them before the bin is taken out).</li> <li>(Advanced Option) This is how old a given file can be before it is deleted permanently.</li> </ol>"},{"location":"services/servarr/radarr/#root-folders","title":"Root Folders","text":"<p>Here we will add the root folder that Radarr will be using to import your existing organized media library and where Radarr will be importing (copy/hardlink/move) your media after your download client has downloaded it.</p> <ul> <li>Non-Windows: If you're using an NFS mount ensure <code>nolock</code> is enabled.  </li> <li>If you're using an SMB mount ensure <code>nobrl</code> is enabled.</li> </ul> <p>The user and group you configured Radarr to run as must have read &amp; write access to this location.</p> <p>Your download client downloads to a download folder and Radarr imports it to your media folder (final destination) that your media server uses.</p> <p>Your download folder and media (library / root) folder can\u2019t be the same location</p> <p>Don\u2019t forget to save your changes</p>"},{"location":"services/servarr/radarr/#profiles","title":"Profiles","text":"<p><code>Settings</code> =&gt; <code>Profiles</code></p> <p></p> <p>Here you\u2019ll be allowed to configure profiles for which you can have for the quality, preferred language, and custom format scoring of a movie you\u2019re looking to download.</p> <p>We recommend you to create your own profiles and only select the Quality Sources and Languages you actually want.</p> <p>For more information on foreign titles and languages see this FAQ entry</p> <p>Many users find TRaSH's Custom Format Language Guide helpful to specify the languages of movies they want.</p> <p>Profiles is also where Custom Format Scores are configured. It's strongly recommended to add the below Custom Formats from TRaSH's Guides to avoid unwanted downloads. Refer to the linked TRaSH Guide Custom Format article and additional referenced 3 TRaSH Custom Format Guides on the top of the Collection of Custom Formats page for more information.</p> <ul> <li>DV (WEB-DL) will avoid grabbing releases with Dolby Vision (DV) that have a green hue if DV is not supported.</li> <li>BR-DISK to avoid grabbing poorly named BR-DISKs that do not match the BR-DISK quality parsing.</li> </ul> <p>More info at Settings =&gt; Profiles. To see what the difference is between the Quality Sources look at our Quality Definitions.</p>"},{"location":"services/servarr/radarr/#quality","title":"Quality","text":"<p><code>Settings</code> =&gt; <code>Quality</code></p> <p></p> <p>Here you\u2019re able to change/fine tune the min and max size of your wanted media files (when using Usenet keep in mind the RAR/PAR2 files)</p> <p>If you need some help with what to use for a Quality Settings check TRaSH's size recommendations for a tested example.</p>"},{"location":"services/servarr/radarr/#indexers","title":"Indexers","text":"<p><code>Settings</code> =&gt; <code>Indexers</code></p> <p></p> <p>Here you\u2019ll be adding the indexer/tracker that you\u2019ll be using to actually download any of your files.</p> <p>Once you\u2019ve clicked the + button to add a new indexer you\u2019ll be presented with a new window with many different options. For the purposes of this wiki Radarr considers both Usenet Indexers and Torrent Trackers as \u201cIndexers\u201d.</p> <p>There are two sections here: Usenet and Torrents. Based upon what download client you\u2019ll be using you\u2019ll want to select the type of indexer you\u2019ll be going with.</p> <p>For torrent trackers - almost all require the use of Prowlarr or Jackett.</p>"},{"location":"services/servarr/radarr/#download-clients","title":"Download Clients","text":"<p><code>Settings</code> =&gt; <code>Download Clients</code></p> <p></p> <p>Downloading and importing is where most people experience issues. From a high level perspective, the software needs to be able to communicate with your download client and to have read &amp; write access to the location the download client reports files the client downloads. There is a large variety of supported download clients and an even bigger variety of setups. This means that while there are some common setups there isn\u2019t one right setup and everyone\u2019s setup can be a little different. But there are many wrong setups.</p> <p>See the settings page, at the More Info (Supported) page for this section, and TRaSH's Download Client Guides for more information.</p> <ul> <li>Radarr will send a download request to your client, and associate it with a label or category name that you have configured in the download client settings.<ul> <li>Examples: movies, tv, series, music, etc.</li> </ul> </li> <li>Radarr will monitor your download clients active downloads that use that category name. It monitors this via your download client's API.</li> <li>When the download is completed, Radarr will know the final file location as reported by your download client. This file location can be almost anywhere, as long as it is somewhere separate from your media folder and accessible by Radarr</li> <li>Radarr will scan that completed file location for files that Radarr can use. It will parse the file name to match it against the requested media. If it can do that, it will rename the file according to your specifications, and move it to the specified media location.</li> <li>Atomic Moves (instant moves) are enabled by default. The file system and mounts must be the same for your completed download directory and your media library. If the the atomic move fails or your setup does not support hard links and atomic moves then Radarr will fall back and copy the file then delete from the source which is IO intensive.</li> <li>If the \"Completed Download Handling - Remove\" option is enabled in Radarr's settings leftover files from the download will be sent to your trash or recycling via a request to your client to delete/remove the release.</li> </ul>"},{"location":"services/servarr/radarr/#how-to-import-your-existing-organized-media-library","title":"How to import your existing organized media library","text":"<p>Note that Radarr does not regularly search for Movies. See the How does Radarr work? FAQ Entry for details to understand how Radarr works.</p> <p>After setting up your profiles/quality sizes and added your indexers and download client(s) it\u2019s time to import your existing organized media library.</p> <p><code>Movies</code></p> <p></p> <p>Select <code>Import Existing Movies</code> or select <code>Import</code> from the sidebar.</p>"},{"location":"services/servarr/radarr/#import-movies","title":"Import movies","text":"<p>Select the root path you added earlier in the root folders section.</p>"},{"location":"services/servarr/radarr/#importing-existing-media","title":"Importing Existing Media","text":"<p>Depending how well you got your existing movie folders named Radarr will try to match it with the correct movie as seen at Nr.5 If all your movies are in a single directory follow this guide</p> <ol> <li> <p>Your movie folder name.</p> </li> <li> <p>Monitor - How you want the movie to be added to Radarr.</p> <ul> <li>None - Do not monitor the movie nor collection for new releases</li> <li>Movie Only - Only Monitor the movie for new releases</li> <li>Movie &amp; Collection - Monitor both the movie for new releases &amp; add and monitor any movies in the movie's collection (if exists)</li> <li> <p>Availability - When will Radarr consider a movie is available.</p> </li> <li> <p>Announced: Radarr shall consider movies available as soon as they are added to Radarr. This setting is recommended if you have good private trackers that do not have fakes.</p> </li> <li>In Cinemas: Radarr shall consider movies available as soon as movies they hit cinemas. This option is not recommended.</li> <li>Released: Radarr shall consider movies available as soon as the Blu-ray is released. This option is recommended if your indexers contain fakes often.</li> <li>Quality Profile - Select your preferred profile to use.</li> </ul> </li> <li> <p>Movie - What Radarr thinks the movie matched for. It is imperative that you review this and edit/search if the match is not correct. Mismatches are often caused by poorly named folders.</p> </li> <li> <p>Mass select Monitor status.</p> </li> <li> <p>Mass select Minimum Availability.</p> </li> <li> <p>Mass select Quality Profile.</p> </li> <li> <p>Start Importing your existing media library.</p> </li> </ol> <p>Once a movie is added to Radarr, Radarr will scan the movie's folder and attempt to match a video file in the folder to the movie. The most common cause for Radarr not matching the file and the movie thus having a Radarr Status of Missing is the filename does not have the year in it. Radarr requires the year in the filename for it to be parsable.</p>"},{"location":"services/servarr/radarr/#no-match-found","title":"No match found","text":"<p>If you\u2019re getting a error like this</p> <p></p> <p>Then you probably made a mistake with your movie folder naming.</p> <p>To fix this issue you can try the following</p> <p>Expand the error message</p> <p></p> <p>and check on the themoviedb if the year or title matches. in this example you will notice that the year is wrong and you can fix it by changing the year and click on the refresh icon.</p> <p></p> <p>Or you can just use the tmdb:id or imdb:id (if tmdb is linked to imdb) and then select the found movie if matched.</p> <p></p> <p></p>"},{"location":"services/servarr/radarr/#fix-faulty-folder-name-after-import","title":"Fix faulty folder name after import","text":"<p>You will notice after the fix we did during the import that the folder name still has the wrong year in it. To fix this we\u2019re going to do a little magic trick.</p> <p>Go to you movie overview</p> <p><code>Movies</code></p> <p>On the top click on <code>Movie Editor</code></p> <p></p> <p>After activating it you select the movie(s) from where you want to have the folder(s) to be renamed.</p> <p></p> <ol> <li>If you want all your movie folders renamed to your folder naming scheme you set earlier movie naming section.</li> <li>Select the movie(s) from where you want to have the folder(s) to be renamed.</li> <li>Choose the same <code>Root Folder</code></li> </ol> <p>A new popup will be shown</p> <p></p> <p>Select <code>Yes, Move the files</code></p> <p>Then Magic</p> <p></p> <p>As you can see the folder has been renamed to the correct year following your naming scheme.</p>"},{"location":"services/servarr/radarr/#how-to-add-a-movie","title":"How to add a movie","text":"<p>After you imported your existing well organized media library it\u2019s time to add the movies you want.</p> <p><code>Movies</code> =&gt; <code>Add New</code></p> <p></p> <p>Type in the box the movie you want or use the tmdb:id or imdb:id.</p> <p>When typing out the movie name you will see it will start showing you results.</p> <p></p> <p>When you see the movie you want click on it.</p> <p></p> <ol> <li> <p>Root Folder - Radarr will add the movie to the Root Folder you\u2019ve setup in the root folders section</p> </li> <li> <p>Monitor - How you want the movie to be added to Radarr.</p> <ul> <li>Movie Only = Radarr will monitor the RSS feed for the movie in your library that you do not have (yet) or upgrade the existing movie to a better quality.</li> <li>Movie &amp; Collection = Radarr will monitor the RSS feed for the movie in your library that you do not have (yet) or upgrade the existing movie to a better quality. It will also add all movies in this movie's collection (if any) with your selected settings.</li> <li>None = Radarr will not monitor the RSS feed, any upgrades or new movies will be ignored and have to be manually done. All searches for unmonitored movies must be manually triggered searches or interactive searches.</li> <li>Availability - When Radarr shall consider a movie is available.</li> </ul> <p>For More Information on TMDB's Dates that impact the below Availabilities See How Does Radarr Determine the Year of the Movie</p> <ul> <li>Announced: Radarr shall consider movies available as soon as they are added to Radarr. This setting is recommended if you have good private trackers (or really good public ones, e.g. rarbg.to) that do not have fakes.</li> <li>In Cinemas: Radarr shall consider movies available as soon as movies hit cinemas (Theatrical Date on TMDb) This option is not recommended.</li> <li>Released: Radarr shall consider movies available as soon as the Blu-Ray or streaming version is released (Digital and Physical dates on TMDb) This option is recommended and likely should be combined with an Availability Delay of <code>-14</code> or <code>-21</code> days.<ul> <li>If TMDb is not populated with a date, it is assumed 90 days after <code>Theatrical Date</code> (Oldest in theater's date) the movie is available in web or physical services.</li> </ul> </li> <li>Quality Profile - Select your profile to use for this movie</li> </ul> </li> <li> <p>Tags - Here you can add certain tags for advanced usage.</p> </li> <li> <p>Search on Add - Make sure you enable this if you want Radarr search for the missing movie when added to Radarr more info</p> </li> <li> <p>Click on <code>Add Movie</code> to add the movie to Radarr.</p> <ul> <li>If you get an error of \"path is already configured\" see this FAQ entry</li> </ul> </li> </ol>"},{"location":"services/servarr/sonarr/","title":"Sonarr","text":"<p>Servarr Wiki   - this is just a copy from here</p> <p>This page is still in progress and not complete. Contributions are welcome</p> <p>For a more detailed breakdown of all the settings, check\u00a0Sonarr =&gt;Settings</p> <p>In this guide we will try to explain the basic setup you need to do to get started with Sonarr. We're going to skip some options that you may see on the screen. If you want to dive deeper into those, please see the appropriate page in the FAQ and docs for a full explanation.</p> <p>Important</p> <p>Please note that within the screenshots and GUI settings in\u00a0<code>orange</code>\u00a0are advanced options, so you will need to click\u00a0<code>Show Advanced</code>\u00a0at the top of the page to make them visible.</p>"},{"location":"services/servarr/sonarr/#startup","title":"Startup","text":"<p>After installation and starting up, you open a browser and go to\u00a0<code>http://{your_ip_here}:8989</code></p> <p></p>"},{"location":"services/servarr/sonarr/#media-management","title":"Media Management","text":"<p>First we\u2019re going to take a look at the\u00a0<code>Media Management</code>\u00a0settings where we can setup our preferred naming and file management settings.</p> <p>Click on\u00a0<code>Settings</code>\u00a0=&gt;\u00a0<code>Media Management</code>\u00a0on the left menu.</p>"},{"location":"services/servarr/sonarr/#episode-naming","title":"Episode Naming","text":"<ul> <li>Check the box to enable Rename Episodes.</li> <li>Decide on your Standard, Daily, and Anime episode naming conventions. You should review the recommended naming conventions\u00a0in the TRaSH Guides documentation.</li> </ul> <p>If you choose not to include quality/resolution or release group, this is information you cannot regain later. It is highly recommended that you include those in your naming scheme.</p>"},{"location":"services/servarr/sonarr/#importing","title":"Importing","text":"<ul> <li>(Advanced Option) If you want TBA episodes to be imported immediately, change Episode Title Required to \"Never\".</li> <li>(Advanced Option) Enable\u00a0<code>Use Hard links instead of Copy</code>\u00a0more info how and why with examples\u00a0TRaSH's Hard links Guide.</li> <li>Check the box to import extra files, and add at least\u00a0<code>.srt</code>\u00a0to the list.</li> </ul>"},{"location":"services/servarr/sonarr/#root-folders","title":"Root Folders","text":"<p>Here we will add the root folder that Sonarr will be using to import your existing organized media library and where Sonarr will be importing (copy/hardlink/move) your media after your download client has downloaded it. This is the folder where your series and episodes are stored for your media player to play them. It is NOT where you download files to!</p> <ul> <li>Non-Windows Users: If you're using an NFS mount ensure\u00a0<code>nolock</code>\u00a0is enabled.  </li> <li>If you're using an SMB mount ensure\u00a0<code>nobrl</code>\u00a0is enabled.</li> </ul> <p>The user and group you configured Sonarr to run as must have read &amp; write access to this location.</p> <p>Your download folder and media folder can\u2019t be the same location</p> <p>Don\u2019t forget to save your changes!</p>"},{"location":"services/servarr/sonarr/#profiles","title":"Profiles","text":"<p><code>Settings</code>\u00a0=&gt;\u00a0<code>Profiles</code></p> <p>We recommend you to create your own profiles and only select the Quality Sources you actually want. However, there are several prefilled quality profiles available to choose from as well, if one of those fits. If you need more information about Profiles, please see the\u00a0appropriate wiki page\u00a0for that section.</p>"},{"location":"services/servarr/sonarr/#indexers","title":"Indexers","text":"<p><code>Settings</code>\u00a0=&gt;\u00a0<code>Indexers</code></p> <p>Here you\u2019ll be adding the indexer/trackers that you\u2019ll be using to actually download any of your files.</p> <p>Once you\u2019ve clicked the + button to add a new indexer, you\u2019ll be presented with a new window with many different options. For the purposes of this wiki Sonarr considers both Usenet Indexers and Torrent Trackers as \u201cIndexers\u201d.</p> <p>There are two sections here: Usenet and Torrents. Based upon what download client you\u2019ll be using you\u2019ll want to select the type of indexer you\u2019ll be going with.</p> <p>Most usenet indexers require an API key, which can be found in your Profile page on the indexer's website.</p> <p>Most torrent trackers require\u00a0Prowlarr\u00a0or Jackett to be used in Sonarr</p> <p>Add at least one indexer in order for Sonarr to work properly.</p> <p>See the\u00a0settings page\u00a0and at the\u00a0More Info (Supported)\u00a0page for this section for more information.</p>"},{"location":"services/servarr/sonarr/#download-clients","title":"Download Clients","text":"<p><code>Settings</code>\u00a0=&gt;\u00a0<code>Download Clients</code></p> <p>Downloading and importing is where most people experience issues. From a high level perspective, the software needs to be able to communicate with your download client and have access to the files it downloads. There is a large variety of supported download clients and an even bigger variety of setups. This means that while there are some common setups there isn\u2019t one right setup and everyone\u2019s setup can be a little different. But there are many wrong setups.</p> <p>See the\u00a0settings page, at the\u00a0More Info (Supported)\u00a0page for this section, and\u00a0TRaSH's Download Client Guides\u00a0for more information.</p> <ul> <li>Sonarr will send a download request to your client, and associate it with a label or category name that you have configured in the download client settings.<ul> <li>Examples: movies, tv, series, music, etc.</li> </ul> </li> <li>Sonarr will monitor your download clients active downloads that use that category name. It monitors this via your download client's API.</li> <li>When the download is completed, Sonarr will know the final file location as reported by your download client. This file location can be almost anywhere, as long as it is somewhere separate from your media folder and accessible by Sonarr</li> <li>Sonarr will scan that completed file location for files that Sonarr can use. It will parse the file name to match it against the requested media. If it can do that, it will rename the file according to your specifications, and move it to the specified media location.</li> <li>Atomic Moves (instant moves) are enabled by default. The file system and mounts must be the same for your completed download directory and your media library. If the the atomic move fails or your setup does not support hard links and atomic moves then Sonarr will fall back and copy the file then delete from the source which is IO intensive.</li> <li>If the \"Completed Download Handling - Remove\" option is enabled in Sonarr's settings leftover files from the download will be sent to your trash or recycling via a request to your client to delete/remove the release.</li> </ul>"},{"location":"services/servarr/sonarr/#how-to-import-your-existing-organized-media-library","title":"How to import your existing organized media library","text":"<p>Note that Sonarr does not regularly search for Episodes. See the FAQ Entry for details to understand how Sonarr works. How does Sonarr find episodes?</p> <p>After setting up your profiles/quality sizes and added your indexers and download client(s) it\u2019s time to import your existing organized media library.</p> <p>Coming soon - Contributions Welcome</p>"},{"location":"services/servarr/sonarr/#importing-existing-media","title":"Importing Existing Media","text":"<p>Depending how well your existing series folders are named, Sonarr will try to match it with the correct series. You should review this list carefully before importing.</p> <p>Library Import is only to be used on an existing organized library and shall not be used on a download folder or to ad-hoc import media.</p> <ol> <li>Navigate to Library Import</li> <li>Read and understand the Library Import Help Text</li> <li>Select or add the root (library) folder to import series from</li> <li>Review Sonarr's mapping/matching of Series Folders to TVDb series</li> <li>Set your monitoring settings and quality profile as appropriate</li> <li>Click Start Import</li> </ol>"},{"location":"services/servarr/sonarr/#no-match-found","title":"No match found","text":"<ol> <li>Search the series name or TVDbId in the series selection box</li> <li>See\u00a0this FAQ entry\u00a0if the series cannot be found</li> </ol>"},{"location":"services/servarr/sonarr/#fix-faulty-folder-name-after-import","title":"Fix faulty folder name after import","text":"<ol> <li>Remove the Series from Sonarr</li> <li>Library Import</li> <li>Ensure the series is mapped correctly</li> </ol>"},{"location":"services/servarr/sonarr/#add-new-series","title":"Add New Series","text":"<p>Refer to the Library Page for additional information</p>"},{"location":"services/servarr/sonarr/#import-episodes","title":"Import Episodes","text":"<ul> <li>Use Wanted =&gt; Manual Import to import episode files to their series folders on an ad-hoc basis</li> <li>Use Manage Episodes on a series' page to remap or map existing episode files in a series folder</li> </ul>"},{"location":"services/tailscale/tailscale/","title":"Tailscale Quick Guide","text":""},{"location":"services/tailscale/tailscale/#overview","title":"Overview","text":"<p>Tailscale is a zero-configuration VPN that creates a secure private network (a \"Tailnet\") between devices. Unlike traditional VPNs that route traffic through a central gateway, Tailscale establishes a mesh network where devices connect directly to each other (peer-to-peer) using the WireGuard protocol.</p>"},{"location":"services/tailscale/tailscale/#key-benefits","title":"Key Benefits","text":"<ul> <li>No Port Forwarding: It traverses firewalls and NATs automatically. No router ports need to be opened.</li> <li>WireGuard Based: utilizes modern, high-performance encryption.</li> <li>Identity Based: Authentication is handled via existing identity providers (Google, Microsoft, GitHub) rather than managing separate VPN keys.</li> </ul>"},{"location":"services/tailscale/tailscale/#installation-setup","title":"Installation &amp; Setup","text":"<p>Setting up a basic mesh network involves three steps.</p>"},{"location":"services/tailscale/tailscale/#1-account-creation","title":"1. Account Creation","text":"<p>Navigate to tailscale.com and sign up using an existing Single Sign-On (SSO) provider.</p>"},{"location":"services/tailscale/tailscale/#2-client-installation","title":"2. Client Installation","text":"<p>Install the application on every device intended for the mesh (Windows, macOS, Linux, iOS, Android).</p> <p>Linux Installation Command:</p> <pre><code>curl -fsSL https://tailscale.com/install.sh | sh\n</code></pre>"},{"location":"services/tailscale/tailscale/#3-authentication","title":"3. Authentication","text":"<p>Open the application on the device and log in. The device will immediately join the Tailnet.</p> <p>MagicDNS: Tailscale automatically assigns a readable domain name to every device (e.g., <code>server-pc</code> or <code>iphone</code>). Devices can be accessed via <code>ping</code> or SSH using this hostname, eliminating the need to memorize IP addresses.</p>"},{"location":"services/tailscale/tailscale/#advanced-features","title":"Advanced Features","text":"<p>Once connected, several configuration options are available to extend the network's utility.</p> Feature Description Best Use Case Exit Nodes Routes all internet traffic through a specific device on the network. Securing traffic on public Wi-Fi; appearing to be at a specific location while traveling. Subnet Routers Allows access to LAN devices that cannot run Tailscale (printers, IoT, legacy servers). Remote access to an entire home LAN without installing the client on every device. Taildrop A peer-to-peer file transfer tool. Transferring files between different operating systems instantly."},{"location":"services/tailscale/tailscale/#configuring-an-exit-node-linux","title":"Configuring an Exit Node (Linux)","text":"<p>To configure a Linux server to act as an Exit Node:</p> <ol> <li>Enable IP Forwarding: Ensure the host allows packet forwarding.</li> <li> <p>Advertise the Node: <pre><code>sudo tailscale up --advertise-exit-node\n</code></pre></p> </li> <li> <p>Approve: In the Admin Console (web), locate the machine, open the ... menu &gt; Edit route settings, and check \"Use as exit node.\"</p> </li> </ol>"},{"location":"services/tailscale/tailscale/#subnet-router-configuration-pi-hole-integration","title":"Subnet Router Configuration (Pi-hole Integration)","text":"<p>This setup allows remote access to local LAN services using a Pi-hole for DNS resolution.</p>"},{"location":"services/tailscale/tailscale/#1-enable-ip-forwarding","title":"1. Enable IP Forwarding","text":"<p>Packet forwarding must be enabled at the kernel level.</p> <pre><code># 1. Enable IPv4 forwarding immediately\nsudo sysctl -w net.ipv4.ip_forward=1\n\n# 2. Make the change permanent across reboots\necho 'net.ipv4.ip_forward = 1' | sudo tee -a /etc/sysctl.conf\n</code></pre>"},{"location":"services/tailscale/tailscale/#2-advertise-subnet-routes","title":"2. Advertise Subnet Routes","text":"<p>Run the following command to advertise the local subnet (replace <code>192.168.0.0/24</code> with the actual LAN subnet):</p> <pre><code>sudo tailscale up --advertise-routes=192.168.0.0/24\n</code></pre>"},{"location":"services/tailscale/tailscale/#3-approve-route","title":"3. Approve Route","text":"<ol> <li>Navigate to the Tailscale Admin Console.</li> <li>Go to Machines -&gt; Select the Router Node -&gt; Edit route settings.</li> <li>Approve the previously advertised subnet.</li> </ol>"},{"location":"services/tailscale/tailscale/#4-dns-resolution-flow","title":"4. DNS Resolution Flow","text":"<p>Once configured, the resolution flow works as follows:</p> <ol> <li>DNS Config: The Pi-hole is set as the Nameserver in Tailscale settings.</li> <li>Query: A remote client queries <code>service.domain.me</code>.</li> <li>Resolution: The Pi-hole responds with the service's local LAN IP.</li> <li>Routing: Since the subnet route is approved, the remote client traffic is automatically routed through the Tailscale tunnel to the Subnet Router, which accesses the device on the LAN.</li> </ol>"},{"location":"services/tailscale/tailscale/#access-control-lists-acls","title":"Access Control Lists (ACLs)","text":"<p>By default, every device in a Tailnet can communicate with every other device. ACLs (defined in JSON) are used to restrict this access.</p> <p>Example: Server Isolation This rule allows Admins to access everything, but prevents servers from initiating connections to personal devices.</p> <pre><code>{\n  \"acls\": [\n    // Admins can access everything\n    { \"action\": \"accept\", \"src\": [\"group:admin\"], \"dst\": [\"*:*\"] },\n\n    // Servers can only talk to other servers (tagged devices)\n    { \"action\": \"accept\", \"src\": [\"tag:server\"], \"dst\": [\"tag:server:*\"] }\n  ]\n}\n</code></pre> <p>Note: Tailscale uses \"Tags\" (e.g., <code>tag:server</code>) to manage permissions for headless devices, rather than relying on user identities. This ensures services do not lose access if a specific user account is removed.</p>"},{"location":"services/tailscale/tailscale/#maintenance-best-practices","title":"Maintenance &amp; Best Practices","text":"<ul> <li>Key Expiry: By default, authentication keys expire every 180 days.</li> <li> <p>Recommendation: In the Admin Console, select \"Disable Key Expiry\" for servers and headless devices to prevent remote lockouts.</p> </li> <li> <p>Tailscale SSH: Enable this feature to allow SSH access between Tailscale devices using Tailscale identity verification, removing the need to manage local SSH keys.</p> </li> <li>Battery Life: While efficient, toggling Tailscale off on mobile devices when not accessing home services can conserve battery life.</li> </ul>"},{"location":"services/utilities/convertx/","title":"ConvertX","text":"<p>Github  </p> <p>A self-hosted online file converter. Supports over a thousand different formats. Written with TypeScript, Bun and Elysia.</p>"},{"location":"services/utilities/convertx/#features","title":"Features","text":"<ul> <li>Convert files to different formats</li> <li>Process multiple files at once</li> <li>Password protection</li> <li>Multiple accounts</li> </ul>"},{"location":"services/utilities/convertx/#converters-supported","title":"Converters supported","text":"Converter Use case Converts from Converts to Inkscape Vector images 7 17 libjxl JPEG XL 11 11 resvg SVG 1 1 Vips Images 45 23 libheif HEIF 2 4 XeLaTeX LaTeX 1 1 Calibre E-books 26 19 LibreOffice Documents 41 22 Dasel Data Files 5 4 Pandoc Documents 43 65 msgconvert Outlook 1 1 dvisvgm Vector images 4 2 ImageMagick Images 245 183 GraphicsMagick Images 167 130 Assimp 3D Assets 77 23 FFmpeg Video ~472 ~199 Potrace Raster to vector 4 11 VTracer Raster to vector 8 1 Markitdown Documents 6 1"},{"location":"services/utilities/convertx/#deployment","title":"Deployment","text":"<p>[!WARNING] If you can't login, make sure you are accessing the service over localhost or https otherwise set HTTP_ALLOWED=true</p> <pre><code># docker-compose.yml\nservices:\n  convertx:\n    image: ghcr.io/c4illin/convertx\n    container_name: convertx\n    restart: unless-stopped\n    ports:\n      - \"3000:3000\"\n    environment:\n      - JWT_SECRET=aLongAndSecretStringUsedToSignTheJSONWebToken1234 # will use randomUUID() if unset\n      # - HTTP_ALLOWED=true # uncomment this if accessing it over a non-https connection\n    volumes:\n      - ./data:/app/data\n</code></pre> <p>or</p> <pre><code>docker run -p 3000:3000 -v ./data:/app/data ghcr.io/c4illin/convertx\n</code></pre> <p>Then visit <code>http://localhost:3000</code> in your browser and create your account. Don't leave it unconfigured and open, as anyone can register the first account.</p> <p>If you get unable to open database file run <code>chown -R $USER:$USER path</code> on the path you choose.</p>"},{"location":"services/utilities/convertx/#environment-variables","title":"Environment variables","text":"<p>All are optional, JWT_SECRET is recommended to be set.</p> Name Default Description JWT_SECRET when unset it will use the value from randomUUID() A long and secret string used to sign the JSON Web Token ACCOUNT_REGISTRATION false Allow users to register accounts HTTP_ALLOWED false Allow HTTP connections, only set this to true locally ALLOW_UNAUTHENTICATED false Allow unauthenticated users to use the service, only set this to true locally AUTO_DELETE_EVERY_N_HOURS 24 Checks every n hours for files older then n hours and deletes them, set to 0 to disable WEBROOT The address to the root path setting this to \"/convert\" will serve the website on \"example.com/convert/\" FFMPEG_ARGS Arguments to pass to the input file of ffmpeg, e.g. <code>-hwaccel vaapi</code>. See https://github.com/C4illin/ConvertX/issues/190 for more info about hw-acceleration. FFMPEG_OUTPUT_ARGS Arguments to pass to the output of ffmpeg, e.g. <code>-preset veryfast</code> HIDE_HISTORY false Hide the history page LANGUAGE en Language to format date strings in, specified as a BCP 47 language tag UNAUTHENTICATED_USER_SHARING false Shares conversion history between all unauthenticated users MAX_CONVERT_PROCESS 0 Maximum number of concurrent conversion processes allowed. Set to 0 for unlimited."},{"location":"services/utilities/convertx/#docker-images","title":"Docker images","text":"<p>There is a <code>:latest</code> tag that is updated with every release and a <code>:main</code> tag that is updated with every push to the main branch. <code>:latest</code> is recommended for normal use.</p> <p>The image is available on GitHub Container Registry and Docker Hub.</p> Image What it is <code>image: ghcr.io/c4illin/convertx</code> The latest release on ghcr <code>image: ghcr.io/c4illin/convertx:main</code> The latest commit on ghcr <code>image: c4illin/convertx</code> The latest release on docker hub <code>image: c4illin/convertx:main</code> The latest commit on docker hub <p> </p>"},{"location":"services/utilities/obslivesync/","title":"Self-Hosted Obsidian LiveSync with Tailscale","text":"<p>Github </p>"},{"location":"services/utilities/obslivesync/#overview","title":"Overview","text":"<p>This covers setting up a self-hosted synchronization backend for Obsidian using CouchDB and Obsidian LiveSync. I used the offical guide and it...did not go well, so here's an adjusted version </p> <p>Tailscale is used to secure the connection, allowing devices to sync remotely without opening ports on the router or exposing the database to the public internet.</p>"},{"location":"services/utilities/obslivesync/#installation","title":"Installation","text":""},{"location":"services/utilities/obslivesync/#1-file-permissions-critical","title":"1. File Permissions (Critical)","text":"<p>The official CouchDB image runs as a specific internal user with ID <code>5984</code>. Docker typically creates volume directories as <code>root</code>, which causes the container to crash with <code>Permission denied</code> errors.</p> <p>You must pre-create the directories and assign the correct ownership before starting the container.</p> <pre><code>mkdir -p ./couchdb-data ./couchdb-etc\nsudo chown -R 5984:5984 ./couchdb-data\nsudo chown -R 5984:5984 ./couchdb-etc\n</code></pre> <p>Note</p> <p>If you need to edit files in <code>couchdb-etc</code> manually later, you may need to temporarily change ownership back to your user (<code>sudo chown -R $USER:$USER ...</code>) or use <code>sudo</code>.</p>"},{"location":"services/utilities/obslivesync/#2-configuration-file-dockerini","title":"2. Configuration File (<code>docker.ini</code>)","text":"<p>A configuration file must be injected to handle Large File Support (LFS), CORS headers (essential for remote access), and authentication limits.</p> <p>Create the file at <code>./couchdb-etc/docker.ini</code>:</p> <pre><code>[couchdb]\nsingle_node = true\nmax_document_size = 50000000\n\n[chttpd]\nbind_address = 0.0.0.0\nport = 5984\nrequire_valid_user = true\nmax_http_request_size = 4294967296\nenable_cors = true\n\n[chttpd_auth]\nrequire_valid_user = true\nauthentication_redirect = /_utils/session.html\n\n[httpd]\nWWW-Authenticate = Basic realm=\"couchdb\"\nenable_cors = true\n\n[cors]\n# Wildcard required for Tailscale/VPN roaming where Client IPs change\norigins = *\ncredentials = true\n# Explicit headers required for Obsidian LiveSync Plugin PUT/Auth operations\nheaders = accept, authorization, content-type, origin, referer\nmethods = GET, PUT, POST, HEAD, DELETE\nmax_age = 3600\n</code></pre>"},{"location":"services/utilities/obslivesync/#3-docker-compose","title":"3. Docker Compose","text":"<p>Create the <code>compose.yaml</code> file.</p> <p>Environment Variables</p> <p>This compose file uses <code>${COUCHDB_USER}</code> and <code>${COUCHDB_PASSWORD}</code>. You must either create a <code>.env</code> file in the same directory with these values or replace them directly in the file.</p> <pre><code>services:\n  couchdb:\n    image: couchdb:latest\n    container_name: couchdb-for-ols\n    user: 5984:5984\n    environment:\n      - COUCHDB_USER=${COUCHDB_USER}  #Please change as you like.\n      - COUCHDB_PASSWORD=${COUCHDB_PASS} #Please change as you like.\n    volumes:\n      - ./couchdb-data:/opt/couchdb/data\n      - ./couchdb-etc:/opt/couchdb/etc/local.d\n    ports:\n      - 5984:5984\n    restart: unless-stopped\n</code></pre> <p>Start the container:</p> <pre><code>docker compose up -d\n</code></pre>"},{"location":"services/utilities/obslivesync/#configuration","title":"Configuration","text":""},{"location":"services/utilities/obslivesync/#1-database-initialization","title":"1. Database Initialization","text":"<p>Once the container is running, the database structure must be initialized.</p> <p>Option A: Automatic Script Run this command on the host machine:</p> <pre><code>curl -s https://raw.githubusercontent.com/vrtmrz/obsidian-livesync/main/utils/couchdb/couchdb-init.sh | bash\n</code></pre> <ul> <li>Success: The output will end with <code>&lt;-- Configuring CouchDB by REST APIs Done!</code>.</li> </ul> <p>Option B: Manual Credential Injection If the script fails with errors like <code>ERROR: Hostname missing</code> (common in some Docker environments), run the command with specific credentials injected:</p> <pre><code>curl -s https://raw.githubusercontent.com/vrtmrz/obsidian-livesync/main/utils/couchdb/couchdb-init.sh | hostname=http://&lt;YOUR SERVER IP&gt;:5984 username=&lt;INSERT USERNAME HERE&gt; password=&lt;INSERT PASSWORD HERE&gt; bash\n</code></pre>"},{"location":"services/utilities/obslivesync/#2-generate-setup-uri","title":"2. Generate Setup URI","text":"<p>The easiest way to configure devices is to generate a \"Setup URI\" that contains all connection strings and encryption keys. This can be run using <code>deno</code> on a desktop or server.</p> <pre><code># Set variables\nexport hostname=https://you.tailscale.ip:5984 # Use Tailscale IP\nexport database=obsidiannotes \nexport passphrase=dfsapkdjaskdjasdas # This is for encryption\nexport username=johndoe\nexport password=abc123 # Matches COUCHDB_PASSWORD in compose.yaml\n\n# Run generator\ndeno run -A https://raw.githubusercontent.com/vrtmrz/obsidian-livesync/main/utils/flyio/generate_setupuri.ts\n</code></pre> <p>Output:</p> <pre><code>obsidian://setuplivesync?settings=%5B%22tm2DpsOE74nJAryprZO2M93wF%2Fvg.......\n\nYour passphrase of Setup-URI is: patient-haze\nThis passphrase is never shown again, so please note it in a safe place.\n</code></pre> <ul> <li>Save the Setup URI: This is needed to configure every device.</li> <li>Save the Passphrase: This is needed to decrypt data on new devices.</li> </ul>"},{"location":"services/utilities/obslivesync/#client-setup","title":"Client Setup","text":"<p>CRITICAL WARNING: Customization Sync</p> <p>During setup, the plugin may ask to enable \"Customization Sync\" or \"Configuration Sync\" (plugins, themes, settings). DO NOT ENABLE THIS. Syncing configuration files between different platforms (e.g., Windows vs. Android) causes boot loops, UI corruption, and file path conflicts. Decline all pop-ups related to syncing hidden folders (<code>.obsidian</code>). Only sync markdown notes and media.  When I used this originally it was a fucking nightmare to get things syncing correctly, so I'd avoid it. </p>"},{"location":"services/utilities/obslivesync/#network-configuration-tailscale","title":"Network Configuration (Tailscale)","text":"<p>When connecting via mobile devices over Tailscale, mobile OSs often block cleartext HTTP traffic to hostnames.</p> <ul> <li>Do NOT use: <code>http://my-server-name:5984</code></li> <li>USE: <code>http://100.x.y.z:5984</code> (The specific Tailscale IP of the server).</li> </ul>"},{"location":"services/utilities/obslivesync/#android-optimization","title":"Android Optimization","text":"<p>To prevent \"Silent Drift\" (where sync stops quietly in the background):</p> <ol> <li>Battery: Set the Obsidian app to Unrestricted / No Optimization in Android settings.</li> <li>Plugin Settings: In Obsidian LiveSync settings, switch the sync mode to \"Periodic\" (30s) or \"Adaptive\".</li> </ol>"},{"location":"services/utilities/obslivesync/#primary-device-setup-first-time","title":"Primary Device Setup (First Time)","text":"<p>Perform these steps on the first device to initialize the database structure.</p> <ol> <li>In Obsidian, open the LiveSync plugin.</li> <li>Select \"I am setting this up for the first time\".</li> <li>Paste the Setup URI generated earlier.</li> <li>Select \"Restart and initialize server\".</li> <li>Confirmation: Select YES on all overwrite prompts.</li> <li>Remote Config Fail: If you see \"Fetch remote config failed\", select \"Skip and Proceed\" (this is normal for an empty database).</li> <li>Send Chunks: Select NO.</li> <li>Config Doctor: Select NO on all prompts.</li> <li>Enable: Under Sync Settings, enable the LiveSync preset configuration.</li> </ol>"},{"location":"services/utilities/obslivesync/#adding-additional-devices","title":"Adding Additional Devices","text":"<p>Perform these steps for additional phones, laptops, or tablets.</p> <ol> <li>Open the LiveSync plugin.</li> <li>Paste the Setup URI and enter the Passphrase (<code>patient-haze</code> in the example above).</li> <li>Select \"Restart and fetch data\".</li> <li>Reset Sync: Choose Merge or Discard Local depending on preference.</li> <li>Optional Features: Select OK.</li> <li>Config Doctor: Select NO on all prompts.</li> <li>Enable: Under Sync Settings, enable the LiveSync preset configuration.</li> </ol>"},{"location":"services/utilities/obslivesync/#maintenance","title":"Maintenance","text":""},{"location":"services/utilities/obslivesync/#updating-couchdb","title":"Updating CouchDB","text":"<p>To update the database version, pull the latest image and restart the container.</p> <pre><code>docker compose pull\ndocker compose up -d\n</code></pre>"},{"location":"services/utilities/obslivesync/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Permission Denied: If the container loops on startup, check that the <code>./couchdb-data</code> folder is owned by <code>5984:5984</code>.</li> <li>CORS Errors: If connection fails from a new device, ensure <code>origins = *</code> is set in the <code>docker.ini</code>.</li> </ul>"}]}